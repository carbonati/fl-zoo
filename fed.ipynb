{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1024,
   "id": "7308860c",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "\n",
    "\n",
    "class CentralServer:\n",
    "    \"\"\"Central Server\"\"\"\n",
    "    def __init__(self, \n",
    "                 model, \n",
    "#                  dataloader,\n",
    "                 save_every_n=None,\n",
    "                 device='cpu'):\n",
    "        self.model = model\n",
    "#         self.dataloader = dataloader\n",
    "        self.save_every_n = save_every_n\n",
    "        self.device = device\n",
    "        \n",
    "    def aggregate(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def send_model(self):\n",
    "        \"\"\"Sends a copy of the global models parameters.\"\"\"\n",
    "        return deepcopy(self.model)\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        \"\"\"Save model parameters to disk.\"\"\"\n",
    "        torch.save(self.model.state_dict(), filepath)\n",
    "    \n",
    "    def update(self, global_state):\n",
    "        \"\"\"Updates the parameters of the global model.\"\"\"\n",
    "        self.model.load_state_dict(global_state)\n",
    "    \n",
    "    def predict(self, dataloader):\n",
    "        with torch.no_grad():\n",
    "            pass\n",
    "        \n",
    "    def validate(self, dataloader, criterion):\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        loss = 0 \n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in dataloader:\n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "                logits = self.model(x)\n",
    "                correct += (logits.argmax(-1) == y).sum().item()\n",
    "                loss += criterion(logits, y).item()\n",
    "        results = {\n",
    "            'loss': loss / len(dataloader),\n",
    "            'accuracy': correct / len(dataloader.dataset)\n",
    "        }\n",
    "        return results\n",
    "        \n",
    "        \n",
    "class FedAvgServer(CentralServer):\n",
    "    \"\"\"FederatedAveraging (FedAvg) central server as proposed in \"Communication-Efficient Learning \n",
    "    of Deep Networks from Decentralized Data\" (https://arxiv.org/pdf/1602.05629.pdf)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : \n",
    "    dataloader :\n",
    "    device : str (default='cpu')\n",
    "    \"\"\"\n",
    "    def __init__(self, model, dataloader=None, device='cpu'):\n",
    "        super(FedAvgServer, self).__init__(model,\n",
    "                                           dataloader,\n",
    "                                           device=device)\n",
    "        \n",
    "    def aggregate(self, clients, client_weights):\n",
    "        layer_names = self.model.state_dict().keys()\n",
    "        global_state = {}\n",
    "        for k, (client_id, client) in enumerate(clients.items()):\n",
    "            local_state = client.model.state_dict()\n",
    "            for layer_name in layer_names:\n",
    "                if k == 0:\n",
    "                    global_state[layer_name] = local_state[layer_name] * client_weights[k]\n",
    "                else:\n",
    "                    global_state[layer_name] += local_state[layer_name] * client_weights[k]\n",
    "\n",
    "        self.update(global_state)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1180,
   "id": "43a6b832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.py\n",
    "\n",
    "\n",
    "class Client:\n",
    "    \"\"\"Base client.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    client_id : str\n",
    "        Id of the client.\n",
    "    dataloader : DataLoader\n",
    "        Local dataset used for training on the client.\n",
    "    device : str, torch.device (default='cpu')\n",
    "        Device type.\n",
    "    \"\"\"\n",
    "    def __init__(self, client_id, train_dl, val_dl=None, device='cpu'):\n",
    "        self.client_id = client_id\n",
    "        self.train_dl = train_dl\n",
    "        self.val_dl = val_dl\n",
    "        self.device = device\n",
    "        self._model = None\n",
    "        \n",
    "    @property\n",
    "    def model(self):\n",
    "        return self._model\n",
    "    \n",
    "    @model.setter\n",
    "    def model(self, model):\n",
    "        self._model = model\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.train_dl.dataset)\n",
    "    \n",
    "    def update(self, optimizer, criterion, num_epochs=1):\n",
    "        \"\"\"Algorithm 1 (ClientUpdate).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        optim_cls : \n",
    "        optim_params :\n",
    "        num_epochs (E) : int\n",
    "            Number of epochs.\n",
    "        criterion : \n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        self.model.to(self.device)\n",
    "#         optim = optim_cls(self.model.parameters(), **optim_params)\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        for i in range(num_epochs):\n",
    "            for i, (x, y) in enumerate(self.train_dl):\n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                logits = self.model(x)\n",
    "                loss = criterion(logits, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                total_correct += (logits.argmax(-1) == y).sum().item()\n",
    "            \n",
    "        self.model.to('cpu')\n",
    "        results = {\n",
    "            'loss': total_loss / len(self.train_dl),\n",
    "            'accuracy': total_correct / len(self)\n",
    "        }\n",
    "        return results\n",
    "        \n",
    "    \n",
    "class FedProxClient(Client):\n",
    "    \"\"\"Base client.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    client_id : str\n",
    "        Id of the client.\n",
    "    dataloader : DataLoader\n",
    "        Local dataset used for training on the client.\n",
    "    device : str, torch.device (default='cpu')\n",
    "        Device type.\n",
    "    \"\"\"\n",
    "    def __init__(self, client_id, train_dl, **kwargs):\n",
    "        super(FedProxClient, self).__init__(client_id, train_dl, **kwargs)\n",
    "        \n",
    "    def update(self, optimizer, criterion, num_epochs=1):\n",
    "        \"\"\"Algorithm 1 (ClientUpdate).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        optimizer :\n",
    "        num_epochs (E) : int\n",
    "            Number of epochs.\n",
    "        criterion : \n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        self.model.to(self.device)\n",
    "#         optim = optim_cls(self.model.parameters(), **optim_params)\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        total_xent_loss = 0\n",
    "        total_weights_delta = 0\n",
    "        for i in range(num_epochs):\n",
    "            for i, (x, y) in enumerate(self.train_dl):\n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                logits = self.model(x)\n",
    "                loss, xent_loss, weights_delta = criterion(logits, y, self.model.state_dict())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                total_correct += (logits.argmax(-1) == y).sum().item()\n",
    "                total_xent_loss += xent_loss.item()\n",
    "                total_weights_delta += weights_delta.item()\n",
    "            \n",
    "        self.model.to('cpu')\n",
    "        results = {\n",
    "            'loss': total_loss / len(self.train_dl),\n",
    "            'accuracy': total_correct / len(self),\n",
    "            'xent_loss': total_xent_loss / len(self.train_dl),\n",
    "            'weights_delta': weights_delta / len(self.train_dl),\n",
    "#             'mean_grad': \n",
    "        }\n",
    "        return results\n",
    "\n",
    "# class FedAvgClient(Client):\n",
    "#     \"\"\"FederatedAveraging (FedAvg) central server as proposed in \"Communication-Efficient Learning \n",
    "#     of Deep Networks from Decentralized Data\" (https://arxiv.org/pdf/1602.05629.pdf)\n",
    "    \n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     client_id : str\n",
    "#         Id of the client.\n",
    "#     dataloader : DataLoader\n",
    "#         Local dataset used for training on the client.\n",
    "#     device : str, torch.device (default='cpu')\n",
    "#         Device type.\n",
    "#     \"\"\"\n",
    "#     def update(self, optim_cls, optim_params, num_epochs, criterion):\n",
    "#         \"\"\"Algorithm 1 (ClientUpdate).\n",
    "        \n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         optim_cls : \n",
    "#         optim_params :\n",
    "#         num_epochs : int\n",
    "#             Number of epochs, referred to as E.\n",
    "#         criterion : \n",
    "#         \"\"\"\n",
    "#         self.model.train()\n",
    "#         self.model.to(self.device)\n",
    "#         optim = optim_cls(self.model.parameters(), **optim_params)\n",
    "        \n",
    "#         for i in range(num_epochs):\n",
    "#             for i, (x, y) in enumerate(self.dataloader):\n",
    "#                 x = x.to(self.device)\n",
    "#                 y = y.to(self.device)\n",
    "                \n",
    "#                 optim.zero_grad()\n",
    "                \n",
    "#                 logits = self.model(x)\n",
    "#                 loss = criterion(logits, y)\n",
    "#                 loss.backward()\n",
    "#                 optim.step()\n",
    "            \n",
    "#         # why set to device ?\n",
    "#         self.model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "id": "ca75e44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.py\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class SplitDataset(Dataset):\n",
    "    \"\"\"Dataset for a client partitioned by a list of indices.\"\"\"\n",
    "    def __init__(self, dataset, indices):\n",
    "        self.dataset = dataset\n",
    "        self.indices = list(indices)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.dataset[self.indices[index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1181,
   "id": "d411d26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling.py\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def get_iid_shards(labels,\n",
    "                   num_clients=100,\n",
    "                   client_ids=None,\n",
    "                   seed=None):\n",
    "    \"\"\"Returns a homogeneous (IID) mapping of client ID's to sample indices of a dataset.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    labels : list, np.ndarray, torch.Tensor\n",
    "        List of class labels for a dataset.\n",
    "    num_clients : int (default=100)\n",
    "        Number of clients used for training.\n",
    "    client_ids : list, np.ndarray (default=None)\n",
    "        List of client ID's. If `None`, defaults to a `range(num_clients)`.\n",
    "    seed : int (default=None)\n",
    "        Random state.\n",
    "       \n",
    "    Notes\n",
    "    -----\n",
    "    ```\n",
    "    num_samples_per_client = len(labels) // num_clients # 60000 / 100 = 600 for MNIST\n",
    "    ```\n",
    "    \"\"\"\n",
    "    random_state = np.random.RandomState(seed)\n",
    "    client_ids = client_ids or list(range(num_clients))\n",
    "    \n",
    "    # randomly shuffle sample indices to generate IID (homogeneous) clients\n",
    "    indices_shuffled = random_state.choice(range(len(labels)), len(labels), replace=False)\n",
    "    num_samples_per_client = len(indices_shuffled) // num_clients\n",
    "\n",
    "    # assign `num_samples_per_client` random samples to each client\n",
    "    for i, client_id in enumerate(client_ids):\n",
    "        client_indices = indices_shuffled[i * num_samples_per_client : (i + 1) * num_samples_per_client]\n",
    "        client_to_shard[client_id] = client_indices\n",
    "    \n",
    "    return client_to_shard\n",
    "    \n",
    "    \n",
    "def get_non_iid_shards(labels, \n",
    "                       num_clients=100,\n",
    "                       client_ids=None,\n",
    "                       shard_size=300,\n",
    "                       drop_last=False,\n",
    "                       seed=None):\n",
    "    \"\"\"Returns a heterogeneous (non-IID) mapping of client ID's to sample indices of a dataset.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    labels : list, np.ndarray, torch.Tensor\n",
    "        List of class labels for a dataset.\n",
    "    num_clients : int (default=100)\n",
    "        Number of clients used for training.\n",
    "    client_ids : list, np.ndarray (default=None)\n",
    "        List of client ID's. If `None`, defaults to a `range(num_clients)`.\n",
    "    shard_size : int (default=300)\n",
    "        Size of each shard to split labels by.\n",
    "    seed : int (default=None)\n",
    "        Random state.\n",
    "       \n",
    "    Notes\n",
    "    -----\n",
    "    ```\n",
    "    num_shards = len(labels) // shard_size # 60000 / 300 = 200 for MNIST\n",
    "    num_shards_per_client = num_shards // num_clients # 200 / 100 = 2 for MNIST\n",
    "    ```\n",
    "    \"\"\"\n",
    "    random_state = np.random.RandomState(seed)\n",
    "    client_ids = client_ids or list(range(num_clients))\n",
    "    classes = np.unique(labels)\n",
    "    num_classes = len(classes)\n",
    "\n",
    "    label_to_indices = {}\n",
    "    client_to_shard = {}\n",
    "    shards = []\n",
    "    shard_labels = []\n",
    "    # map each class to the sample indices in the dataset\n",
    "    for label in classes:\n",
    "        label_to_indices[label] = np.where(labels == label)[0]\n",
    "\n",
    "    # split each class labels indices to shards of size `shard_size`\n",
    "    for label in classes:\n",
    "        indices = label_to_indices[label]\n",
    "        num_extra = len(indices) % shard_size\n",
    "        # (num_shards_per_label, shard_size)\n",
    "        label_shards = indices[:-num_extra].reshape(-1, shard_size).tolist()\n",
    "\n",
    "        if num_extra > 0 and not drop_last:\n",
    "            label_shards.append(indices[-num_extra:])\n",
    "\n",
    "        # store the shards in a list to resample and assign to a client\n",
    "        shard_labels.extend([label] * len(label_shards))\n",
    "        shards.extend(label_shards)\n",
    "\n",
    "    num_shards = len(shard_labels) # number of shards of size `shard_size`\n",
    "    num_shards_per_client = num_shards // num_clients # how many shards fit in each client\n",
    "\n",
    "    # shuffle the shards and assign `num_shards_per_client` to each client\n",
    "    # each client should ideally have `num_shards_per_client` from a different class lablel\n",
    "    shard_indices_shuffled = random_state.choice(range(num_shards), \n",
    "                                                 num_shards,\n",
    "                                                 replace=False)\n",
    "    for i, client_id in enumerate(client_ids):\n",
    "        client_to_shard[client_id] = []\n",
    "        shard_indices = shard_indices_shuffled[i * num_shards_per_client : (i + 1) * num_shards_per_client]\n",
    "        for idx in shard_indices:\n",
    "            client_to_shard[client_id].extend(shards[idx])\n",
    "            \n",
    "    return client_to_shard\n",
    "\n",
    "\n",
    "def get_client_data(dataset, \n",
    "                    num_clients,\n",
    "                    client_ids=None,\n",
    "                    is_iid=True,\n",
    "                    shard_size=300,\n",
    "                    drop_last=True,\n",
    "                    seed=None,\n",
    "                    **kwargs):\n",
    "    \"\"\"Returns a mapping of client ID's to their corresponding train & validation dataloaders.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    is_iid : bool (default=True)\n",
    "        Boolean whether data is IID (Homogeneous) or non-IID (Heterogeneous)\n",
    "    **kwargs\n",
    "        Additional parameters used when instantiating each clients dataloader \n",
    "        \n",
    "    \"\"\"\n",
    "    client_to_data = {}\n",
    "    labels = dataset.targets\n",
    "    if is_iid:\n",
    "        client_to_shard = get_iid_shards(\n",
    "            labels,\n",
    "            num_clients=num_clients,\n",
    "            client_ids=client_ids,\n",
    "            seed=seed\n",
    "        )\n",
    "    else: # non-iid\n",
    "        client_to_shard = get_non_iid_shards(\n",
    "            labels,\n",
    "            num_clients=num_clients,\n",
    "            client_ids=client_ids,\n",
    "            shard_size=shard_size,\n",
    "            drop_last=drop_last,\n",
    "            seed=seed\n",
    "        )\n",
    "        \n",
    "    # iterate through each client and create train/val dataloaders using the shard indices\n",
    "    for k in client_to_shard.keys():\n",
    "        client_indices = client_to_shard[k]\n",
    "#         if val_size > 0:\n",
    "#             train_indices, val_indices = train_test_split(client_indices, test_size=val_size,  random_state=seed)\n",
    "#             train_dataset = SplitDataset(dataset, train_indices)\n",
    "#             val_dataset = SplitDataset(dataset, val_indices)\n",
    "#         else:\n",
    "#             train_dataset = SplitDataset(dataset, client_indices)\n",
    "#             val_dataset = None\n",
    "        client_dataset = SplitDataset(dataset, client_indices)\n",
    "        client_to_data[k] = DataLoader(client_dataset, drop_last=drop_last, **kwargs)\n",
    "#             'val': DataLoader(train_dataset, drop_last=drop_last, **kwargs)\n",
    "        \n",
    "    return client_to_data\n",
    "\n",
    "\n",
    "def get_clients(dataset,\n",
    "                method='fedavg',\n",
    "                num_clients=100,\n",
    "                client_ids=None,\n",
    "                is_iid=True,\n",
    "                shard_size=300,\n",
    "                drop_last=True,\n",
    "                batch_size=32,\n",
    "                num_workers=0,\n",
    "                device='cpu',\n",
    "                **kwargs):\n",
    "    if method == 'fedavg':\n",
    "        client_cls = Client\n",
    "    elif method == 'fedprox':\n",
    "        client_cls = FedProxClient\n",
    "    else:\n",
    "        raise ValueError(f'Unrecognized `method` {method}')\n",
    "    client_to_data = get_client_data(\n",
    "        dataset,\n",
    "        num_clients=num_clients,\n",
    "        client_ids=client_ids,\n",
    "        is_iid=is_iid,\n",
    "        shard_size=shard_size,\n",
    "        drop_last=drop_last,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        **kwargs\n",
    "    )\n",
    "    clients = {\n",
    "        k: client_cls(k, dl, device=device)\n",
    "        for k, dl\n",
    "        in client_to_data.items()\n",
    "    }\n",
    "    return clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1140,
   "id": "d1c0da81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "\n",
    "\n",
    "class BaseFederater:\n",
    "    \"\"\"Base Federater.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "    dataset : torch.utils.data.Dataset\n",
    "    criterion : nn.Module\n",
    "        Loss function to optimize on each client.\n",
    "    num_clients (K) : int (default=100)\n",
    "        Number of clients to partition `dataset`.\n",
    "    batch_size (B) : int, dict[str, int] (defualt=32)\n",
    "        Number of samples per batch to load on each client. \n",
    "        Can be a dictionary mapping each client ID to it's corresponding batch size\n",
    "        to allow for various batch sizes across clients.\n",
    "    shard_size : int (default=300)\n",
    "    is_iid : bool (default=False)\n",
    "    drop_last : bool (default=True)\n",
    "    num_workers : int (default=0)\n",
    "    device : str (default='cpu')\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 model,\n",
    "                 clients,\n",
    "                 criterion,\n",
    "                 optim_cls,\n",
    "                 optim_params=None,\n",
    "                 C=0.1,\n",
    "                 eval_every_n=1,\n",
    "                 num_workers=0,\n",
    "                 device='cpu',\n",
    "                 output_dir=None):\n",
    "        self.model = model\n",
    "        self.clients = clients\n",
    "        self.criterion = criterion\n",
    "        self.optim_cls = optim_cls\n",
    "        self.optim_params = optim_params\n",
    "        self.C = C\n",
    "        self.eval_every_n = eval_every_n\n",
    "        self.num_workers = num_workers\n",
    "        self.device = device\n",
    "        self.output_dir = output_dir\n",
    "        self.writer = SummaryWriter(self.output_dir)\n",
    "        self.server = None\n",
    "        \n",
    "        self.num_clients = len(self.clients)\n",
    "        self.num_samples = sum([len(c) for c in self.clients.values()]) # n\n",
    "        self.client_ids = list(self.clients.keys())\n",
    "        \n",
    "        self._global_round = 0\n",
    "        \n",
    "    @property\n",
    "    def global_round(self):\n",
    "        return self._global_round\n",
    "\n",
    "    @global_round.setter\n",
    "    def global_round(self, global_round):\n",
    "        self._global_round = global_round\n",
    "    \n",
    "    def update(self, client_ids, num_epochs, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def get_optimizer(self, model, optim_params):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def fit(self, num_rounds, num_epochs, val_dl=None):\n",
    "        m = max(int(np.ceil(self.num_clients * self.C)), 1)\n",
    "        for t in range(num_rounds):\n",
    "            self.global_round += 1\n",
    "            S = np.random.choice(self.client_ids, m, replace=False)\n",
    "            train_metrics = self.update(client_ids=S, num_epochs=num_epochs)\n",
    "            \n",
    "            if self.eval_every_n is not None and t % self.eval_every_n == 0:\n",
    "                template_str = f'round {self.global_round}'\n",
    "                val_metrics = self.validate(val_dl)\n",
    "                for metric, value in train_metrics.items():\n",
    "                    self.writer.add_scalar(f'train/{metric}', value, self.global_round)\n",
    "                    template_str += f' - train_{metric} : {value:0.4f}'\n",
    "                for metric, value in val_metrics.items():\n",
    "                    self.writer.add_scalar(f'val/{metric}', value, self.global_round)\n",
    "                    template_str += f' - val_{metric} : {value:0.4f}'\n",
    "                \n",
    "                print(template_str)\n",
    "        \n",
    "    def validate(self, val_dl):\n",
    "        return self.server.validate(val_dl, self.criterion)\n",
    "        \n",
    "    def send_model(self):\n",
    "        \"\"\"Send the current state of the global model to each client.\"\"\"\n",
    "        for client_id, client in self.clients.items():\n",
    "            client.model = self.server.send_model()\n",
    "            \n",
    "    def get_gradients(self):\n",
    "        pass\n",
    "            \n",
    "            \n",
    "class FedAvg(BaseFederater):\n",
    "    \"\"\"FederatedAveraging (FedAvg) as proposed in \"Communication-Efficient Learning \n",
    "    of Deep Networks from Decentralized Data\" (https://arxiv.org/pdf/1602.05629.pdf)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 clients,\n",
    "                 criterion,\n",
    "                 optim_cls,\n",
    "                 optim_params=None,\n",
    "                 C=0.1,\n",
    "                 eval_every_n=1,\n",
    "                 device='cpu',\n",
    "                 num_workers=0,\n",
    "                 output_dir=None):\n",
    "        super(FedAvg, self).__init__(model, \n",
    "                                     clients,\n",
    "                                     criterion,\n",
    "                                     optim_cls=optim_cls,\n",
    "                                     optim_params=optim_params,\n",
    "                                     C=C,\n",
    "                                     eval_every_n=eval_every_n,\n",
    "                                     num_workers=num_workers,\n",
    "                                     device=device,\n",
    "                                     output_dir=output_dir)\n",
    "        \n",
    "        self.server = FedAvgServer(self.model, device=self.device)\n",
    "        self.client_weights = [len(c) / self.num_samples for c in self.clients.values()] # n_k / n\n",
    "        \n",
    "    def get_client_optimizer(self, model, optim_params):\n",
    "        optim_params = optim_params or {}\n",
    "        return self.optim_cls(model.parameters(), **optim_params)\n",
    "        \n",
    "    def update(self, client_ids, num_epochs):\n",
    "        \"\"\"Performs a full round of training on each client for E epochs.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        client_ids : list, np.ndarray\n",
    "            List of client ID's to train on the current round (S_t).\n",
    "        E : int\n",
    "            Number of epochs to train on each client.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        metrics_dict : dict\n",
    "            Dictionary mapping each metric to the average score across `client_ids`\n",
    "        \"\"\"\n",
    "        # send the global model parameters to each client\n",
    "        self.send_model()\n",
    "        # instantiate the client optimizer\n",
    "        optimizer = self.get_client_optimizer(self.clients[k].model, self.optim_params)\n",
    "        \n",
    "        metrics_dict = defaultdict(lambda: 0)\n",
    "        for k in client_ids:\n",
    "            \n",
    "            # update the client weights and record the local training metrics\n",
    "            client_metrics_dict = self.clients[k].update(\n",
    "                optimizer,\n",
    "                criterion=self.criterion,\n",
    "                num_epochs=num_epochs\n",
    "            )\n",
    "            \n",
    "            # update the summary writer and record loss/acc from the client\n",
    "            for metric, value in client_metrics_dict.items():\n",
    "                self.writer.add_scalar(f'client/{k}/{metric}', value, self.global_round)\n",
    "                metrics_dict[metric] += value / len(client_ids)\n",
    "        \n",
    "        # note that we use all clients as shown in the \"Server executes\" section of \"Algorithm 1\"\n",
    "        # when aggregating client weights, not just the `m` clients sampled for the current round\n",
    "        self.server.aggregate(self.clients, self.client_weights)\n",
    "        \n",
    "        return metrics_dict\n",
    "    \n",
    "    \n",
    "class FedProx(BaseFederater):\n",
    "    \"\"\"FederatedAveraging (FedAvg) as proposed in \"Communication-Efficient Learning \n",
    "    of Deep Networks from Decentralized Data\" (https://arxiv.org/pdf/1602.05629.pdf)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 clients,\n",
    "                 criterion,\n",
    "                 optim_cls,\n",
    "                 C,\n",
    "                 mu=0,\n",
    "                 optim_params=None,\n",
    "                 eval_every_n=1,\n",
    "                 device='cpu',\n",
    "                 num_workers=0,\n",
    "                 output_dir=None):\n",
    "        super(FedProx, self).__init__(model, \n",
    "                                     clients,\n",
    "                                     criterion,\n",
    "                                      optim_cls,\n",
    "                                     C=C,\n",
    "                                      optim_params=optim_params,\n",
    "                                     eval_every_n=eval_every_n,\n",
    "                                     num_workers=num_workers,\n",
    "                                     device=device,\n",
    "                                     output_dir=output_dir)\n",
    "        self.mu = mu\n",
    "        self.server = FedAvgServer(self.model, device=self.device)\n",
    "        self.client_weights = [len(c) / self.num_samples for c in self.clients.values()] # n_k / n\n",
    "        \n",
    "    def get_client_optimizer(self, model, optim_params):\n",
    "        optim_params = optim_params or {}\n",
    "        return self.optim_cls(model.parameters(), **optim_params)\n",
    "        \n",
    "    def update(self, client_ids, num_epochs):\n",
    "        \"\"\"Performs a full round of training on each client for E epochs.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        client_ids : list, np.ndarray\n",
    "            List of client ID's to train on the current round (S_t).\n",
    "        E : int\n",
    "            Number of epochs to train on each client.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        metrics_dict : dict\n",
    "            Dictionary mapping each metric to the average score across `client_ids`\n",
    "        \"\"\"\n",
    "        # send the global model parameters to each client\n",
    "        self.send_model()\n",
    "        \n",
    "        metrics_dict = defaultdict(lambda: 0)\n",
    "        for k in client_ids:\n",
    "            # instantiate a client optimizer\n",
    "            client_optimizer = self.get_client_optimizer(self.clients[k].model, self.optim_params)\n",
    "            criterion = FedProxLoss(self.mu, self.model.state_dict())\n",
    "            # update the client weights and record the local training metrics\n",
    "            client_metrics_dict = self.clients[k].update(\n",
    "                client_optimizer,\n",
    "                criterion=criterion,\n",
    "                num_epochs=num_epochs\n",
    "            )\n",
    "            \n",
    "            # update the summary writer and record loss/acc from the client\n",
    "            for metric, value in client_metrics_dict.items():\n",
    "                self.writer.add_scalar(f'client/{k}/{metric}', value, self.global_round)\n",
    "                metrics_dict[metric] += value / len(client_ids)\n",
    "        \n",
    "        # note that we use all clients as shown in the \"Server executes\" section of \"Algorithm 1\"\n",
    "        # when aggregating client weights, not just the `m` clients sampled for the current round\n",
    "        self.server.aggregate(self.clients, self.client_weights)\n",
    "        \n",
    "        return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1194,
   "id": "deea1e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class FedProxLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Empirically, we observe that increasing Âµ leads to smaller dissimilarity among local functions\n",
    "    Fk, and that the dissimilarity metric is consistent with the\n",
    "    training loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, mu, weights_initial):\n",
    "        super(FedProxLoss, self).__init__()\n",
    "        self.mu = mu\n",
    "        self.weights_initial = weights_initial\n",
    "        self._layer_names = list(weights_initial.keys())\n",
    "        \n",
    "        \n",
    "    def forward(self, pred, target, weights_new):\n",
    "        loss = 0\n",
    "        xent_loss = F.cross_entropy(pred, target)\n",
    "#         loss = xent_loss\n",
    "        weights_delta = []\n",
    "        for layer in self._layer_names:\n",
    "            layer_delta = torch.sum(torch.pow(self.weights_initial[layer] - weights_new[layer], 2))\n",
    "            loss += self.mu / 2 * layer_delta\n",
    "            weights_delta.append(layer_delta)\n",
    "#             weights_delta.append(\n",
    "#                 torch.sum(torch.pow(self.weights_initial[layer] - weights_new[layer], 2))\n",
    "#             )\n",
    "        weights_delta = torch.sum(torch.stack(weights_delta))\n",
    "        loss += xent_loss\n",
    "#         print(xent_loss)\n",
    "#         loss = xent_loss + self.mu / 2. * weights_delta\n",
    "        return loss, xent_loss, weights_delta\n",
    "#         loss += "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1075,
   "id": "e5acf2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.py\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \"\"\"CNN described in \"Communication-Efficient Learning of Deep Networks\n",
    "    from Decentralized Data\" (https://arxiv.org/pdf/1602.05629.pdf).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    in_features : int (default=1)\n",
    "        Number of channels in the input image.\n",
    "    num_classes : int (default=10)\n",
    "        Number of class labels.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features=1, num_classes=10):\n",
    "        super(CNN, self).__init__()\n",
    "        self._in_features = in_features\n",
    "        self._num_classes = num_classes\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(self._in_features,\n",
    "                               32,\n",
    "                               kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32,\n",
    "                               64,\n",
    "                               kernel_size=5)\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, self._num_classes)\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(2,2))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = x.reshape(len(x), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class MLP(nn.Module):\n",
    "    \"\"\"CNN described in \"Communication-Efficient Learning of Deep Networks\n",
    "    from Decentralized Data\" (https://arxiv.org/pdf/1602.05629.pdf).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    in_features : int (default=784)\n",
    "        Number input features.\n",
    "    hidden_dim : int (default=200)\n",
    "        Number of hidden units.\n",
    "    num_classes : int (default=10)\n",
    "        Number of class labels.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features=784, hidden_dim=200, num_classes=10):\n",
    "        super(MLP, self).__init__()\n",
    "        self._in_features = in_features\n",
    "        self._hidden_dim = hidden_dim\n",
    "        self._num_classes = num_classes\n",
    "        \n",
    "        self.fc1 = nn.Linear(self._in_features, self._hidden_dim)\n",
    "        self.fc2 = nn.Linear(self._hidden_dim, self._num_classes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "id": "4320609f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl = DataLoader(test_ds, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1191,
   "id": "f7c6279f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "optim_cls = torch.optim.SGD\n",
    "optim_params = {\n",
    "    'lr': 0.01,\n",
    "    'momentum': 0.9,\n",
    "    'weight_decay': 1e-5\n",
    "}\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1197,
   "id": "d12bb2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in train_dl:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1200,
   "id": "623fc315",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = model(x[None,...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1233,
   "id": "f4343ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.optimizer import Optimizer, required\n",
    "\n",
    "class FedProx(Optimizer):\n",
    "    r\"\"\"Implements FedAvg and FedProx. Local Solver can have momentum.\n",
    "    Nesterov momentum is based on the formula from\n",
    "    `On the importance of initialization and momentum in deep learning`__.\n",
    "    Args:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        ratio (float): relative sample size of client\n",
    "        gmf (float): global/server/slow momentum factor\n",
    "        mu (float): parameter for proximal local SGD\n",
    "        lr (float): learning rate\n",
    "        momentum (float, optional): momentum factor (default: 0)\n",
    "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
    "        dampening (float, optional): dampening for momentum (default: 0)\n",
    "        nesterov (bool, optional): enables Nesterov momentum (default: False)\n",
    "    Example:\n",
    "        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "        >>> optimizer.zero_grad()\n",
    "        >>> loss_fn(model(input), target).backward()\n",
    "        >>> optimizer.step()\n",
    "    __ http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf\n",
    "    .. note::\n",
    "        The implementation of SGD with Momentum/Nesterov subtly differs from\n",
    "        Sutskever et. al. and implementations in some other frameworks.\n",
    "        Considering the specific case of Momentum, the update can be written as\n",
    "        .. math::\n",
    "                  v = \\rho * v + g \\\\\n",
    "                  p = p - lr * v\n",
    "        where p, g, v and :math:`\\rho` denote the parameters, gradient,\n",
    "        velocity, and momentum respectively.\n",
    "        This is in contrast to Sutskever et. al. and\n",
    "        other frameworks which employ an update of the form\n",
    "        .. math::\n",
    "             v = \\rho * v + lr * g \\\\\n",
    "             p = p - v\n",
    "        The Nesterov version is analogously modified.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, ratio, gmf, lr=required, momentum=0, dampening=0,\n",
    "                 weight_decay=0, nesterov=False, variance=0, mu=0):\n",
    "        \n",
    "        self.gmf = gmf\n",
    "        self.ratio = ratio\n",
    "        self.itr = 0\n",
    "        self.a_sum = 0\n",
    "        self.mu = mu\n",
    "\n",
    "\n",
    "        if lr is not required and lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if momentum < 0.0:\n",
    "            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n",
    "        if weight_decay < 0.0:\n",
    "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
    "\n",
    "        defaults = dict(lr=lr, momentum=momentum, dampening=dampening,\n",
    "                        weight_decay=weight_decay, nesterov=nesterov, variance=variance)\n",
    "        if nesterov and (momentum <= 0 or dampening != 0):\n",
    "            raise ValueError(\"Nesterov momentum requires a momentum and zero dampening\")\n",
    "        super(FedProx, self).__init__(params, defaults)\n",
    "\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(FedProx, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('nesterov', False)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        print(self.state)\n",
    "        for group in self.param_groups:\n",
    "            weight_decay = group['weight_decay']\n",
    "            momentum = group['momentum']\n",
    "            dampening = group['dampening']\n",
    "            nesterov = group['nesterov']\n",
    "            \n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                d_p = p.grad.data\n",
    "\n",
    "                if weight_decay != 0:\n",
    "                    d_p.add_(weight_decay, p.data)\n",
    "                \n",
    "                param_state = self.state[p]\n",
    "                print('param_state')\n",
    "                if 'old_init' not in param_state:\n",
    "                    print('here')\n",
    "                    param_state['old_init'] = torch.clone(p.data).detach()\n",
    "\n",
    "                if momentum != 0:\n",
    "                    if 'momentum_buffer' not in param_state:\n",
    "                        buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
    "                    else:\n",
    "                        buf = param_state['momentum_buffer']\n",
    "                        buf.mul_(momentum).add_(1 - dampening, d_p)\n",
    "                    if nesterov:\n",
    "                        d_p = d_p.add(momentum, buf)\n",
    "                    else:\n",
    "                        d_p = buf\n",
    "\n",
    "                # apply proximal update\n",
    "                d_p.add_(self.mu, p.data - param_state['old_init'])\n",
    "                p.data.add_(-group['lr'], d_p)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def average(self):\n",
    "        param_list = []\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                p.data.mul_(self.ratio)\n",
    "                param_list.append(p.data)\n",
    "\n",
    "        communicate(param_list, dist.all_reduce)\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                param_state = self.state[p]\n",
    "                param_state['old_init'] = torch.clone(p.data).detach()\n",
    "                # Reinitialize momentum buffer\n",
    "                if 'momentum_buffer' in param_state:\n",
    "                    param_state['momentum_buffer'].zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b8c3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FedProx(Optimizer):\n",
    "    def __init__(self, \n",
    "                 params,\n",
    "                 lr=required,\n",
    "                 mu=0,\n",
    "                 momentum=0,\n",
    "                 dampening=0,\n",
    "                 weight_decay=0,\n",
    "                 nesterov=False):\n",
    "        self.mu = mu\n",
    "        \n",
    "        if lr is not required and lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if momentum < 0.0:\n",
    "            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n",
    "        if weight_decay < 0.0:\n",
    "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
    "\n",
    "        defaults = dict(lr=lr, momentum=momentum, dampening=dampening,\n",
    "                        weight_decay=weight_decay, nesterov=nesterov)\n",
    "        if nesterov and (momentum <= 0 or dampening != 0):\n",
    "            raise ValueError(\"Nesterov momentum requires a momentum and zero dampening\")\n",
    "        super(FedProx, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(FedProx, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('nesterov', False)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        closure : callable (default=NOne)\n",
    "            A closure that reevaluates the model and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            params_with_grad = []\n",
    "            d_p_list = []\n",
    "            momentum_buffer_list = []\n",
    "            weight_decay = group['weight_decay']\n",
    "            momentum = group['momentum']\n",
    "            dampening = group['dampening']\n",
    "            nesterov = group['nesterov']\n",
    "            lr = group['lr']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is not None:\n",
    "                    params_with_grad.append(p)\n",
    "                    d_p_list.append(p.grad)\n",
    "\n",
    "                    state = self.state[p]\n",
    "                    if 'momentum_buffer' not in state:\n",
    "                        momentum_buffer_list.append(None)\n",
    "                    else:\n",
    "                        momentum_buffer_list.append(state['momentum_buffer'])\n",
    "                    \n",
    "#                     # add the initial (global) weights on the first step call\n",
    "#                     if 'weights_initial' not in state:\n",
    "#                         state['weights_initial'] = p.data.clone().detach()\n",
    "\n",
    "            for i, param in enumerate(params_with_grad):\n",
    "                d_p = d_p_list[i]\n",
    "                if weight_decay != 0:\n",
    "                    d_p = d_p.add(param, alpha=weight_decay)\n",
    "\n",
    "                if momentum != 0:\n",
    "                    buf = momentum_buffer_list[i]\n",
    "                    if buf is None:\n",
    "                        buf = torch.clone(d_p).detach()\n",
    "                        momentum_buffer_list[i] = buf\n",
    "                    else:\n",
    "                        buf.mul_(momentum).add_(d_p, alpha=1 - dampening)\n",
    "                    if nesterov:\n",
    "                        d_p = d_p.add(buf, alpha=momentum)\n",
    "                    else:\n",
    "                        d_p = buf\n",
    "                \n",
    "                # add the proximal term shown in equation (2)\n",
    "                d_p.add_(p.data - param['initial_weights'], alpha=self.mu)\n",
    "                # gradient descent\n",
    "                param.add_(d_p, alpha=-lr)\n",
    "\n",
    "            # update momentum_buffers in state\n",
    "            for p, momentum_buffer in zip(params_with_grad, momentum_buffer_list):\n",
    "                state = self.state[p]\n",
    "                state['momentum_buffer'] = momentum_buffer\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def set_initial_weights(self, params):\n",
    "        for p in params:\n",
    "            for p in group['params']:\n",
    "                # add the initial (global) weights to each parameter state\n",
    "                state = self.state[p]\n",
    "                if 'initial_weights' not in state:\n",
    "                    state['initial_weights'] = p.data.clone().detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1314,
   "id": "57db4c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[ 0.0135,  0.0834, -0.1635, -0.1240,  0.0150],\n",
       "          [ 0.1076,  0.0894,  0.1221, -0.0690, -0.1478],\n",
       "          [-0.1232,  0.1202,  0.1382,  0.1460,  0.0215],\n",
       "          [ 0.1577,  0.0469,  0.0594,  0.0215,  0.1838],\n",
       "          [ 0.1121, -0.2022, -0.1584,  0.0095,  0.0925]]],\n",
       "\n",
       "\n",
       "        [[[-0.0625,  0.1570,  0.0061,  0.1393,  0.1369],\n",
       "          [-0.1410,  0.1502,  0.0581,  0.1548, -0.0272],\n",
       "          [-0.1242, -0.0803, -0.0600, -0.1837, -0.0414],\n",
       "          [ 0.0703, -0.0023,  0.0040, -0.1584, -0.1531],\n",
       "          [ 0.0853,  0.2344,  0.2251,  0.0915,  0.1444]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0254,  0.0159, -0.1718,  0.0243,  0.0359],\n",
       "          [ 0.1028,  0.0580,  0.1281, -0.1112,  0.0933],\n",
       "          [ 0.0104,  0.1602, -0.0586,  0.0608, -0.0146],\n",
       "          [-0.0350, -0.0496,  0.1493, -0.1740, -0.0495],\n",
       "          [-0.0526, -0.0541, -0.1318,  0.1012,  0.0460]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0523, -0.1491, -0.1451, -0.1036,  0.1483],\n",
       "          [ 0.1593, -0.1866,  0.0807, -0.1690,  0.1838],\n",
       "          [-0.1610, -0.1925, -0.0687, -0.0771, -0.1243],\n",
       "          [-0.1310, -0.1723, -0.1332, -0.1387,  0.1657],\n",
       "          [ 0.0673,  0.1843, -0.1727, -0.0221,  0.0475]]],\n",
       "\n",
       "\n",
       "        [[[-0.1166, -0.2003,  0.0023, -0.1722, -0.1807],\n",
       "          [ 0.1743, -0.1324,  0.1766, -0.1933,  0.0421],\n",
       "          [ 0.0664, -0.1061,  0.1350, -0.1796,  0.0959],\n",
       "          [-0.1545, -0.0873, -0.1879, -0.0640, -0.0866],\n",
       "          [ 0.0948, -0.0368, -0.0359,  0.0155,  0.1910]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0258,  0.1585, -0.0605, -0.1808, -0.1016],\n",
       "          [ 0.1053,  0.1681, -0.0658, -0.1539,  0.0900],\n",
       "          [ 0.1305,  0.0858,  0.0231,  0.2134,  0.0425],\n",
       "          [-0.0314, -0.0843, -0.1412,  0.1217, -0.1732],\n",
       "          [ 0.1602,  0.1400,  0.1269,  0.0630,  0.0827]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0358, -0.1917, -0.1120,  0.2049, -0.1158],\n",
       "          [-0.0061, -0.1651,  0.0857, -0.0805,  0.1048],\n",
       "          [ 0.2273,  0.0632, -0.0317,  0.1479,  0.1683],\n",
       "          [-0.1713, -0.0821,  0.2332,  0.1289,  0.1782],\n",
       "          [-0.2157,  0.0590, -0.1970,  0.1742,  0.1031]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1946,  0.1226, -0.0379,  0.1271,  0.1310],\n",
       "          [ 0.0521,  0.1187,  0.0522, -0.0669,  0.0973],\n",
       "          [ 0.0018,  0.2133, -0.1862,  0.1631, -0.0204],\n",
       "          [ 0.0216, -0.1197, -0.1432, -0.0932,  0.0281],\n",
       "          [ 0.0581,  0.1875,  0.1514, -0.1499, -0.1222]]],\n",
       "\n",
       "\n",
       "        [[[-0.1871,  0.0648, -0.0124,  0.0088, -0.0019],\n",
       "          [ 0.1854, -0.1833, -0.1256, -0.0211, -0.2010],\n",
       "          [-0.1089,  0.0371, -0.1193, -0.1104,  0.0947],\n",
       "          [ 0.0985,  0.0924, -0.1460, -0.1077,  0.1423],\n",
       "          [-0.0402, -0.0136,  0.1983, -0.0739,  0.2171]]],\n",
       "\n",
       "\n",
       "        [[[-0.0755,  0.0595,  0.1407, -0.2088, -0.0349],\n",
       "          [ 0.1426, -0.2198, -0.0138, -0.0581, -0.1707],\n",
       "          [ 0.1371,  0.0209, -0.0216, -0.0762,  0.1080],\n",
       "          [ 0.0938,  0.0840,  0.0349, -0.1152, -0.2312],\n",
       "          [ 0.0812, -0.0777, -0.0365,  0.0252, -0.0585]]],\n",
       "\n",
       "\n",
       "        [[[-0.0267,  0.0877,  0.0532,  0.0864, -0.0226],\n",
       "          [-0.1922,  0.2107,  0.1760,  0.1296, -0.1210],\n",
       "          [ 0.0003,  0.0457,  0.1716, -0.0477,  0.0795],\n",
       "          [-0.0709,  0.0425,  0.1391,  0.1358,  0.0675],\n",
       "          [-0.0902,  0.1600, -0.0145,  0.0142, -0.0277]]],\n",
       "\n",
       "\n",
       "        [[[-0.1873,  0.1259,  0.1329,  0.2461, -0.0952],\n",
       "          [ 0.0395, -0.1034,  0.2120, -0.0411,  0.0971],\n",
       "          [ 0.0161, -0.0857,  0.1236,  0.1347, -0.0836],\n",
       "          [ 0.1901,  0.0359,  0.1728, -0.1359,  0.2418],\n",
       "          [-0.1364, -0.1452,  0.0169,  0.2173,  0.0485]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1180, -0.1290,  0.1516, -0.0370,  0.0083],\n",
       "          [-0.1335, -0.1046, -0.1384, -0.0997, -0.2158],\n",
       "          [ 0.0763, -0.1950,  0.0490, -0.1629, -0.1906],\n",
       "          [-0.1712,  0.1970, -0.1103,  0.1659, -0.1250],\n",
       "          [ 0.0452, -0.0650,  0.0322,  0.1534,  0.0450]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1303, -0.0990,  0.1991, -0.1845, -0.1459],\n",
       "          [ 0.0317, -0.1254,  0.1798,  0.0878, -0.0153],\n",
       "          [-0.0719,  0.2238,  0.0349,  0.0481, -0.0195],\n",
       "          [-0.0012,  0.1490,  0.0777, -0.0401, -0.1298],\n",
       "          [-0.0932, -0.1288, -0.1446,  0.0063, -0.1167]]],\n",
       "\n",
       "\n",
       "        [[[-0.0092,  0.0875,  0.1393, -0.0748, -0.0498],\n",
       "          [ 0.0256, -0.0404,  0.1814,  0.0301,  0.0224],\n",
       "          [-0.0543, -0.1303,  0.1696,  0.0547,  0.1267],\n",
       "          [ 0.1552, -0.0524,  0.0304, -0.0342,  0.0414],\n",
       "          [ 0.0382, -0.1561, -0.1196,  0.0560, -0.0426]]],\n",
       "\n",
       "\n",
       "        [[[-0.1724, -0.0990,  0.0374, -0.0619, -0.0433],\n",
       "          [ 0.0275, -0.0777, -0.1589,  0.0274,  0.1841],\n",
       "          [-0.1549,  0.0612,  0.0661, -0.0064,  0.1761],\n",
       "          [ 0.0730,  0.1787, -0.1636,  0.0290,  0.1941],\n",
       "          [ 0.1649, -0.0269, -0.1209, -0.0951,  0.0009]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2331,  0.2258,  0.2711,  0.2831,  0.1886],\n",
       "          [ 0.0851,  0.1380,  0.1660,  0.0121, -0.0609],\n",
       "          [ 0.0124, -0.0006,  0.0990, -0.1002, -0.1323],\n",
       "          [-0.2045,  0.1496,  0.0704, -0.0544,  0.1827],\n",
       "          [-0.0494, -0.1172,  0.1525, -0.1420,  0.1531]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0690,  0.1429,  0.1603, -0.1402,  0.1642],\n",
       "          [-0.1474,  0.1451,  0.1482, -0.0824,  0.2107],\n",
       "          [ 0.1589, -0.0015,  0.0661,  0.0453, -0.0813],\n",
       "          [-0.0687,  0.0027, -0.1749, -0.0774, -0.0202],\n",
       "          [ 0.0614, -0.0686, -0.1603, -0.1345,  0.1668]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2234, -0.1157,  0.2208,  0.1601,  0.0766],\n",
       "          [-0.0649,  0.0621, -0.0811,  0.1370,  0.0545],\n",
       "          [-0.0655, -0.0088, -0.1224, -0.1125,  0.1804],\n",
       "          [-0.0673, -0.1320,  0.1635, -0.1471,  0.0069],\n",
       "          [-0.1517, -0.2006,  0.0924,  0.1809,  0.1496]]],\n",
       "\n",
       "\n",
       "        [[[-0.0025,  0.1443,  0.0174,  0.0232, -0.1461],\n",
       "          [-0.0627,  0.0182, -0.0657,  0.2101,  0.0402],\n",
       "          [ 0.0805, -0.1545,  0.1720,  0.1626, -0.0382],\n",
       "          [ 0.1976,  0.0982, -0.0351,  0.0006,  0.2624],\n",
       "          [ 0.0055,  0.2480, -0.1111,  0.2202,  0.1832]]],\n",
       "\n",
       "\n",
       "        [[[ 0.3022,  0.2427, -0.1028, -0.0976,  0.1901],\n",
       "          [ 0.1156,  0.1059,  0.0971,  0.1397,  0.1990],\n",
       "          [ 0.1251, -0.1811, -0.0711, -0.0224, -0.0822],\n",
       "          [-0.1492, -0.1688, -0.0462, -0.1098, -0.1868],\n",
       "          [ 0.0509, -0.0629, -0.0965,  0.0679,  0.1303]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0047, -0.1515, -0.1914, -0.0853, -0.0127],\n",
       "          [-0.1211,  0.0330, -0.0010, -0.1034,  0.0737],\n",
       "          [-0.0227,  0.0031, -0.0279,  0.1258,  0.1343],\n",
       "          [-0.0298,  0.0576, -0.1494, -0.0595,  0.1605],\n",
       "          [-0.1548, -0.1767,  0.1322,  0.0387, -0.0155]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2051,  0.2821,  0.2756,  0.0940,  0.1375],\n",
       "          [-0.1472, -0.0164, -0.1026, -0.0379, -0.1374],\n",
       "          [ 0.0634, -0.0502,  0.1892,  0.1894, -0.1735],\n",
       "          [-0.1031,  0.0435, -0.0016, -0.2340,  0.1660],\n",
       "          [ 0.0055, -0.0299,  0.0141,  0.1227, -0.1556]]],\n",
       "\n",
       "\n",
       "        [[[-0.0964,  0.0513, -0.1124, -0.0549, -0.0088],\n",
       "          [ 0.0687, -0.2043,  0.0323, -0.0136, -0.0300],\n",
       "          [ 0.0765, -0.0577, -0.1009, -0.2061,  0.0690],\n",
       "          [ 0.1918, -0.0174, -0.1005,  0.0680,  0.1170],\n",
       "          [-0.1754, -0.0726, -0.0102, -0.1012,  0.0426]]],\n",
       "\n",
       "\n",
       "        [[[-0.0140, -0.1365, -0.2059,  0.0698,  0.0709],\n",
       "          [-0.1770,  0.1414, -0.0972,  0.1095, -0.2063],\n",
       "          [-0.1484,  0.1153, -0.0299, -0.0252, -0.1552],\n",
       "          [-0.1791, -0.0278,  0.1730,  0.1276, -0.0038],\n",
       "          [-0.1634, -0.1221, -0.1781,  0.0704, -0.1323]]],\n",
       "\n",
       "\n",
       "        [[[-0.1827,  0.0661,  0.0416,  0.0590, -0.0038],\n",
       "          [ 0.0322, -0.0074, -0.1251,  0.1627,  0.0389],\n",
       "          [ 0.0722, -0.0004, -0.1702, -0.1344, -0.1649],\n",
       "          [-0.0498, -0.0869,  0.1245,  0.0755,  0.0189],\n",
       "          [ 0.0202, -0.0826,  0.0735,  0.1341,  0.1714]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0910, -0.1679, -0.1144,  0.0028, -0.1899],\n",
       "          [ 0.2008, -0.1236, -0.1167, -0.0485, -0.2017],\n",
       "          [ 0.0075,  0.1203,  0.1463,  0.1102,  0.2008],\n",
       "          [ 0.1607,  0.0987,  0.1726,  0.1617, -0.1414],\n",
       "          [ 0.2336, -0.0606, -0.1322,  0.1067, -0.1322]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1610,  0.1389, -0.1359,  0.1826, -0.0117],\n",
       "          [ 0.1951,  0.2412,  0.2236,  0.1284, -0.0325],\n",
       "          [ 0.1164, -0.1455, -0.0687,  0.0780,  0.0011],\n",
       "          [ 0.0872,  0.0874, -0.0834, -0.1183, -0.1861],\n",
       "          [-0.0710, -0.1174, -0.0541, -0.1283,  0.0560]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1656,  0.0900,  0.1720, -0.1492, -0.1609],\n",
       "          [ 0.0581,  0.1254, -0.0758, -0.0985, -0.0491],\n",
       "          [ 0.0385, -0.0426,  0.1291, -0.1667,  0.0530],\n",
       "          [-0.1847,  0.1817, -0.1761, -0.0490,  0.1052],\n",
       "          [-0.0158, -0.0972, -0.0699, -0.0585,  0.0422]]],\n",
       "\n",
       "\n",
       "        [[[-0.1290,  0.0740, -0.1960,  0.1197, -0.2121],\n",
       "          [-0.1132, -0.0993, -0.0719,  0.1210, -0.1927],\n",
       "          [-0.1608,  0.0266, -0.1144, -0.1234,  0.1449],\n",
       "          [-0.0518,  0.0473, -0.0158,  0.0591, -0.1861],\n",
       "          [-0.1444, -0.1597,  0.1668, -0.0949,  0.0619]]],\n",
       "\n",
       "\n",
       "        [[[-0.0260, -0.1610, -0.0087,  0.1878, -0.0471],\n",
       "          [ 0.0208, -0.1951, -0.0533,  0.0065, -0.1599],\n",
       "          [-0.0274, -0.0910, -0.0093, -0.1148, -0.0837],\n",
       "          [-0.1454,  0.1133, -0.0086,  0.0076, -0.0154],\n",
       "          [ 0.1372,  0.0088, -0.1366,  0.0757, -0.1980]]],\n",
       "\n",
       "\n",
       "        [[[-0.1191, -0.1079, -0.0525, -0.2092, -0.1821],\n",
       "          [-0.2208,  0.1350, -0.1045,  0.0367, -0.0656],\n",
       "          [ 0.2145, -0.1643,  0.1399, -0.0457,  0.0070],\n",
       "          [ 0.0715, -0.0375,  0.2151, -0.1547,  0.1216],\n",
       "          [ 0.1202,  0.0983,  0.1527,  0.1357,  0.1246]]]], requires_grad=True)"
      ]
     },
     "execution_count": 1314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = tmp.param_groups[0]['params'][0]#[0]\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1315,
   "id": "a1c2114c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'old_init': tensor([[[[-0.0152,  0.0386, -0.1857, -0.1259,  0.0449],\n",
       "           [ 0.0759,  0.0275,  0.0646, -0.0870, -0.1584],\n",
       "           [-0.1323,  0.0762,  0.0831,  0.1156,  0.0071],\n",
       "           [ 0.1702,  0.0632,  0.0493,  0.0004,  0.1970],\n",
       "           [ 0.1274, -0.1625, -0.1157,  0.0324,  0.1151]]],\n",
       " \n",
       " \n",
       "         [[[-0.0372,  0.1566,  0.0055,  0.1552,  0.1695],\n",
       "           [-0.1298,  0.1493,  0.0488,  0.1698, -0.0087],\n",
       "           [-0.1463, -0.0848, -0.0599, -0.1671,  0.0008],\n",
       "           [ 0.0395, -0.0326, -0.0025, -0.1419, -0.1266],\n",
       "           [ 0.0562,  0.1891,  0.1971,  0.0958,  0.1444]]],\n",
       " \n",
       " \n",
       "         [[[-0.0024, -0.0018, -0.1555,  0.0271,  0.0251],\n",
       "           [ 0.0881,  0.0476,  0.1273, -0.0854,  0.1086],\n",
       "           [ 0.0024,  0.1491, -0.0637,  0.0970,  0.0140],\n",
       "           [-0.0280, -0.0347,  0.1469, -0.1742, -0.0478],\n",
       "           [-0.0275, -0.0190, -0.0903,  0.1244,  0.0652]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0477, -0.1364, -0.1304, -0.0934,  0.1813],\n",
       "           [ 0.1439, -0.1863,  0.0850, -0.1653,  0.2010],\n",
       "           [-0.1756, -0.1918, -0.0653, -0.0690, -0.1052],\n",
       "           [-0.1325, -0.1737, -0.1309, -0.1341,  0.1612],\n",
       "           [ 0.0635,  0.1815, -0.1735, -0.0317,  0.0330]]],\n",
       " \n",
       " \n",
       "         [[[-0.1073, -0.1985,  0.0102, -0.1667, -0.1728],\n",
       "           [ 0.1718, -0.1388,  0.1744, -0.1905,  0.0423],\n",
       "           [ 0.0574, -0.1110,  0.1356, -0.1725,  0.1017],\n",
       "           [-0.1617, -0.0941, -0.1831, -0.0623, -0.0877],\n",
       "           [ 0.0744, -0.0582, -0.0429,  0.0141,  0.1858]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0595,  0.1826, -0.0433, -0.1696, -0.0952],\n",
       "           [ 0.1020,  0.1536, -0.0785, -0.1544,  0.0721],\n",
       "           [ 0.1073,  0.0370, -0.0289,  0.2015,  0.0399],\n",
       "           [-0.0380, -0.0941, -0.1910,  0.0866, -0.1735],\n",
       "           [ 0.1529,  0.1513,  0.1084,  0.0184,  0.0508]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0230, -0.1833, -0.1282,  0.1670, -0.1448],\n",
       "           [-0.0429, -0.1925,  0.0490, -0.1237,  0.0517],\n",
       "           [ 0.1884,  0.0144, -0.0966,  0.0894,  0.1150],\n",
       "           [-0.1623, -0.0973,  0.1976,  0.0590,  0.1257],\n",
       "           [-0.1922,  0.0811, -0.1917,  0.1439,  0.0464]]],\n",
       " \n",
       " \n",
       "         [[[ 0.1886,  0.1154, -0.0527,  0.1223,  0.1283],\n",
       "           [ 0.0274,  0.0996,  0.0391, -0.0656,  0.0809],\n",
       "           [-0.0360,  0.1981, -0.1783,  0.1901, -0.0071],\n",
       "           [ 0.0025, -0.1209, -0.1283, -0.0763,  0.0456],\n",
       "           [ 0.0456,  0.1856,  0.1684, -0.1269, -0.0896]]],\n",
       " \n",
       " \n",
       "         [[[-0.1803,  0.0844,  0.0176,  0.0300,  0.0136],\n",
       "           [ 0.1944, -0.1746, -0.1113,  0.0008, -0.1678],\n",
       "           [-0.1079,  0.0400, -0.1120, -0.0939,  0.1270],\n",
       "           [ 0.0797,  0.0765, -0.1396, -0.0967,  0.1382],\n",
       "           [-0.0492, -0.0358,  0.1660, -0.0980,  0.1805]]],\n",
       " \n",
       " \n",
       "         [[[-0.0905,  0.0672,  0.1725, -0.1804,  0.0064],\n",
       "           [ 0.1572, -0.1944,  0.0274,  0.0042, -0.1084],\n",
       "           [ 0.1831,  0.0645,  0.0250, -0.0237,  0.1637],\n",
       "           [ 0.1474,  0.1325,  0.0963, -0.0480, -0.1729],\n",
       "           [ 0.1320, -0.0456, -0.0085,  0.0557, -0.0415]]],\n",
       " \n",
       " \n",
       "         [[[-0.0431,  0.0749,  0.0567,  0.0913, -0.0173],\n",
       "           [-0.1927,  0.1948,  0.1634,  0.1316, -0.1074],\n",
       "           [-0.0176,  0.0320,  0.1703, -0.0569,  0.0806],\n",
       "           [-0.0866,  0.0385,  0.1417,  0.1258,  0.0583],\n",
       "           [-0.1024,  0.1575,  0.0050,  0.0361, -0.0351]]],\n",
       " \n",
       " \n",
       "         [[[-0.1710,  0.0959,  0.0859,  0.1934, -0.1124],\n",
       "           [ 0.0389, -0.1318,  0.1484, -0.0972,  0.0633],\n",
       "           [ 0.0069, -0.1007,  0.1038,  0.0829, -0.1468],\n",
       "           [ 0.1986,  0.0411,  0.1542, -0.1694,  0.1883],\n",
       "           [-0.1018, -0.1374,  0.0124,  0.1893,  0.0081]]],\n",
       " \n",
       " \n",
       "         [[[ 0.1251, -0.1244,  0.1572, -0.0280,  0.0052],\n",
       "           [-0.1216, -0.0892, -0.1198, -0.0778, -0.1979],\n",
       "           [ 0.0866, -0.1720,  0.0683, -0.1439, -0.1753],\n",
       "           [-0.1781,  0.1923, -0.1146,  0.1725, -0.1119],\n",
       "           [ 0.0117, -0.0818,  0.0193,  0.1374,  0.0325]]],\n",
       " \n",
       " \n",
       "         [[[ 0.1084, -0.1389,  0.1719, -0.1924, -0.1475],\n",
       "           [ 0.0411, -0.1488,  0.1454,  0.0603, -0.0269],\n",
       "           [-0.0699,  0.1974, -0.0028,  0.0197, -0.0368],\n",
       "           [-0.0175,  0.1351,  0.0616, -0.0519, -0.1321],\n",
       "           [-0.1060, -0.1300, -0.1506,  0.0127, -0.1086]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0201,  0.0771,  0.1142, -0.0948, -0.0440],\n",
       "           [ 0.0595, -0.0318,  0.1449,  0.0009, -0.0008],\n",
       "           [-0.0471, -0.1449,  0.1396,  0.0340,  0.1216],\n",
       "           [ 0.1542, -0.0669,  0.0037, -0.0425,  0.0316],\n",
       "           [ 0.0299, -0.1717, -0.1425,  0.0301, -0.0656]]],\n",
       " \n",
       " \n",
       "         [[[-0.1616, -0.1006,  0.0118, -0.0837, -0.0611],\n",
       "           [ 0.0457, -0.0545, -0.1394,  0.0267,  0.1666],\n",
       "           [-0.1407,  0.0643,  0.0728, -0.0080,  0.1470],\n",
       "           [ 0.0779,  0.1736, -0.1783,  0.0114,  0.1641],\n",
       "           [ 0.1532, -0.0358, -0.1317, -0.1161, -0.0059]]],\n",
       " \n",
       " \n",
       "         [[[ 0.1487,  0.1396,  0.1898,  0.2061,  0.1425],\n",
       "           [ 0.0374,  0.0989,  0.1337, -0.0301, -0.0937],\n",
       "           [ 0.0058, -0.0086,  0.1076, -0.0974, -0.1181],\n",
       "           [-0.1817,  0.1607,  0.1002, -0.0092,  0.1923],\n",
       "           [-0.0049, -0.0880,  0.1694, -0.1278,  0.1519]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0508,  0.1124,  0.1443, -0.1432,  0.1463],\n",
       "           [-0.1509,  0.1322,  0.1359, -0.0763,  0.1929],\n",
       "           [ 0.1529, -0.0030,  0.0661,  0.0419, -0.0780],\n",
       "           [-0.0642,  0.0051, -0.1784, -0.0909, -0.0187],\n",
       "           [ 0.0708, -0.0608, -0.1534, -0.1253,  0.1936]]],\n",
       " \n",
       " \n",
       "         [[[ 0.1965, -0.1381,  0.1828,  0.1172,  0.0639],\n",
       "           [-0.0640,  0.0604, -0.0892,  0.1004,  0.0318],\n",
       "           [-0.0540,  0.0097, -0.1111, -0.1160,  0.1498],\n",
       "           [-0.0576, -0.1234,  0.1718, -0.1387, -0.0076],\n",
       "           [-0.1519, -0.1910,  0.0961,  0.1909,  0.1467]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0140,  0.1391,  0.0101,  0.0170, -0.1416],\n",
       "           [-0.0431,  0.0249, -0.0728,  0.1794,  0.0083],\n",
       "           [ 0.0748, -0.1763,  0.1295,  0.1052, -0.1135],\n",
       "           [ 0.1456,  0.0483, -0.0860, -0.0580,  0.1848],\n",
       "           [-0.0545,  0.1899, -0.1692,  0.1652,  0.1359]]],\n",
       " \n",
       " \n",
       "         [[[ 0.1929,  0.1725, -0.1544, -0.1523,  0.1434],\n",
       "           [ 0.0150,  0.0705,  0.0660,  0.1043,  0.1766],\n",
       "           [ 0.0908, -0.1699, -0.0748, -0.0482, -0.0769],\n",
       "           [-0.1621, -0.1562, -0.0464, -0.1193, -0.1743],\n",
       "           [ 0.0320, -0.0532, -0.0827,  0.0716,  0.1355]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0324, -0.1384, -0.1822, -0.0865, -0.0304],\n",
       "           [-0.0874,  0.0720,  0.0351, -0.0851,  0.0638],\n",
       "           [ 0.0008,  0.0268, -0.0026,  0.1466,  0.1324],\n",
       "           [-0.0324,  0.0514, -0.1477, -0.0563,  0.1445],\n",
       "           [-0.1676, -0.1892,  0.1035,  0.0217, -0.0269]]],\n",
       " \n",
       " \n",
       "         [[[ 0.1566,  0.1982,  0.2012,  0.0467,  0.1341],\n",
       "           [-0.1758, -0.0684, -0.1491, -0.0833, -0.1503],\n",
       "           [ 0.0382, -0.0608,  0.1872,  0.1754, -0.1834],\n",
       "           [-0.0984,  0.0626,  0.0218, -0.1976,  0.1927],\n",
       "           [ 0.0284, -0.0061,  0.0373,  0.1648, -0.1281]]],\n",
       " \n",
       " \n",
       "         [[[-0.0862,  0.0757, -0.0994, -0.0675, -0.0109],\n",
       "           [ 0.0647, -0.1737,  0.0891,  0.0442,  0.0137],\n",
       "           [ 0.0670, -0.0391, -0.0472, -0.1127,  0.1518],\n",
       "           [ 0.1914, -0.0186, -0.0848,  0.1347,  0.1913],\n",
       "           [-0.1766, -0.0806, -0.0013, -0.0791,  0.0682]]],\n",
       " \n",
       " \n",
       "         [[[-0.0282, -0.1343, -0.1901,  0.0987,  0.0981],\n",
       "           [-0.1866,  0.1324, -0.0892,  0.1291, -0.1765],\n",
       "           [-0.1439,  0.1072, -0.0331, -0.0277, -0.1493],\n",
       "           [-0.1698, -0.0214,  0.1714,  0.1197, -0.0188],\n",
       "           [-0.1590, -0.1142, -0.1661,  0.0675, -0.1524]]],\n",
       " \n",
       " \n",
       "         [[[-0.1539,  0.1004,  0.0761,  0.0871,  0.0096],\n",
       "           [ 0.0486,  0.0213, -0.0878,  0.2010,  0.0486],\n",
       "           [ 0.0754,  0.0430, -0.1219, -0.1061, -0.1713],\n",
       "           [-0.0520, -0.0800,  0.1372,  0.0812,  0.0217],\n",
       "           [-0.0214, -0.1069,  0.0513,  0.1281,  0.1611]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0779, -0.1636, -0.0921,  0.0246, -0.1633],\n",
       "           [ 0.1965, -0.1053, -0.0969, -0.0293, -0.1930],\n",
       "           [-0.0015,  0.1042,  0.1207,  0.0950,  0.1729],\n",
       "           [ 0.1402,  0.0450,  0.1160,  0.1089, -0.1816],\n",
       "           [ 0.2011, -0.1109, -0.1863,  0.0595, -0.1705]]],\n",
       " \n",
       " \n",
       "         [[[ 0.1029,  0.0774, -0.1909,  0.1519, -0.0276],\n",
       "           [ 0.1548,  0.1980,  0.1968,  0.1334, -0.0076],\n",
       "           [ 0.0989, -0.1721, -0.0771,  0.0728,  0.0091],\n",
       "           [ 0.0727,  0.0696, -0.0844, -0.1031, -0.1809],\n",
       "           [-0.0808, -0.1465, -0.0490, -0.1040,  0.0738]]],\n",
       " \n",
       " \n",
       "         [[[ 0.1688,  0.0992,  0.1947, -0.1457, -0.1433],\n",
       "           [ 0.0727,  0.1356, -0.0551, -0.1076, -0.0527],\n",
       "           [ 0.0629, -0.0227,  0.1369, -0.1776,  0.0321],\n",
       "           [-0.1736,  0.1888, -0.1870, -0.0627,  0.0882],\n",
       "           [-0.0228, -0.1034, -0.0808, -0.0636,  0.0443]]],\n",
       " \n",
       " \n",
       "         [[[-0.1285,  0.0774, -0.1589,  0.1692, -0.1979],\n",
       "           [-0.1155, -0.1007, -0.0506,  0.1736, -0.1556],\n",
       "           [-0.1636,  0.0238, -0.1127, -0.0875,  0.1941],\n",
       "           [-0.0541,  0.0484, -0.0071,  0.0744, -0.1486],\n",
       "           [-0.1375, -0.1526,  0.1776, -0.0893,  0.0776]]],\n",
       " \n",
       " \n",
       "         [[[-0.0146, -0.1678, -0.0129,  0.1797, -0.0573],\n",
       "           [ 0.0424, -0.1938, -0.0580, -0.0025, -0.1767],\n",
       "           [ 0.0003, -0.0705, -0.0041, -0.1216, -0.0830],\n",
       "           [-0.1024,  0.1557,  0.0191,  0.0182, -0.0061],\n",
       "           [ 0.1597,  0.0347, -0.1171,  0.0839, -0.1919]]],\n",
       " \n",
       " \n",
       "         [[[-0.0687, -0.0541, -0.0125, -0.1856, -0.1628],\n",
       "           [-0.1945,  0.1688, -0.0685,  0.0450, -0.0609],\n",
       "           [ 0.1902, -0.1606,  0.1492, -0.0622, -0.0159],\n",
       "           [ 0.0233, -0.0643,  0.1896, -0.1938,  0.0679],\n",
       "           [ 0.0488,  0.0222,  0.0765,  0.0598,  0.0466]]]])}"
      ]
     },
     "execution_count": 1315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.state[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1312,
   "id": "03f04e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 1312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.state[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe74b17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1302,
   "id": "d90d9f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0135,  0.0834, -0.1635, -0.1240,  0.0150],\n",
       "         [ 0.1076,  0.0894,  0.1221, -0.0690, -0.1478],\n",
       "         [-0.1232,  0.1202,  0.1382,  0.1460,  0.0215],\n",
       "         [ 0.1577,  0.0469,  0.0594,  0.0215,  0.1838],\n",
       "         [ 0.1121, -0.2022, -0.1584,  0.0095,  0.0925]]],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 1302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.param_groups[0]['params'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1304,
   "id": "096f12e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in tmp.state.items():\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1308,
   "id": "f8137e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[ 0.0135,  0.0834, -0.1635, -0.1240,  0.0150],\n",
       "          [ 0.1076,  0.0894,  0.1221, -0.0690, -0.1478],\n",
       "          [-0.1232,  0.1202,  0.1382,  0.1460,  0.0215],\n",
       "          [ 0.1577,  0.0469,  0.0594,  0.0215,  0.1838],\n",
       "          [ 0.1121, -0.2022, -0.1584,  0.0095,  0.0925]]],\n",
       "\n",
       "\n",
       "        [[[-0.0625,  0.1570,  0.0061,  0.1393,  0.1369],\n",
       "          [-0.1410,  0.1502,  0.0581,  0.1548, -0.0272],\n",
       "          [-0.1242, -0.0803, -0.0600, -0.1837, -0.0414],\n",
       "          [ 0.0703, -0.0023,  0.0040, -0.1584, -0.1531],\n",
       "          [ 0.0853,  0.2344,  0.2251,  0.0915,  0.1444]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0254,  0.0159, -0.1718,  0.0243,  0.0359],\n",
       "          [ 0.1028,  0.0580,  0.1281, -0.1112,  0.0933],\n",
       "          [ 0.0104,  0.1602, -0.0586,  0.0608, -0.0146],\n",
       "          [-0.0350, -0.0496,  0.1493, -0.1740, -0.0495],\n",
       "          [-0.0526, -0.0541, -0.1318,  0.1012,  0.0460]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0523, -0.1491, -0.1451, -0.1036,  0.1483],\n",
       "          [ 0.1593, -0.1866,  0.0807, -0.1690,  0.1838],\n",
       "          [-0.1610, -0.1925, -0.0687, -0.0771, -0.1243],\n",
       "          [-0.1310, -0.1723, -0.1332, -0.1387,  0.1657],\n",
       "          [ 0.0673,  0.1843, -0.1727, -0.0221,  0.0475]]],\n",
       "\n",
       "\n",
       "        [[[-0.1166, -0.2003,  0.0023, -0.1722, -0.1807],\n",
       "          [ 0.1743, -0.1324,  0.1766, -0.1933,  0.0421],\n",
       "          [ 0.0664, -0.1061,  0.1350, -0.1796,  0.0959],\n",
       "          [-0.1545, -0.0873, -0.1879, -0.0640, -0.0866],\n",
       "          [ 0.0948, -0.0368, -0.0359,  0.0155,  0.1910]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0258,  0.1585, -0.0605, -0.1808, -0.1016],\n",
       "          [ 0.1053,  0.1681, -0.0658, -0.1539,  0.0900],\n",
       "          [ 0.1305,  0.0858,  0.0231,  0.2134,  0.0425],\n",
       "          [-0.0314, -0.0843, -0.1412,  0.1217, -0.1732],\n",
       "          [ 0.1602,  0.1400,  0.1269,  0.0630,  0.0827]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0358, -0.1917, -0.1120,  0.2049, -0.1158],\n",
       "          [-0.0061, -0.1651,  0.0857, -0.0805,  0.1048],\n",
       "          [ 0.2273,  0.0632, -0.0317,  0.1479,  0.1683],\n",
       "          [-0.1713, -0.0821,  0.2332,  0.1289,  0.1782],\n",
       "          [-0.2157,  0.0590, -0.1970,  0.1742,  0.1031]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1946,  0.1226, -0.0379,  0.1271,  0.1310],\n",
       "          [ 0.0521,  0.1187,  0.0522, -0.0669,  0.0973],\n",
       "          [ 0.0018,  0.2133, -0.1862,  0.1631, -0.0204],\n",
       "          [ 0.0216, -0.1197, -0.1432, -0.0932,  0.0281],\n",
       "          [ 0.0581,  0.1875,  0.1514, -0.1499, -0.1222]]],\n",
       "\n",
       "\n",
       "        [[[-0.1871,  0.0648, -0.0124,  0.0088, -0.0019],\n",
       "          [ 0.1854, -0.1833, -0.1256, -0.0211, -0.2010],\n",
       "          [-0.1089,  0.0371, -0.1193, -0.1104,  0.0947],\n",
       "          [ 0.0985,  0.0924, -0.1460, -0.1077,  0.1423],\n",
       "          [-0.0402, -0.0136,  0.1983, -0.0739,  0.2171]]],\n",
       "\n",
       "\n",
       "        [[[-0.0755,  0.0595,  0.1407, -0.2088, -0.0349],\n",
       "          [ 0.1426, -0.2198, -0.0138, -0.0581, -0.1707],\n",
       "          [ 0.1371,  0.0209, -0.0216, -0.0762,  0.1080],\n",
       "          [ 0.0938,  0.0840,  0.0349, -0.1152, -0.2312],\n",
       "          [ 0.0812, -0.0777, -0.0365,  0.0252, -0.0585]]],\n",
       "\n",
       "\n",
       "        [[[-0.0267,  0.0877,  0.0532,  0.0864, -0.0226],\n",
       "          [-0.1922,  0.2107,  0.1760,  0.1296, -0.1210],\n",
       "          [ 0.0003,  0.0457,  0.1716, -0.0477,  0.0795],\n",
       "          [-0.0709,  0.0425,  0.1391,  0.1358,  0.0675],\n",
       "          [-0.0902,  0.1600, -0.0145,  0.0142, -0.0277]]],\n",
       "\n",
       "\n",
       "        [[[-0.1873,  0.1259,  0.1329,  0.2461, -0.0952],\n",
       "          [ 0.0395, -0.1034,  0.2120, -0.0411,  0.0971],\n",
       "          [ 0.0161, -0.0857,  0.1236,  0.1347, -0.0836],\n",
       "          [ 0.1901,  0.0359,  0.1728, -0.1359,  0.2418],\n",
       "          [-0.1364, -0.1452,  0.0169,  0.2173,  0.0485]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1180, -0.1290,  0.1516, -0.0370,  0.0083],\n",
       "          [-0.1335, -0.1046, -0.1384, -0.0997, -0.2158],\n",
       "          [ 0.0763, -0.1950,  0.0490, -0.1629, -0.1906],\n",
       "          [-0.1712,  0.1970, -0.1103,  0.1659, -0.1250],\n",
       "          [ 0.0452, -0.0650,  0.0322,  0.1534,  0.0450]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1303, -0.0990,  0.1991, -0.1845, -0.1459],\n",
       "          [ 0.0317, -0.1254,  0.1798,  0.0878, -0.0153],\n",
       "          [-0.0719,  0.2238,  0.0349,  0.0481, -0.0195],\n",
       "          [-0.0012,  0.1490,  0.0777, -0.0401, -0.1298],\n",
       "          [-0.0932, -0.1288, -0.1446,  0.0063, -0.1167]]],\n",
       "\n",
       "\n",
       "        [[[-0.0092,  0.0875,  0.1393, -0.0748, -0.0498],\n",
       "          [ 0.0256, -0.0404,  0.1814,  0.0301,  0.0224],\n",
       "          [-0.0543, -0.1303,  0.1696,  0.0547,  0.1267],\n",
       "          [ 0.1552, -0.0524,  0.0304, -0.0342,  0.0414],\n",
       "          [ 0.0382, -0.1561, -0.1196,  0.0560, -0.0426]]],\n",
       "\n",
       "\n",
       "        [[[-0.1724, -0.0990,  0.0374, -0.0619, -0.0433],\n",
       "          [ 0.0275, -0.0777, -0.1589,  0.0274,  0.1841],\n",
       "          [-0.1549,  0.0612,  0.0661, -0.0064,  0.1761],\n",
       "          [ 0.0730,  0.1787, -0.1636,  0.0290,  0.1941],\n",
       "          [ 0.1649, -0.0269, -0.1209, -0.0951,  0.0009]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2331,  0.2258,  0.2711,  0.2831,  0.1886],\n",
       "          [ 0.0851,  0.1380,  0.1660,  0.0121, -0.0609],\n",
       "          [ 0.0124, -0.0006,  0.0990, -0.1002, -0.1323],\n",
       "          [-0.2045,  0.1496,  0.0704, -0.0544,  0.1827],\n",
       "          [-0.0494, -0.1172,  0.1525, -0.1420,  0.1531]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0690,  0.1429,  0.1603, -0.1402,  0.1642],\n",
       "          [-0.1474,  0.1451,  0.1482, -0.0824,  0.2107],\n",
       "          [ 0.1589, -0.0015,  0.0661,  0.0453, -0.0813],\n",
       "          [-0.0687,  0.0027, -0.1749, -0.0774, -0.0202],\n",
       "          [ 0.0614, -0.0686, -0.1603, -0.1345,  0.1668]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2234, -0.1157,  0.2208,  0.1601,  0.0766],\n",
       "          [-0.0649,  0.0621, -0.0811,  0.1370,  0.0545],\n",
       "          [-0.0655, -0.0088, -0.1224, -0.1125,  0.1804],\n",
       "          [-0.0673, -0.1320,  0.1635, -0.1471,  0.0069],\n",
       "          [-0.1517, -0.2006,  0.0924,  0.1809,  0.1496]]],\n",
       "\n",
       "\n",
       "        [[[-0.0025,  0.1443,  0.0174,  0.0232, -0.1461],\n",
       "          [-0.0627,  0.0182, -0.0657,  0.2101,  0.0402],\n",
       "          [ 0.0805, -0.1545,  0.1720,  0.1626, -0.0382],\n",
       "          [ 0.1976,  0.0982, -0.0351,  0.0006,  0.2624],\n",
       "          [ 0.0055,  0.2480, -0.1111,  0.2202,  0.1832]]],\n",
       "\n",
       "\n",
       "        [[[ 0.3022,  0.2427, -0.1028, -0.0976,  0.1901],\n",
       "          [ 0.1156,  0.1059,  0.0971,  0.1397,  0.1990],\n",
       "          [ 0.1251, -0.1811, -0.0711, -0.0224, -0.0822],\n",
       "          [-0.1492, -0.1688, -0.0462, -0.1098, -0.1868],\n",
       "          [ 0.0509, -0.0629, -0.0965,  0.0679,  0.1303]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0047, -0.1515, -0.1914, -0.0853, -0.0127],\n",
       "          [-0.1211,  0.0330, -0.0010, -0.1034,  0.0737],\n",
       "          [-0.0227,  0.0031, -0.0279,  0.1258,  0.1343],\n",
       "          [-0.0298,  0.0576, -0.1494, -0.0595,  0.1605],\n",
       "          [-0.1548, -0.1767,  0.1322,  0.0387, -0.0155]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2051,  0.2821,  0.2756,  0.0940,  0.1375],\n",
       "          [-0.1472, -0.0164, -0.1026, -0.0379, -0.1374],\n",
       "          [ 0.0634, -0.0502,  0.1892,  0.1894, -0.1735],\n",
       "          [-0.1031,  0.0435, -0.0016, -0.2340,  0.1660],\n",
       "          [ 0.0055, -0.0299,  0.0141,  0.1227, -0.1556]]],\n",
       "\n",
       "\n",
       "        [[[-0.0964,  0.0513, -0.1124, -0.0549, -0.0088],\n",
       "          [ 0.0687, -0.2043,  0.0323, -0.0136, -0.0300],\n",
       "          [ 0.0765, -0.0577, -0.1009, -0.2061,  0.0690],\n",
       "          [ 0.1918, -0.0174, -0.1005,  0.0680,  0.1170],\n",
       "          [-0.1754, -0.0726, -0.0102, -0.1012,  0.0426]]],\n",
       "\n",
       "\n",
       "        [[[-0.0140, -0.1365, -0.2059,  0.0698,  0.0709],\n",
       "          [-0.1770,  0.1414, -0.0972,  0.1095, -0.2063],\n",
       "          [-0.1484,  0.1153, -0.0299, -0.0252, -0.1552],\n",
       "          [-0.1791, -0.0278,  0.1730,  0.1276, -0.0038],\n",
       "          [-0.1634, -0.1221, -0.1781,  0.0704, -0.1323]]],\n",
       "\n",
       "\n",
       "        [[[-0.1827,  0.0661,  0.0416,  0.0590, -0.0038],\n",
       "          [ 0.0322, -0.0074, -0.1251,  0.1627,  0.0389],\n",
       "          [ 0.0722, -0.0004, -0.1702, -0.1344, -0.1649],\n",
       "          [-0.0498, -0.0869,  0.1245,  0.0755,  0.0189],\n",
       "          [ 0.0202, -0.0826,  0.0735,  0.1341,  0.1714]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0910, -0.1679, -0.1144,  0.0028, -0.1899],\n",
       "          [ 0.2008, -0.1236, -0.1167, -0.0485, -0.2017],\n",
       "          [ 0.0075,  0.1203,  0.1463,  0.1102,  0.2008],\n",
       "          [ 0.1607,  0.0987,  0.1726,  0.1617, -0.1414],\n",
       "          [ 0.2336, -0.0606, -0.1322,  0.1067, -0.1322]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1610,  0.1389, -0.1359,  0.1826, -0.0117],\n",
       "          [ 0.1951,  0.2412,  0.2236,  0.1284, -0.0325],\n",
       "          [ 0.1164, -0.1455, -0.0687,  0.0780,  0.0011],\n",
       "          [ 0.0872,  0.0874, -0.0834, -0.1183, -0.1861],\n",
       "          [-0.0710, -0.1174, -0.0541, -0.1283,  0.0560]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1656,  0.0900,  0.1720, -0.1492, -0.1609],\n",
       "          [ 0.0581,  0.1254, -0.0758, -0.0985, -0.0491],\n",
       "          [ 0.0385, -0.0426,  0.1291, -0.1667,  0.0530],\n",
       "          [-0.1847,  0.1817, -0.1761, -0.0490,  0.1052],\n",
       "          [-0.0158, -0.0972, -0.0699, -0.0585,  0.0422]]],\n",
       "\n",
       "\n",
       "        [[[-0.1290,  0.0740, -0.1960,  0.1197, -0.2121],\n",
       "          [-0.1132, -0.0993, -0.0719,  0.1210, -0.1927],\n",
       "          [-0.1608,  0.0266, -0.1144, -0.1234,  0.1449],\n",
       "          [-0.0518,  0.0473, -0.0158,  0.0591, -0.1861],\n",
       "          [-0.1444, -0.1597,  0.1668, -0.0949,  0.0619]]],\n",
       "\n",
       "\n",
       "        [[[-0.0260, -0.1610, -0.0087,  0.1878, -0.0471],\n",
       "          [ 0.0208, -0.1951, -0.0533,  0.0065, -0.1599],\n",
       "          [-0.0274, -0.0910, -0.0093, -0.1148, -0.0837],\n",
       "          [-0.1454,  0.1133, -0.0086,  0.0076, -0.0154],\n",
       "          [ 0.1372,  0.0088, -0.1366,  0.0757, -0.1980]]],\n",
       "\n",
       "\n",
       "        [[[-0.1191, -0.1079, -0.0525, -0.2092, -0.1821],\n",
       "          [-0.2208,  0.1350, -0.1045,  0.0367, -0.0656],\n",
       "          [ 0.2145, -0.1643,  0.1399, -0.0457,  0.0070],\n",
       "          [ 0.0715, -0.0375,  0.2151, -0.1547,  0.1216],\n",
       "          [ 0.1202,  0.0983,  0.1527,  0.1357,  0.1246]]]], requires_grad=True)"
      ]
     },
     "execution_count": 1308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1301,
   "id": "9ebe037a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.param_groups[0]['params'][0][0] in tmp.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1290,
   "id": "56955b75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'params': [Parameter containing:\n",
       "  tensor([[[[ 0.0135,  0.0834, -0.1635, -0.1240,  0.0150],\n",
       "            [ 0.1076,  0.0894,  0.1221, -0.0690, -0.1478],\n",
       "            [-0.1232,  0.1202,  0.1382,  0.1460,  0.0215],\n",
       "            [ 0.1577,  0.0469,  0.0594,  0.0215,  0.1838],\n",
       "            [ 0.1121, -0.2022, -0.1584,  0.0095,  0.0925]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.0625,  0.1570,  0.0061,  0.1393,  0.1369],\n",
       "            [-0.1410,  0.1502,  0.0581,  0.1548, -0.0272],\n",
       "            [-0.1242, -0.0803, -0.0600, -0.1837, -0.0414],\n",
       "            [ 0.0703, -0.0023,  0.0040, -0.1584, -0.1531],\n",
       "            [ 0.0853,  0.2344,  0.2251,  0.0915,  0.1444]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.0254,  0.0159, -0.1718,  0.0243,  0.0359],\n",
       "            [ 0.1028,  0.0580,  0.1281, -0.1112,  0.0933],\n",
       "            [ 0.0104,  0.1602, -0.0586,  0.0608, -0.0146],\n",
       "            [-0.0350, -0.0496,  0.1493, -0.1740, -0.0495],\n",
       "            [-0.0526, -0.0541, -0.1318,  0.1012,  0.0460]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.0523, -0.1491, -0.1451, -0.1036,  0.1483],\n",
       "            [ 0.1593, -0.1866,  0.0807, -0.1690,  0.1838],\n",
       "            [-0.1610, -0.1925, -0.0687, -0.0771, -0.1243],\n",
       "            [-0.1310, -0.1723, -0.1332, -0.1387,  0.1657],\n",
       "            [ 0.0673,  0.1843, -0.1727, -0.0221,  0.0475]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.1166, -0.2003,  0.0023, -0.1722, -0.1807],\n",
       "            [ 0.1743, -0.1324,  0.1766, -0.1933,  0.0421],\n",
       "            [ 0.0664, -0.1061,  0.1350, -0.1796,  0.0959],\n",
       "            [-0.1545, -0.0873, -0.1879, -0.0640, -0.0866],\n",
       "            [ 0.0948, -0.0368, -0.0359,  0.0155,  0.1910]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.0258,  0.1585, -0.0605, -0.1808, -0.1016],\n",
       "            [ 0.1053,  0.1681, -0.0658, -0.1539,  0.0900],\n",
       "            [ 0.1305,  0.0858,  0.0231,  0.2134,  0.0425],\n",
       "            [-0.0314, -0.0843, -0.1412,  0.1217, -0.1732],\n",
       "            [ 0.1602,  0.1400,  0.1269,  0.0630,  0.0827]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.0358, -0.1917, -0.1120,  0.2049, -0.1158],\n",
       "            [-0.0061, -0.1651,  0.0857, -0.0805,  0.1048],\n",
       "            [ 0.2273,  0.0632, -0.0317,  0.1479,  0.1683],\n",
       "            [-0.1713, -0.0821,  0.2332,  0.1289,  0.1782],\n",
       "            [-0.2157,  0.0590, -0.1970,  0.1742,  0.1031]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.1946,  0.1226, -0.0379,  0.1271,  0.1310],\n",
       "            [ 0.0521,  0.1187,  0.0522, -0.0669,  0.0973],\n",
       "            [ 0.0018,  0.2133, -0.1862,  0.1631, -0.0204],\n",
       "            [ 0.0216, -0.1197, -0.1432, -0.0932,  0.0281],\n",
       "            [ 0.0581,  0.1875,  0.1514, -0.1499, -0.1222]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.1871,  0.0648, -0.0124,  0.0088, -0.0019],\n",
       "            [ 0.1854, -0.1833, -0.1256, -0.0211, -0.2010],\n",
       "            [-0.1089,  0.0371, -0.1193, -0.1104,  0.0947],\n",
       "            [ 0.0985,  0.0924, -0.1460, -0.1077,  0.1423],\n",
       "            [-0.0402, -0.0136,  0.1983, -0.0739,  0.2171]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.0755,  0.0595,  0.1407, -0.2088, -0.0349],\n",
       "            [ 0.1426, -0.2198, -0.0138, -0.0581, -0.1707],\n",
       "            [ 0.1371,  0.0209, -0.0216, -0.0762,  0.1080],\n",
       "            [ 0.0938,  0.0840,  0.0349, -0.1152, -0.2312],\n",
       "            [ 0.0812, -0.0777, -0.0365,  0.0252, -0.0585]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.0267,  0.0877,  0.0532,  0.0864, -0.0226],\n",
       "            [-0.1922,  0.2107,  0.1760,  0.1296, -0.1210],\n",
       "            [ 0.0003,  0.0457,  0.1716, -0.0477,  0.0795],\n",
       "            [-0.0709,  0.0425,  0.1391,  0.1358,  0.0675],\n",
       "            [-0.0902,  0.1600, -0.0145,  0.0142, -0.0277]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.1873,  0.1259,  0.1329,  0.2461, -0.0952],\n",
       "            [ 0.0395, -0.1034,  0.2120, -0.0411,  0.0971],\n",
       "            [ 0.0161, -0.0857,  0.1236,  0.1347, -0.0836],\n",
       "            [ 0.1901,  0.0359,  0.1728, -0.1359,  0.2418],\n",
       "            [-0.1364, -0.1452,  0.0169,  0.2173,  0.0485]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.1180, -0.1290,  0.1516, -0.0370,  0.0083],\n",
       "            [-0.1335, -0.1046, -0.1384, -0.0997, -0.2158],\n",
       "            [ 0.0763, -0.1950,  0.0490, -0.1629, -0.1906],\n",
       "            [-0.1712,  0.1970, -0.1103,  0.1659, -0.1250],\n",
       "            [ 0.0452, -0.0650,  0.0322,  0.1534,  0.0450]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.1303, -0.0990,  0.1991, -0.1845, -0.1459],\n",
       "            [ 0.0317, -0.1254,  0.1798,  0.0878, -0.0153],\n",
       "            [-0.0719,  0.2238,  0.0349,  0.0481, -0.0195],\n",
       "            [-0.0012,  0.1490,  0.0777, -0.0401, -0.1298],\n",
       "            [-0.0932, -0.1288, -0.1446,  0.0063, -0.1167]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.0092,  0.0875,  0.1393, -0.0748, -0.0498],\n",
       "            [ 0.0256, -0.0404,  0.1814,  0.0301,  0.0224],\n",
       "            [-0.0543, -0.1303,  0.1696,  0.0547,  0.1267],\n",
       "            [ 0.1552, -0.0524,  0.0304, -0.0342,  0.0414],\n",
       "            [ 0.0382, -0.1561, -0.1196,  0.0560, -0.0426]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.1724, -0.0990,  0.0374, -0.0619, -0.0433],\n",
       "            [ 0.0275, -0.0777, -0.1589,  0.0274,  0.1841],\n",
       "            [-0.1549,  0.0612,  0.0661, -0.0064,  0.1761],\n",
       "            [ 0.0730,  0.1787, -0.1636,  0.0290,  0.1941],\n",
       "            [ 0.1649, -0.0269, -0.1209, -0.0951,  0.0009]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.2331,  0.2258,  0.2711,  0.2831,  0.1886],\n",
       "            [ 0.0851,  0.1380,  0.1660,  0.0121, -0.0609],\n",
       "            [ 0.0124, -0.0006,  0.0990, -0.1002, -0.1323],\n",
       "            [-0.2045,  0.1496,  0.0704, -0.0544,  0.1827],\n",
       "            [-0.0494, -0.1172,  0.1525, -0.1420,  0.1531]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.0690,  0.1429,  0.1603, -0.1402,  0.1642],\n",
       "            [-0.1474,  0.1451,  0.1482, -0.0824,  0.2107],\n",
       "            [ 0.1589, -0.0015,  0.0661,  0.0453, -0.0813],\n",
       "            [-0.0687,  0.0027, -0.1749, -0.0774, -0.0202],\n",
       "            [ 0.0614, -0.0686, -0.1603, -0.1345,  0.1668]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.2234, -0.1157,  0.2208,  0.1601,  0.0766],\n",
       "            [-0.0649,  0.0621, -0.0811,  0.1370,  0.0545],\n",
       "            [-0.0655, -0.0088, -0.1224, -0.1125,  0.1804],\n",
       "            [-0.0673, -0.1320,  0.1635, -0.1471,  0.0069],\n",
       "            [-0.1517, -0.2006,  0.0924,  0.1809,  0.1496]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.0025,  0.1443,  0.0174,  0.0232, -0.1461],\n",
       "            [-0.0627,  0.0182, -0.0657,  0.2101,  0.0402],\n",
       "            [ 0.0805, -0.1545,  0.1720,  0.1626, -0.0382],\n",
       "            [ 0.1976,  0.0982, -0.0351,  0.0006,  0.2624],\n",
       "            [ 0.0055,  0.2480, -0.1111,  0.2202,  0.1832]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.3022,  0.2427, -0.1028, -0.0976,  0.1901],\n",
       "            [ 0.1156,  0.1059,  0.0971,  0.1397,  0.1990],\n",
       "            [ 0.1251, -0.1811, -0.0711, -0.0224, -0.0822],\n",
       "            [-0.1492, -0.1688, -0.0462, -0.1098, -0.1868],\n",
       "            [ 0.0509, -0.0629, -0.0965,  0.0679,  0.1303]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.0047, -0.1515, -0.1914, -0.0853, -0.0127],\n",
       "            [-0.1211,  0.0330, -0.0010, -0.1034,  0.0737],\n",
       "            [-0.0227,  0.0031, -0.0279,  0.1258,  0.1343],\n",
       "            [-0.0298,  0.0576, -0.1494, -0.0595,  0.1605],\n",
       "            [-0.1548, -0.1767,  0.1322,  0.0387, -0.0155]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.2051,  0.2821,  0.2756,  0.0940,  0.1375],\n",
       "            [-0.1472, -0.0164, -0.1026, -0.0379, -0.1374],\n",
       "            [ 0.0634, -0.0502,  0.1892,  0.1894, -0.1735],\n",
       "            [-0.1031,  0.0435, -0.0016, -0.2340,  0.1660],\n",
       "            [ 0.0055, -0.0299,  0.0141,  0.1227, -0.1556]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.0964,  0.0513, -0.1124, -0.0549, -0.0088],\n",
       "            [ 0.0687, -0.2043,  0.0323, -0.0136, -0.0300],\n",
       "            [ 0.0765, -0.0577, -0.1009, -0.2061,  0.0690],\n",
       "            [ 0.1918, -0.0174, -0.1005,  0.0680,  0.1170],\n",
       "            [-0.1754, -0.0726, -0.0102, -0.1012,  0.0426]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.0140, -0.1365, -0.2059,  0.0698,  0.0709],\n",
       "            [-0.1770,  0.1414, -0.0972,  0.1095, -0.2063],\n",
       "            [-0.1484,  0.1153, -0.0299, -0.0252, -0.1552],\n",
       "            [-0.1791, -0.0278,  0.1730,  0.1276, -0.0038],\n",
       "            [-0.1634, -0.1221, -0.1781,  0.0704, -0.1323]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.1827,  0.0661,  0.0416,  0.0590, -0.0038],\n",
       "            [ 0.0322, -0.0074, -0.1251,  0.1627,  0.0389],\n",
       "            [ 0.0722, -0.0004, -0.1702, -0.1344, -0.1649],\n",
       "            [-0.0498, -0.0869,  0.1245,  0.0755,  0.0189],\n",
       "            [ 0.0202, -0.0826,  0.0735,  0.1341,  0.1714]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.0910, -0.1679, -0.1144,  0.0028, -0.1899],\n",
       "            [ 0.2008, -0.1236, -0.1167, -0.0485, -0.2017],\n",
       "            [ 0.0075,  0.1203,  0.1463,  0.1102,  0.2008],\n",
       "            [ 0.1607,  0.0987,  0.1726,  0.1617, -0.1414],\n",
       "            [ 0.2336, -0.0606, -0.1322,  0.1067, -0.1322]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.1610,  0.1389, -0.1359,  0.1826, -0.0117],\n",
       "            [ 0.1951,  0.2412,  0.2236,  0.1284, -0.0325],\n",
       "            [ 0.1164, -0.1455, -0.0687,  0.0780,  0.0011],\n",
       "            [ 0.0872,  0.0874, -0.0834, -0.1183, -0.1861],\n",
       "            [-0.0710, -0.1174, -0.0541, -0.1283,  0.0560]]],\n",
       "  \n",
       "  \n",
       "          [[[ 0.1656,  0.0900,  0.1720, -0.1492, -0.1609],\n",
       "            [ 0.0581,  0.1254, -0.0758, -0.0985, -0.0491],\n",
       "            [ 0.0385, -0.0426,  0.1291, -0.1667,  0.0530],\n",
       "            [-0.1847,  0.1817, -0.1761, -0.0490,  0.1052],\n",
       "            [-0.0158, -0.0972, -0.0699, -0.0585,  0.0422]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.1290,  0.0740, -0.1960,  0.1197, -0.2121],\n",
       "            [-0.1132, -0.0993, -0.0719,  0.1210, -0.1927],\n",
       "            [-0.1608,  0.0266, -0.1144, -0.1234,  0.1449],\n",
       "            [-0.0518,  0.0473, -0.0158,  0.0591, -0.1861],\n",
       "            [-0.1444, -0.1597,  0.1668, -0.0949,  0.0619]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.0260, -0.1610, -0.0087,  0.1878, -0.0471],\n",
       "            [ 0.0208, -0.1951, -0.0533,  0.0065, -0.1599],\n",
       "            [-0.0274, -0.0910, -0.0093, -0.1148, -0.0837],\n",
       "            [-0.1454,  0.1133, -0.0086,  0.0076, -0.0154],\n",
       "            [ 0.1372,  0.0088, -0.1366,  0.0757, -0.1980]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.1191, -0.1079, -0.0525, -0.2092, -0.1821],\n",
       "            [-0.2208,  0.1350, -0.1045,  0.0367, -0.0656],\n",
       "            [ 0.2145, -0.1643,  0.1399, -0.0457,  0.0070],\n",
       "            [ 0.0715, -0.0375,  0.2151, -0.1547,  0.1216],\n",
       "            [ 0.1202,  0.0983,  0.1527,  0.1357,  0.1246]]]], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([ 0.1843, -0.0959,  0.0776,  0.0679,  0.2058, -0.0646, -0.0948, -0.0818,\n",
       "          -0.1473,  0.1604, -0.1550, -0.0956,  0.0592,  0.0685, -0.0744, -0.0407,\n",
       "          -0.1664, -0.0991, -0.0954, -0.0941, -0.1528,  0.0499,  0.1375, -0.1847,\n",
       "          -0.0059, -0.1087, -0.0830, -0.0058,  0.0077, -0.0898,  0.0126, -0.0229],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[[[ 1.6701e-02,  8.7833e-03,  4.1980e-02,  3.7858e-02, -6.0850e-03],\n",
       "            [-1.5669e-02, -1.2226e-02, -3.8945e-02,  3.5946e-02, -2.9957e-02],\n",
       "            [-5.1612e-02,  5.0463e-02,  7.8135e-02,  5.3092e-03,  4.8493e-02],\n",
       "            [ 7.4844e-04,  2.2790e-02,  6.3969e-02,  2.7960e-02,  1.7805e-02],\n",
       "            [ 4.1365e-02,  2.7963e-02,  7.2958e-04, -1.2172e-03,  4.3596e-02]],\n",
       "  \n",
       "           [[-5.5195e-04,  3.5474e-02,  4.2482e-02, -4.0888e-03,  2.3075e-02],\n",
       "            [-1.8875e-02, -8.1097e-04,  6.3287e-02,  5.8074e-02,  1.6550e-02],\n",
       "            [-1.2617e-02, -4.1441e-03, -1.2805e-02, -1.0872e-02,  3.0019e-02],\n",
       "            [-3.6194e-02, -1.3910e-02,  1.7109e-02,  5.1690e-02,  4.5130e-02],\n",
       "            [ 2.9172e-02, -4.9641e-03,  1.7892e-02,  6.9318e-02,  2.4951e-02]],\n",
       "  \n",
       "           [[ 3.1722e-02,  5.2252e-02,  2.3147e-02,  3.0718e-02,  3.9214e-03],\n",
       "            [ 2.5326e-02, -1.5305e-03,  2.9193e-02,  8.8923e-03,  2.7550e-02],\n",
       "            [ 1.0631e-02, -2.8263e-02,  3.8224e-02,  3.5795e-02,  1.9196e-03],\n",
       "            [-1.2026e-02,  7.9063e-03,  1.0707e-02,  4.1354e-02,  2.2981e-02],\n",
       "            [ 2.4357e-02, -7.6240e-03,  1.7462e-02,  1.6239e-02,  2.0307e-02]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-3.0148e-02, -1.2774e-02, -2.1658e-02,  2.4022e-02,  3.6424e-03],\n",
       "            [-1.0152e-02, -1.7737e-02,  3.6860e-02,  1.3165e-02, -3.2402e-02],\n",
       "            [ 7.0377e-03,  1.4510e-02, -1.5025e-02, -1.2067e-02,  2.4068e-02],\n",
       "            [ 2.1438e-02,  4.0998e-02,  3.1523e-02, -3.9569e-02,  3.4165e-03],\n",
       "            [-1.6811e-02, -1.0333e-02, -3.4366e-03, -2.9226e-02, -1.3677e-02]],\n",
       "  \n",
       "           [[ 2.2554e-02, -3.0753e-02,  2.5018e-02, -2.7895e-02, -2.5701e-02],\n",
       "            [-1.0768e-03, -2.1928e-02,  3.2152e-02, -1.4097e-02,  2.0152e-02],\n",
       "            [-2.7385e-02, -3.1126e-02,  7.2339e-03, -4.3070e-03, -1.0107e-02],\n",
       "            [-1.7624e-02,  1.0794e-02, -1.6516e-02,  1.5664e-03,  1.1380e-02],\n",
       "            [ 2.7031e-02,  9.8262e-04, -2.6768e-02, -8.7899e-03,  3.3382e-02]],\n",
       "  \n",
       "           [[-2.5464e-02, -1.9484e-02, -8.5022e-03, -9.7516e-03,  2.5068e-02],\n",
       "            [-3.5801e-02, -1.8048e-02,  7.0868e-03, -2.5862e-02,  1.8760e-03],\n",
       "            [ 2.9388e-02,  7.6436e-03,  4.2682e-02,  3.8518e-02,  1.7517e-02],\n",
       "            [ 1.7322e-02, -2.2063e-02,  1.9047e-03,  2.5052e-02,  4.4638e-03],\n",
       "            [-3.1992e-02,  4.0722e-02, -2.1561e-02, -1.5278e-02,  1.6855e-02]]],\n",
       "  \n",
       "  \n",
       "          [[[ 1.5766e-02,  3.9926e-02, -2.1767e-02, -7.4832e-02,  2.2026e-02],\n",
       "            [ 3.6630e-02,  2.4216e-02,  3.0323e-02, -2.8563e-02, -3.5186e-02],\n",
       "            [ 1.4701e-02, -7.5331e-05, -2.3219e-02, -2.0965e-02, -3.1268e-02],\n",
       "            [-4.8909e-02, -2.9488e-02, -3.9316e-02, -3.3757e-02, -4.1905e-03],\n",
       "            [-3.0306e-02, -2.8605e-02,  8.9910e-03,  2.5681e-02, -1.5436e-02]],\n",
       "  \n",
       "           [[-2.7259e-02, -6.5152e-03, -3.4704e-02, -6.2945e-03, -1.5469e-02],\n",
       "            [ 3.0203e-02, -2.5674e-02, -5.4877e-04, -3.4844e-02, -2.9931e-02],\n",
       "            [-5.3138e-03,  2.5839e-02, -3.6758e-02,  8.9743e-03, -2.8170e-02],\n",
       "            [-9.5193e-03, -2.5296e-02, -2.0372e-02, -4.1195e-03, -3.2060e-02],\n",
       "            [-2.1134e-02, -1.9379e-02, -6.6554e-02, -2.0431e-03,  9.3959e-03]],\n",
       "  \n",
       "           [[ 5.2276e-03, -4.1530e-02, -2.8019e-02, -4.3656e-02, -3.4915e-03],\n",
       "            [ 3.9160e-02,  1.3785e-02, -6.2577e-03, -8.8990e-03,  1.2652e-02],\n",
       "            [ 3.7255e-02, -2.0245e-02,  3.4209e-02, -3.7810e-02, -2.8125e-03],\n",
       "            [-1.8318e-02, -4.4881e-02, -2.5931e-02, -5.1157e-02, -1.1654e-02],\n",
       "            [-5.5285e-02, -4.9060e-02, -4.8782e-02, -3.9164e-02,  5.2110e-03]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-1.8893e-02,  9.8774e-03, -9.4403e-03, -7.1816e-03, -2.0629e-02],\n",
       "            [ 1.4333e-02, -3.7053e-02, -4.9528e-02,  1.9381e-02, -7.9162e-03],\n",
       "            [-1.0022e-02, -2.1747e-02,  2.2375e-03, -1.2178e-03,  2.2664e-02],\n",
       "            [ 2.0728e-02,  7.5902e-03,  2.0427e-02, -1.2019e-02, -1.7492e-02],\n",
       "            [ 9.1529e-03, -1.2740e-02,  2.4201e-02, -1.3157e-02, -2.8677e-02]],\n",
       "  \n",
       "           [[ 2.5423e-02, -7.1705e-03,  9.6423e-03,  1.7813e-02,  2.4837e-02],\n",
       "            [-2.8538e-02,  2.6121e-03,  3.1367e-02,  3.9089e-02,  1.3789e-02],\n",
       "            [ 7.8296e-03,  2.3037e-02, -1.1186e-02, -1.7479e-02,  6.1687e-03],\n",
       "            [-1.8788e-02,  1.1597e-02, -1.7674e-02, -2.2384e-02,  2.1971e-02],\n",
       "            [-3.1496e-03,  2.4127e-02,  1.3948e-02, -7.7528e-03, -1.4848e-02]],\n",
       "  \n",
       "           [[ 1.2994e-02, -1.2133e-03,  1.4922e-02,  1.2423e-02, -5.9958e-03],\n",
       "            [-2.0267e-02,  4.0791e-02, -6.0847e-03, -3.4430e-02, -4.8108e-02],\n",
       "            [-3.8602e-02, -2.2341e-02, -1.3367e-02,  1.0562e-02,  2.6229e-02],\n",
       "            [-4.5766e-02, -5.2034e-02, -2.3326e-02, -2.8351e-03,  3.6054e-02],\n",
       "            [-1.0711e-02, -2.6859e-02, -3.0706e-02,  1.1889e-02,  2.4134e-02]]],\n",
       "  \n",
       "  \n",
       "          [[[ 7.5486e-02,  6.7498e-02,  6.2558e-02,  8.7704e-02,  7.7898e-02],\n",
       "            [ 1.3064e-03,  7.1163e-02,  4.6591e-02,  9.3389e-03,  3.4769e-02],\n",
       "            [ 7.8726e-03, -4.6812e-04,  7.8602e-02,  3.9835e-02,  1.7684e-02],\n",
       "            [ 5.3654e-02,  1.1084e-02,  4.2242e-02,  9.3810e-02,  3.8566e-02],\n",
       "            [ 5.0470e-02,  8.5136e-02,  3.1466e-02,  5.9155e-02,  9.5728e-02]],\n",
       "  \n",
       "           [[ 4.9700e-02,  4.4671e-02,  7.9067e-02,  4.8174e-02, -2.1819e-03],\n",
       "            [ 4.9305e-02,  3.0071e-02,  6.3161e-02,  7.6519e-02,  5.8998e-02],\n",
       "            [ 9.6251e-03,  6.3592e-02,  2.3504e-02,  6.9000e-02,  4.5552e-02],\n",
       "            [ 2.0726e-02,  7.5719e-02,  8.0766e-02,  5.4224e-02,  8.3376e-02],\n",
       "            [ 3.8435e-02,  7.0363e-02,  4.5328e-02,  3.2340e-02,  1.4881e-02]],\n",
       "  \n",
       "           [[ 3.5907e-02,  4.0328e-02,  2.5759e-02,  6.4826e-02,  3.3397e-02],\n",
       "            [ 6.3424e-02,  2.1796e-02,  5.8838e-02,  4.3417e-02, -6.7552e-03],\n",
       "            [-8.1796e-03,  2.9062e-02,  3.6528e-02,  4.3180e-02,  2.5438e-02],\n",
       "            [-6.5667e-03, -2.0411e-03,  2.8392e-02,  1.0146e-02,  2.5474e-02],\n",
       "            [ 5.4668e-03,  7.4851e-02,  4.7259e-02,  2.2703e-02,  2.8898e-02]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-1.8396e-02, -1.9422e-03, -3.9727e-03, -4.5143e-03, -5.2103e-03],\n",
       "            [ 1.5672e-02,  1.3098e-03,  2.3505e-02, -1.6070e-02,  2.1796e-02],\n",
       "            [ 3.8295e-02,  3.2013e-02,  7.2800e-03,  2.1183e-02,  3.4831e-02],\n",
       "            [ 9.0370e-03,  5.5142e-02,  1.3368e-02,  8.1202e-03,  3.5873e-02],\n",
       "            [-1.2659e-02,  2.1763e-02, -8.3004e-03,  2.0973e-04,  1.7948e-02]],\n",
       "  \n",
       "           [[ 2.1720e-02, -1.9555e-02,  2.7172e-02, -1.2684e-02, -2.3608e-02],\n",
       "            [ 4.0483e-02,  3.4415e-02, -1.0544e-02,  1.5613e-02,  3.5787e-02],\n",
       "            [ 3.4318e-03,  3.6490e-02, -1.0694e-02,  1.3899e-02,  2.7772e-02],\n",
       "            [ 4.7515e-03,  6.1966e-02,  5.5944e-02,  5.8417e-03,  1.2195e-02],\n",
       "            [ 1.9169e-02,  9.3469e-04, -1.0369e-02,  4.8194e-02,  3.9572e-02]],\n",
       "  \n",
       "           [[-1.2776e-02,  3.4195e-02,  5.2614e-02,  3.4921e-02,  6.8041e-02],\n",
       "            [ 2.5679e-02,  3.0943e-02, -2.1855e-02, -1.6246e-02,  3.8805e-02],\n",
       "            [ 3.5221e-02,  1.3969e-02, -1.4071e-02,  3.1711e-02,  5.7022e-02],\n",
       "            [ 3.1370e-02,  6.3395e-02,  4.6080e-02,  5.6186e-02,  4.6229e-02],\n",
       "            [-7.7025e-03,  6.9363e-02,  4.9700e-02,  3.6046e-02,  3.1270e-02]]],\n",
       "  \n",
       "  \n",
       "          ...,\n",
       "  \n",
       "  \n",
       "          [[[-1.7058e-02, -5.2795e-02, -3.4652e-02, -2.3286e-03, -7.4170e-03],\n",
       "            [-8.9162e-03, -6.7074e-03, -3.8416e-02,  1.9568e-02, -3.8761e-02],\n",
       "            [ 2.3069e-02,  5.2704e-02,  5.2155e-03,  4.8273e-02, -1.0729e-02],\n",
       "            [-5.6284e-03,  3.4518e-02,  2.6500e-03,  1.4114e-02, -1.2313e-02],\n",
       "            [-5.5497e-02, -3.9384e-02, -3.0044e-03,  1.9407e-02, -2.7139e-02]],\n",
       "  \n",
       "           [[-4.5080e-03,  1.0727e-02, -6.3163e-02, -4.7308e-02, -2.4307e-02],\n",
       "            [-1.9856e-02, -2.0083e-02, -1.9645e-02,  8.8463e-03, -2.0419e-04],\n",
       "            [-2.1994e-03, -2.1192e-02,  3.2458e-02,  1.2301e-02, -3.5353e-02],\n",
       "            [-7.6475e-03,  1.1326e-02, -7.6018e-03,  3.3649e-02,  1.9614e-02],\n",
       "            [ 1.9621e-02,  1.3456e-02,  4.3862e-02,  2.1138e-02, -1.8012e-02]],\n",
       "  \n",
       "           [[ 1.9028e-02, -3.6069e-02, -7.9748e-03, -5.0456e-03, -2.5015e-02],\n",
       "            [-6.9235e-03, -4.0440e-02, -6.0963e-04,  2.5022e-03,  1.5958e-02],\n",
       "            [ 1.0367e-02, -2.4537e-02,  3.4420e-02,  2.4649e-02, -3.5087e-02],\n",
       "            [-3.5065e-02, -2.8464e-02,  2.4531e-02,  2.6704e-03,  4.3844e-03],\n",
       "            [-2.9114e-02,  7.5812e-03,  2.3086e-02,  2.7288e-02, -1.4773e-03]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 1.8403e-02,  1.2903e-02, -5.6831e-03,  1.9427e-02, -3.2759e-02],\n",
       "            [ 2.8646e-02,  1.2029e-02, -1.2850e-02,  1.4479e-02,  2.8229e-02],\n",
       "            [-3.1469e-02,  4.4504e-03, -2.3946e-02, -3.2359e-02,  2.9300e-02],\n",
       "            [ 3.2756e-02,  1.7864e-02,  2.3634e-02,  2.1147e-02, -2.9838e-02],\n",
       "            [ 1.8764e-02,  3.6638e-03, -1.6790e-02, -1.4154e-02, -3.1937e-02]],\n",
       "  \n",
       "           [[-9.4800e-03, -9.8383e-03, -2.8863e-02, -3.1875e-02, -7.3504e-03],\n",
       "            [ 2.6102e-02,  1.4085e-02,  1.9846e-02, -6.4607e-03,  1.6943e-03],\n",
       "            [ 2.1051e-03,  2.3550e-02, -2.1308e-02,  1.1342e-03,  2.5361e-02],\n",
       "            [-3.8917e-02, -8.7427e-03, -3.9521e-02,  3.5069e-02,  3.2605e-02],\n",
       "            [-2.3650e-02, -4.2749e-02, -3.5875e-02,  1.6359e-02,  2.5714e-02]],\n",
       "  \n",
       "           [[ 3.2827e-03,  5.0575e-05, -1.4786e-02, -1.8725e-02, -1.9671e-03],\n",
       "            [-1.2127e-02,  2.7171e-02,  3.6657e-02, -2.0267e-02,  5.7942e-03],\n",
       "            [-9.3062e-03,  2.1211e-02,  4.7076e-02,  1.3863e-02, -6.6550e-03],\n",
       "            [ 1.0039e-02, -3.9061e-02, -3.1022e-02,  3.7258e-02,  2.9949e-02],\n",
       "            [ 6.4180e-03, -2.7931e-02, -3.4090e-02, -4.2421e-02,  2.7882e-02]]],\n",
       "  \n",
       "  \n",
       "          [[[-2.2366e-02,  2.9919e-02, -8.3848e-03,  9.2272e-03, -7.5370e-03],\n",
       "            [ 5.0282e-02,  4.7428e-02,  4.1480e-02,  4.0492e-02,  4.7914e-02],\n",
       "            [ 6.3274e-02,  9.4855e-02,  8.0329e-02,  6.3571e-02,  1.1703e-02],\n",
       "            [ 2.3097e-02, -1.3911e-02,  2.7665e-02,  2.9644e-02,  5.9628e-03],\n",
       "            [-3.7817e-02, -1.6567e-02,  4.0527e-02, -2.4802e-02,  5.6563e-02]],\n",
       "  \n",
       "           [[-3.5349e-02, -5.3724e-03, -1.1715e-02, -3.3646e-03,  5.6507e-02],\n",
       "            [ 3.8989e-02,  6.6448e-02,  6.1516e-02,  7.5144e-02,  6.5767e-02],\n",
       "            [ 5.3751e-03, -2.6550e-02, -4.0083e-02,  3.8167e-02, -1.2395e-02],\n",
       "            [ 1.2117e-03,  6.6735e-02,  4.4510e-02,  2.7192e-02, -4.1008e-03],\n",
       "            [-1.2178e-02,  1.4865e-02,  2.6927e-02, -4.2599e-03,  3.3515e-02]],\n",
       "  \n",
       "           [[-4.4922e-02, -2.4109e-02,  3.5538e-03,  1.9545e-02, -1.0128e-02],\n",
       "            [ 3.4883e-03, -2.2908e-03, -3.6823e-02, -2.0715e-02,  5.6279e-03],\n",
       "            [ 3.0970e-02, -8.5952e-03,  1.0413e-02,  4.5054e-02,  2.8178e-02],\n",
       "            [ 7.6876e-02,  7.3889e-02,  2.2496e-02,  4.3884e-02,  2.4221e-02],\n",
       "            [-1.6576e-02,  8.0335e-03,  3.2250e-02, -1.3278e-03,  1.1933e-02]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[-6.8328e-03,  1.5858e-02, -1.4926e-02,  1.7709e-02, -2.4187e-02],\n",
       "            [ 3.1522e-03,  3.5438e-02,  1.8905e-02, -1.4843e-02,  2.6914e-02],\n",
       "            [-8.9474e-03, -3.8492e-02,  7.1632e-03,  2.2829e-03,  1.5085e-02],\n",
       "            [-2.2545e-03,  4.0681e-03, -1.3269e-02,  1.8366e-02, -5.4624e-03],\n",
       "            [ 1.4978e-02, -2.0123e-02,  1.0244e-02, -2.1811e-02, -2.2933e-02]],\n",
       "  \n",
       "           [[-1.2696e-03,  1.8283e-02,  2.4881e-02, -5.6405e-03, -1.0816e-02],\n",
       "            [ 2.1288e-02,  5.1898e-03,  7.3735e-04,  4.3487e-02,  3.3601e-02],\n",
       "            [ 2.1421e-02, -7.5332e-03, -1.9243e-02, -2.8647e-02,  1.5879e-02],\n",
       "            [ 2.4400e-02, -2.6828e-02,  3.3001e-02,  6.6647e-03, -2.1903e-02],\n",
       "            [-1.3811e-02, -2.4667e-02, -1.0836e-02,  3.6637e-02,  2.4798e-02]],\n",
       "  \n",
       "           [[-4.3620e-03, -3.4383e-03,  1.8035e-03,  2.4731e-02, -5.6520e-03],\n",
       "            [ 3.9523e-02,  2.2889e-02,  3.9929e-02,  3.9909e-02,  5.6753e-02],\n",
       "            [ 2.6256e-03,  5.8318e-02,  2.4715e-02,  1.1824e-02,  3.1545e-02],\n",
       "            [-3.3405e-02,  1.7741e-02, -1.8027e-02, -9.0935e-03, -3.4300e-02],\n",
       "            [ 1.1639e-02,  2.1164e-02,  1.0904e-02, -3.8901e-03, -2.0859e-02]]],\n",
       "  \n",
       "  \n",
       "          [[[ 2.5332e-02,  8.0130e-02,  9.4993e-02,  1.0870e-01,  4.4522e-02],\n",
       "            [ 2.6898e-02,  7.3872e-02,  6.2344e-02,  6.3539e-02,  6.4537e-02],\n",
       "            [ 2.8654e-02,  2.9506e-02,  6.0525e-02,  4.5299e-02,  4.7288e-02],\n",
       "            [ 1.3750e-02,  2.5594e-02,  2.3160e-02,  5.9934e-02, -1.0789e-02],\n",
       "            [ 8.5313e-02,  1.2587e-02,  1.3390e-03,  5.3953e-03,  3.4275e-02]],\n",
       "  \n",
       "           [[ 5.8431e-02,  6.5170e-02,  2.9313e-02,  2.4955e-02,  1.0869e-02],\n",
       "            [ 1.3354e-02,  4.6082e-02,  7.9081e-02,  9.6011e-02,  4.7767e-02],\n",
       "            [ 4.6133e-02,  2.0277e-03,  5.5257e-02,  8.1858e-02,  6.2910e-03],\n",
       "            [ 2.5126e-02,  3.9356e-02,  4.9666e-02,  1.4565e-02,  1.9015e-02],\n",
       "            [ 8.7123e-03,  2.8684e-02,  6.5328e-02,  6.3735e-03,  6.9387e-03]],\n",
       "  \n",
       "           [[ 3.3657e-02, -4.3515e-03,  2.3293e-02,  4.6264e-02,  4.7670e-02],\n",
       "            [ 5.6113e-02,  3.1452e-02,  6.8932e-03,  6.5525e-02,  4.1323e-02],\n",
       "            [-6.0547e-04,  2.3368e-02, -7.7998e-03,  2.8468e-02,  3.9617e-02],\n",
       "            [-1.1232e-02,  1.3823e-04,  1.2489e-02,  3.7703e-02,  5.2310e-02],\n",
       "            [ 4.0039e-02,  3.8251e-03,  3.5750e-02, -1.2510e-02,  6.0444e-03]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 1.6183e-02, -1.3638e-02,  3.3930e-02,  3.1793e-02,  2.7209e-02],\n",
       "            [-2.2973e-02, -1.3569e-02,  1.5023e-02,  2.6491e-02,  9.1223e-03],\n",
       "            [ 3.5701e-02,  1.3964e-03,  1.1409e-02,  1.6065e-02, -1.3533e-02],\n",
       "            [-4.3766e-03, -2.4820e-02, -2.6851e-02,  5.4070e-03,  2.6896e-03],\n",
       "            [ 2.5636e-02, -9.6206e-04, -1.9188e-02,  2.6695e-02,  1.8975e-02]],\n",
       "  \n",
       "           [[ 2.8120e-02,  4.4768e-02,  4.4436e-02,  1.5505e-02,  1.1827e-02],\n",
       "            [ 3.9214e-02, -9.9129e-03,  1.5771e-02, -9.2354e-04,  5.9653e-03],\n",
       "            [ 2.0886e-02, -1.0248e-03,  3.2420e-02, -5.8766e-03,  3.1024e-02],\n",
       "            [ 2.8268e-02,  3.7177e-02, -1.5416e-02,  1.2821e-02,  3.1573e-02],\n",
       "            [ 2.3242e-02, -7.3273e-03,  2.5613e-02,  4.7608e-02,  7.9237e-04]],\n",
       "  \n",
       "           [[ 4.9121e-02,  1.7367e-02,  3.7813e-02,  4.2962e-02,  3.9087e-02],\n",
       "            [ 1.0069e-02,  5.8485e-03,  3.0465e-02,  2.6392e-02,  2.0558e-02],\n",
       "            [-6.4752e-03,  7.9696e-03,  5.1215e-02,  4.1546e-02, -2.1053e-02],\n",
       "            [-2.2668e-03, -1.3547e-02,  3.0162e-02, -3.2775e-03, -2.7632e-02],\n",
       "            [ 3.5585e-02,  2.5011e-02, -1.7435e-02, -3.6892e-02,  2.7566e-02]]]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([-0.0086, -0.0050,  0.0671,  0.0075, -0.0331, -0.0085, -0.0060,  0.0211,\n",
       "           0.0524, -0.0165,  0.0098,  0.0172,  0.0577,  0.0368, -0.0037,  0.0301,\n",
       "          -0.0512, -0.0067,  0.0054,  0.0366,  0.0627, -0.0030,  0.0747,  0.0203,\n",
       "          -0.0367, -0.0036,  0.0029, -0.0175, -0.0325,  0.0059, -0.0207,  0.0118,\n",
       "           0.0087, -0.0414,  0.0089, -0.0358, -0.0226, -0.0476, -0.0461,  0.0544,\n",
       "           0.0262,  0.0791, -0.0087,  0.1035, -0.0034,  0.0345,  0.0177,  0.0893,\n",
       "           0.0303, -0.0075, -0.0356, -0.0125, -0.0052, -0.0408,  0.0493,  0.0371,\n",
       "           0.0066, -0.0313,  0.0316, -0.0299, -0.0064,  0.0071,  0.0341,  0.0145],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0142, -0.0080,  0.0280,  ...,  0.0124, -0.0010,  0.0051],\n",
       "          [ 0.0230,  0.0118,  0.0018,  ..., -0.0264,  0.0098, -0.0267],\n",
       "          [ 0.0042, -0.0228, -0.0268,  ...,  0.0050,  0.0061, -0.0181],\n",
       "          ...,\n",
       "          [ 0.0271,  0.0123,  0.0531,  ...,  0.0073,  0.0659,  0.0254],\n",
       "          [ 0.0038,  0.0177, -0.0115,  ..., -0.0097, -0.0264, -0.0060],\n",
       "          [ 0.0036, -0.0089,  0.0155,  ...,  0.0073,  0.0039,  0.0016]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([-1.5014e-02,  2.7167e-02, -1.8762e-02,  5.6500e-03,  3.5492e-02,\n",
       "          -2.5427e-02, -1.9837e-02,  5.9704e-04,  1.5771e-02, -1.0484e-02,\n",
       "          -2.7753e-02,  2.8411e-02, -1.8881e-02, -1.2449e-02, -2.8969e-02,\n",
       "           2.2648e-02, -3.2879e-02, -3.8309e-02,  3.0655e-02,  3.2975e-02,\n",
       "           2.0601e-02,  2.6068e-02, -1.4796e-02,  2.8815e-02, -1.4164e-02,\n",
       "          -1.2534e-02,  2.5051e-02, -9.8884e-03, -5.5921e-02,  1.2249e-02,\n",
       "          -4.5575e-03, -1.1136e-02, -2.0198e-03, -2.7706e-02,  2.2427e-03,\n",
       "          -2.8352e-02, -2.2383e-02, -2.2084e-02,  2.0964e-02, -2.7535e-02,\n",
       "           8.2531e-04,  2.9925e-02, -5.4893e-03, -7.8679e-03, -1.9892e-02,\n",
       "           4.5041e-02, -5.9190e-02, -2.3236e-02,  1.2650e-02, -3.0488e-02,\n",
       "           1.6560e-02,  7.3937e-03,  3.0317e-02,  5.1569e-02,  9.8904e-03,\n",
       "           4.2360e-03, -4.3738e-03,  5.4619e-03, -1.2048e-02, -1.2202e-02,\n",
       "           1.9142e-02,  3.1533e-02, -4.4089e-03, -1.2862e-03,  1.9433e-02,\n",
       "          -3.2537e-02,  2.4227e-02, -4.2608e-02, -5.6595e-03, -1.0659e-02,\n",
       "           3.6608e-02,  3.9588e-02,  1.1275e-02,  4.2755e-04, -3.1279e-02,\n",
       "          -2.0162e-02,  9.6216e-03,  3.3339e-03,  2.8276e-02,  3.2140e-02,\n",
       "          -2.7536e-02,  2.7046e-03,  8.9967e-03, -2.4494e-02,  5.1284e-02,\n",
       "           4.3938e-02, -2.6627e-02,  2.4252e-03,  3.7267e-02,  3.5251e-03,\n",
       "           3.4622e-03, -4.1719e-02,  9.6295e-03, -2.0162e-02,  1.6560e-02,\n",
       "           3.4536e-03,  2.0336e-02,  8.0655e-03, -1.3707e-02,  1.9437e-02,\n",
       "          -3.1173e-02, -2.9105e-03,  9.8901e-03, -1.4592e-03, -2.1270e-02,\n",
       "          -2.5483e-02,  4.7283e-02,  7.9949e-03, -4.4258e-02,  2.2646e-02,\n",
       "          -1.5364e-05, -2.9589e-02,  2.5826e-02,  4.6207e-03,  7.5396e-03,\n",
       "          -1.9409e-02,  1.0388e-02,  2.3470e-02,  8.2717e-03, -2.4235e-02,\n",
       "           4.1296e-04, -2.4931e-02, -2.4303e-02,  3.4489e-03,  9.2502e-03,\n",
       "           2.9593e-02, -3.3982e-02,  5.4559e-02, -1.0745e-02, -6.2671e-03,\n",
       "          -3.3826e-03,  7.0952e-04, -1.2624e-02, -2.1085e-02, -1.8246e-02,\n",
       "          -2.1612e-03, -4.6226e-02,  3.8946e-03,  2.0734e-02, -2.6878e-02,\n",
       "           5.8546e-02, -1.6473e-02,  2.7820e-02, -1.4705e-02, -2.3284e-02,\n",
       "           3.2901e-02,  5.6613e-03, -1.7120e-03,  2.7828e-02,  5.6509e-03,\n",
       "           1.0548e-03, -2.5234e-02,  3.0478e-03,  2.3420e-02,  4.9480e-03,\n",
       "           9.3008e-03,  2.9692e-02, -7.8338e-03, -3.0175e-02,  2.4516e-02,\n",
       "          -4.5736e-03,  1.0236e-03,  2.3455e-02,  2.3512e-02, -5.8713e-02,\n",
       "          -5.8948e-05, -8.0037e-03, -2.9973e-02, -4.0582e-02, -2.1649e-02,\n",
       "          -5.2034e-03,  1.6393e-02,  3.8271e-02, -1.0325e-02, -8.9941e-04,\n",
       "           3.7573e-02, -1.4876e-03,  7.4907e-03, -5.6352e-02,  1.2375e-02,\n",
       "          -2.5127e-02,  1.6934e-02,  1.2279e-02,  1.4983e-02, -1.0046e-02,\n",
       "           2.6173e-02, -2.7371e-02, -1.0122e-02,  1.3628e-02, -2.3223e-02,\n",
       "           1.5142e-02,  3.3925e-02,  2.6067e-02,  7.2719e-04,  3.5799e-02,\n",
       "           1.8271e-02, -4.2072e-02,  2.4237e-02, -1.4649e-02,  3.0413e-02,\n",
       "          -1.1212e-02, -1.3550e-02,  5.3686e-03, -3.8544e-03, -2.7703e-02,\n",
       "           1.3730e-03,  2.7088e-02,  3.3572e-02, -4.7206e-03,  3.4348e-02,\n",
       "          -1.6543e-02, -3.5779e-02,  4.2226e-03, -1.3541e-02,  1.2995e-02,\n",
       "          -2.5492e-03, -1.1319e-02,  6.8318e-03,  3.0808e-02, -7.1886e-03,\n",
       "          -8.1466e-03,  2.3612e-02,  8.5978e-03, -1.4416e-02,  3.0363e-02,\n",
       "           3.6567e-02,  3.6466e-03, -4.1062e-02, -9.7187e-03,  7.9166e-03,\n",
       "          -5.4961e-03,  2.5511e-02,  3.1521e-03,  1.0108e-02, -2.1276e-02,\n",
       "           1.2740e-02, -3.4038e-02,  3.1485e-02, -7.0251e-03, -1.2462e-02,\n",
       "           1.7823e-02,  3.1059e-03,  3.0977e-02, -1.1046e-02,  1.0528e-02,\n",
       "           1.7728e-02, -2.0551e-02, -8.9893e-03,  2.5751e-02,  9.8601e-03,\n",
       "           2.6759e-02, -3.3643e-02,  3.3517e-03, -1.8854e-03,  1.1989e-02,\n",
       "           8.9923e-03,  2.9821e-02,  1.2768e-02,  6.6306e-03,  9.0862e-03,\n",
       "           1.7279e-02, -1.4506e-03,  4.5298e-03, -3.6835e-02,  1.7464e-02,\n",
       "          -3.5694e-02,  3.0902e-03,  1.6251e-02, -2.4803e-02, -1.3117e-02,\n",
       "           5.0269e-02,  1.4800e-02, -1.3869e-02,  8.2642e-03, -1.1383e-02,\n",
       "          -7.1375e-03,  3.0514e-02,  3.0014e-02, -1.2239e-02,  2.0372e-02,\n",
       "          -2.6510e-03,  4.6620e-03,  1.9598e-02,  3.7020e-02,  4.1387e-03,\n",
       "           2.7625e-02, -4.1916e-02, -3.2156e-04, -2.3301e-02, -2.8727e-02,\n",
       "           5.3031e-02,  1.5077e-02,  3.0326e-02, -1.1303e-02,  9.6344e-03,\n",
       "           9.4463e-03, -1.0202e-02, -2.1429e-02,  1.7104e-02, -1.1573e-02,\n",
       "          -1.7551e-02, -4.5423e-03,  1.2794e-02,  1.3429e-02, -1.6031e-02,\n",
       "           3.0786e-02,  1.0059e-02,  2.9071e-02, -5.2034e-03,  1.8610e-02,\n",
       "           2.8382e-02, -2.2175e-02, -1.2631e-02,  2.2675e-03, -1.8024e-02,\n",
       "           3.2747e-02, -7.7838e-03, -3.6409e-02, -1.3737e-02,  1.7313e-02,\n",
       "          -2.2260e-02,  4.0410e-03, -2.9277e-02,  2.9143e-02,  9.6328e-03,\n",
       "           3.0377e-02,  1.0184e-02,  3.1339e-02,  2.8429e-02, -1.9923e-02,\n",
       "          -2.6908e-02,  3.2130e-02, -3.7146e-02, -5.5803e-03,  2.8247e-02,\n",
       "           5.8501e-02, -9.4723e-03,  4.7224e-02,  6.2875e-02, -1.1709e-02,\n",
       "           2.6759e-02, -1.6174e-02,  1.5638e-02,  3.0513e-02, -9.6240e-03,\n",
       "          -1.2581e-02, -1.4626e-02, -2.0443e-02, -1.5347e-02, -2.5652e-02,\n",
       "          -4.7095e-02,  4.0725e-02, -1.9763e-02,  4.2171e-02,  1.2461e-02,\n",
       "          -2.1856e-02,  4.8239e-02, -1.5329e-02, -1.2033e-02, -4.4894e-03,\n",
       "           9.8144e-03,  2.7972e-02,  4.3753e-03,  1.4404e-03, -2.8469e-02,\n",
       "           2.6880e-03,  4.4949e-02,  2.6290e-02, -5.7661e-03,  1.5920e-02,\n",
       "           3.7575e-02,  2.7437e-02, -1.1881e-02,  8.0718e-03,  2.3094e-02,\n",
       "          -3.1469e-03,  5.0671e-03,  1.0800e-03,  8.4571e-03, -2.5469e-02,\n",
       "           3.0509e-02,  1.4297e-02, -1.2508e-03, -1.2880e-02, -2.3735e-02,\n",
       "          -2.7488e-02,  2.7652e-03, -3.1078e-02,  5.0980e-02,  3.1905e-03,\n",
       "           2.6874e-02, -3.9199e-03, -7.2583e-04,  4.6227e-03, -1.5888e-02,\n",
       "           5.7330e-03,  9.0155e-03,  1.1233e-02,  5.0582e-02, -2.2725e-02,\n",
       "          -1.9572e-02, -1.8308e-02, -2.0142e-02, -4.5095e-02, -1.3661e-02,\n",
       "          -3.8651e-02, -8.1463e-03, -1.7586e-02, -2.6175e-02, -7.6208e-03,\n",
       "          -1.3342e-02, -1.8077e-03, -1.5290e-02, -3.2309e-04,  5.3769e-03,\n",
       "          -2.7386e-02, -1.8501e-02,  2.9320e-02,  2.9647e-02,  1.6486e-02,\n",
       "          -2.2416e-02, -1.0107e-02, -2.7156e-02, -8.3679e-03, -3.6506e-02,\n",
       "          -9.1898e-03, -5.5672e-03,  2.7053e-02, -1.2564e-04,  3.0508e-02,\n",
       "           2.9714e-02, -4.8171e-03,  1.3303e-02,  3.5581e-02,  1.5961e-02,\n",
       "           6.3135e-02, -6.0026e-04,  4.2822e-03, -2.0111e-02,  1.1368e-02,\n",
       "          -9.1601e-03, -2.1328e-02, -1.8180e-02, -3.9077e-02,  1.5738e-02,\n",
       "           9.7244e-03, -1.9936e-02,  3.5810e-02,  1.0389e-02,  1.1538e-02,\n",
       "           1.0562e-02, -3.8671e-03, -4.5352e-02, -2.3273e-02,  2.4189e-02,\n",
       "           7.8327e-03, -4.1984e-02,  1.3262e-02,  1.5655e-02,  1.3456e-02,\n",
       "           2.1307e-02, -2.1197e-02,  2.8025e-02,  8.9569e-03, -4.7258e-03,\n",
       "          -1.8840e-02,  6.4759e-02, -1.1695e-02,  2.5326e-02, -4.1348e-02,\n",
       "          -3.1514e-03,  4.5137e-03, -3.0435e-03,  5.3983e-03,  1.9520e-02,\n",
       "           2.6670e-02,  7.4768e-03, -6.7055e-03, -2.7095e-02, -9.9242e-03,\n",
       "          -1.0577e-03,  9.9071e-03, -3.0285e-02,  1.4458e-02, -2.7577e-02,\n",
       "          -2.2822e-02,  3.1006e-02, -2.9068e-02, -3.1995e-02,  3.8711e-02,\n",
       "          -5.8949e-05, -2.8406e-02, -5.9847e-02,  8.9855e-03, -7.2919e-03,\n",
       "           5.3985e-04, -6.0643e-02, -1.6122e-02, -2.1111e-02,  1.1811e-02,\n",
       "           3.1476e-04,  2.7534e-02,  2.0474e-02,  2.3548e-02, -4.3580e-02,\n",
       "          -3.1332e-03, -2.4816e-02,  2.5337e-02, -7.9936e-03,  2.6133e-02,\n",
       "          -1.1696e-02,  1.1851e-02], requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0277, -0.0181, -0.0108,  ..., -0.0538,  0.0203,  0.0191],\n",
       "          [ 0.0089, -0.0076, -0.0139,  ..., -0.0477,  0.0035, -0.0482],\n",
       "          [-0.0173,  0.0101, -0.0397,  ..., -0.1093, -0.0136, -0.0085],\n",
       "          ...,\n",
       "          [ 0.0339,  0.0080,  0.0308,  ..., -0.0766,  0.0043, -0.0093],\n",
       "          [-0.0417,  0.0071,  0.0091,  ..., -0.0197,  0.0193, -0.0354],\n",
       "          [-0.0422,  0.0207,  0.0240,  ..., -0.0659, -0.0279,  0.0017]],\n",
       "         requires_grad=True),\n",
       "  Parameter containing:\n",
       "  tensor([-0.1434, -0.1015, -0.1162, -0.0842, -0.0748,  0.8578, -0.0959, -0.0589,\n",
       "          -0.1204, -0.0854], requires_grad=True)],\n",
       " 'lr': 1,\n",
       " 'momentum': 0,\n",
       " 'dampening': 0,\n",
       " 'weight_decay': 0,\n",
       " 'nesterov': False,\n",
       " 'variance': 0}"
      ]
     },
     "execution_count": 1290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.param_groups[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1280,
   "id": "e40ced00",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = torch.tensor(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1285,
   "id": "400c1e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15)"
      ]
     },
     "execution_count": 1285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa.add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1273,
   "id": "ebb709d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[True, True, True, True, True],\n",
       "         [True, True, True, True, True],\n",
       "         [True, True, True, True, True],\n",
       "         [True, True, True, True, True],\n",
       "         [True, True, True, True, True]]])"
      ]
     },
     "execution_count": 1273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.clone(pp[0][0]) == pp[0][0].clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1234,
   "id": "f06530a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = FedProx(model.parameters(), 1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1247,
   "id": "e25c431d",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = model(x[None,...])\n",
    "loss = criterion(p, torch.tensor([y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1248,
   "id": "22867bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1246,
   "id": "34b7a2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1260,
   "id": "5aa85c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in tmp.param_groups:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1261,
   "id": "3aaf2772",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in group['params']:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1249,
   "id": "6b594d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'dict'>, {Parameter containing:\n",
      "tensor([[[[ 0.0135,  0.0834, -0.1635, -0.1240,  0.0150],\n",
      "          [ 0.1076,  0.0894,  0.1221, -0.0690, -0.1478],\n",
      "          [-0.1232,  0.1202,  0.1382,  0.1460,  0.0215],\n",
      "          [ 0.1577,  0.0469,  0.0594,  0.0215,  0.1838],\n",
      "          [ 0.1121, -0.2022, -0.1584,  0.0095,  0.0925]]],\n",
      "\n",
      "\n",
      "        [[[-0.0625,  0.1570,  0.0061,  0.1393,  0.1369],\n",
      "          [-0.1410,  0.1502,  0.0581,  0.1548, -0.0272],\n",
      "          [-0.1242, -0.0803, -0.0600, -0.1837, -0.0414],\n",
      "          [ 0.0703, -0.0023,  0.0040, -0.1584, -0.1531],\n",
      "          [ 0.0853,  0.2344,  0.2251,  0.0915,  0.1444]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0254,  0.0159, -0.1718,  0.0243,  0.0359],\n",
      "          [ 0.1028,  0.0580,  0.1281, -0.1112,  0.0933],\n",
      "          [ 0.0104,  0.1602, -0.0586,  0.0608, -0.0146],\n",
      "          [-0.0350, -0.0496,  0.1493, -0.1740, -0.0495],\n",
      "          [-0.0526, -0.0541, -0.1318,  0.1012,  0.0460]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0523, -0.1491, -0.1451, -0.1036,  0.1483],\n",
      "          [ 0.1593, -0.1866,  0.0807, -0.1690,  0.1838],\n",
      "          [-0.1610, -0.1925, -0.0687, -0.0771, -0.1243],\n",
      "          [-0.1310, -0.1723, -0.1332, -0.1387,  0.1657],\n",
      "          [ 0.0673,  0.1843, -0.1727, -0.0221,  0.0475]]],\n",
      "\n",
      "\n",
      "        [[[-0.1166, -0.2003,  0.0023, -0.1722, -0.1807],\n",
      "          [ 0.1743, -0.1324,  0.1766, -0.1933,  0.0421],\n",
      "          [ 0.0664, -0.1061,  0.1350, -0.1796,  0.0959],\n",
      "          [-0.1545, -0.0873, -0.1879, -0.0640, -0.0866],\n",
      "          [ 0.0948, -0.0368, -0.0359,  0.0155,  0.1910]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0258,  0.1585, -0.0605, -0.1808, -0.1016],\n",
      "          [ 0.1053,  0.1681, -0.0658, -0.1539,  0.0900],\n",
      "          [ 0.1305,  0.0858,  0.0231,  0.2134,  0.0425],\n",
      "          [-0.0314, -0.0843, -0.1412,  0.1217, -0.1732],\n",
      "          [ 0.1602,  0.1400,  0.1269,  0.0630,  0.0827]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0358, -0.1917, -0.1120,  0.2049, -0.1158],\n",
      "          [-0.0061, -0.1651,  0.0857, -0.0805,  0.1048],\n",
      "          [ 0.2273,  0.0632, -0.0317,  0.1479,  0.1683],\n",
      "          [-0.1713, -0.0821,  0.2332,  0.1289,  0.1782],\n",
      "          [-0.2157,  0.0590, -0.1970,  0.1742,  0.1031]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1946,  0.1226, -0.0379,  0.1271,  0.1310],\n",
      "          [ 0.0521,  0.1187,  0.0522, -0.0669,  0.0973],\n",
      "          [ 0.0018,  0.2133, -0.1862,  0.1631, -0.0204],\n",
      "          [ 0.0216, -0.1197, -0.1432, -0.0932,  0.0281],\n",
      "          [ 0.0581,  0.1875,  0.1514, -0.1499, -0.1222]]],\n",
      "\n",
      "\n",
      "        [[[-0.1871,  0.0648, -0.0124,  0.0088, -0.0019],\n",
      "          [ 0.1854, -0.1833, -0.1256, -0.0211, -0.2010],\n",
      "          [-0.1089,  0.0371, -0.1193, -0.1104,  0.0947],\n",
      "          [ 0.0985,  0.0924, -0.1460, -0.1077,  0.1423],\n",
      "          [-0.0402, -0.0136,  0.1983, -0.0739,  0.2171]]],\n",
      "\n",
      "\n",
      "        [[[-0.0755,  0.0595,  0.1407, -0.2088, -0.0349],\n",
      "          [ 0.1426, -0.2198, -0.0138, -0.0581, -0.1707],\n",
      "          [ 0.1371,  0.0209, -0.0216, -0.0762,  0.1080],\n",
      "          [ 0.0938,  0.0840,  0.0349, -0.1152, -0.2312],\n",
      "          [ 0.0812, -0.0777, -0.0365,  0.0252, -0.0585]]],\n",
      "\n",
      "\n",
      "        [[[-0.0267,  0.0877,  0.0532,  0.0864, -0.0226],\n",
      "          [-0.1922,  0.2107,  0.1760,  0.1296, -0.1210],\n",
      "          [ 0.0003,  0.0457,  0.1716, -0.0477,  0.0795],\n",
      "          [-0.0709,  0.0425,  0.1391,  0.1358,  0.0675],\n",
      "          [-0.0902,  0.1600, -0.0145,  0.0142, -0.0277]]],\n",
      "\n",
      "\n",
      "        [[[-0.1873,  0.1259,  0.1329,  0.2461, -0.0952],\n",
      "          [ 0.0395, -0.1034,  0.2120, -0.0411,  0.0971],\n",
      "          [ 0.0161, -0.0857,  0.1236,  0.1347, -0.0836],\n",
      "          [ 0.1901,  0.0359,  0.1728, -0.1359,  0.2418],\n",
      "          [-0.1364, -0.1452,  0.0169,  0.2173,  0.0485]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1180, -0.1290,  0.1516, -0.0370,  0.0083],\n",
      "          [-0.1335, -0.1046, -0.1384, -0.0997, -0.2158],\n",
      "          [ 0.0763, -0.1950,  0.0490, -0.1629, -0.1906],\n",
      "          [-0.1712,  0.1970, -0.1103,  0.1659, -0.1250],\n",
      "          [ 0.0452, -0.0650,  0.0322,  0.1534,  0.0450]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1303, -0.0990,  0.1991, -0.1845, -0.1459],\n",
      "          [ 0.0317, -0.1254,  0.1798,  0.0878, -0.0153],\n",
      "          [-0.0719,  0.2238,  0.0349,  0.0481, -0.0195],\n",
      "          [-0.0012,  0.1490,  0.0777, -0.0401, -0.1298],\n",
      "          [-0.0932, -0.1288, -0.1446,  0.0063, -0.1167]]],\n",
      "\n",
      "\n",
      "        [[[-0.0092,  0.0875,  0.1393, -0.0748, -0.0498],\n",
      "          [ 0.0256, -0.0404,  0.1814,  0.0301,  0.0224],\n",
      "          [-0.0543, -0.1303,  0.1696,  0.0547,  0.1267],\n",
      "          [ 0.1552, -0.0524,  0.0304, -0.0342,  0.0414],\n",
      "          [ 0.0382, -0.1561, -0.1196,  0.0560, -0.0426]]],\n",
      "\n",
      "\n",
      "        [[[-0.1724, -0.0990,  0.0374, -0.0619, -0.0433],\n",
      "          [ 0.0275, -0.0777, -0.1589,  0.0274,  0.1841],\n",
      "          [-0.1549,  0.0612,  0.0661, -0.0064,  0.1761],\n",
      "          [ 0.0730,  0.1787, -0.1636,  0.0290,  0.1941],\n",
      "          [ 0.1649, -0.0269, -0.1209, -0.0951,  0.0009]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2331,  0.2258,  0.2711,  0.2831,  0.1886],\n",
      "          [ 0.0851,  0.1380,  0.1660,  0.0121, -0.0609],\n",
      "          [ 0.0124, -0.0006,  0.0990, -0.1002, -0.1323],\n",
      "          [-0.2045,  0.1496,  0.0704, -0.0544,  0.1827],\n",
      "          [-0.0494, -0.1172,  0.1525, -0.1420,  0.1531]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0690,  0.1429,  0.1603, -0.1402,  0.1642],\n",
      "          [-0.1474,  0.1451,  0.1482, -0.0824,  0.2107],\n",
      "          [ 0.1589, -0.0015,  0.0661,  0.0453, -0.0813],\n",
      "          [-0.0687,  0.0027, -0.1749, -0.0774, -0.0202],\n",
      "          [ 0.0614, -0.0686, -0.1603, -0.1345,  0.1668]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2234, -0.1157,  0.2208,  0.1601,  0.0766],\n",
      "          [-0.0649,  0.0621, -0.0811,  0.1370,  0.0545],\n",
      "          [-0.0655, -0.0088, -0.1224, -0.1125,  0.1804],\n",
      "          [-0.0673, -0.1320,  0.1635, -0.1471,  0.0069],\n",
      "          [-0.1517, -0.2006,  0.0924,  0.1809,  0.1496]]],\n",
      "\n",
      "\n",
      "        [[[-0.0025,  0.1443,  0.0174,  0.0232, -0.1461],\n",
      "          [-0.0627,  0.0182, -0.0657,  0.2101,  0.0402],\n",
      "          [ 0.0805, -0.1545,  0.1720,  0.1626, -0.0382],\n",
      "          [ 0.1976,  0.0982, -0.0351,  0.0006,  0.2624],\n",
      "          [ 0.0055,  0.2480, -0.1111,  0.2202,  0.1832]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3022,  0.2427, -0.1028, -0.0976,  0.1901],\n",
      "          [ 0.1156,  0.1059,  0.0971,  0.1397,  0.1990],\n",
      "          [ 0.1251, -0.1811, -0.0711, -0.0224, -0.0822],\n",
      "          [-0.1492, -0.1688, -0.0462, -0.1098, -0.1868],\n",
      "          [ 0.0509, -0.0629, -0.0965,  0.0679,  0.1303]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0047, -0.1515, -0.1914, -0.0853, -0.0127],\n",
      "          [-0.1211,  0.0330, -0.0010, -0.1034,  0.0737],\n",
      "          [-0.0227,  0.0031, -0.0279,  0.1258,  0.1343],\n",
      "          [-0.0298,  0.0576, -0.1494, -0.0595,  0.1605],\n",
      "          [-0.1548, -0.1767,  0.1322,  0.0387, -0.0155]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2051,  0.2821,  0.2756,  0.0940,  0.1375],\n",
      "          [-0.1472, -0.0164, -0.1026, -0.0379, -0.1374],\n",
      "          [ 0.0634, -0.0502,  0.1892,  0.1894, -0.1735],\n",
      "          [-0.1031,  0.0435, -0.0016, -0.2340,  0.1660],\n",
      "          [ 0.0055, -0.0299,  0.0141,  0.1227, -0.1556]]],\n",
      "\n",
      "\n",
      "        [[[-0.0964,  0.0513, -0.1124, -0.0549, -0.0088],\n",
      "          [ 0.0687, -0.2043,  0.0323, -0.0136, -0.0300],\n",
      "          [ 0.0765, -0.0577, -0.1009, -0.2061,  0.0690],\n",
      "          [ 0.1918, -0.0174, -0.1005,  0.0680,  0.1170],\n",
      "          [-0.1754, -0.0726, -0.0102, -0.1012,  0.0426]]],\n",
      "\n",
      "\n",
      "        [[[-0.0140, -0.1365, -0.2059,  0.0698,  0.0709],\n",
      "          [-0.1770,  0.1414, -0.0972,  0.1095, -0.2063],\n",
      "          [-0.1484,  0.1153, -0.0299, -0.0252, -0.1552],\n",
      "          [-0.1791, -0.0278,  0.1730,  0.1276, -0.0038],\n",
      "          [-0.1634, -0.1221, -0.1781,  0.0704, -0.1323]]],\n",
      "\n",
      "\n",
      "        [[[-0.1827,  0.0661,  0.0416,  0.0590, -0.0038],\n",
      "          [ 0.0322, -0.0074, -0.1251,  0.1627,  0.0389],\n",
      "          [ 0.0722, -0.0004, -0.1702, -0.1344, -0.1649],\n",
      "          [-0.0498, -0.0869,  0.1245,  0.0755,  0.0189],\n",
      "          [ 0.0202, -0.0826,  0.0735,  0.1341,  0.1714]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0910, -0.1679, -0.1144,  0.0028, -0.1899],\n",
      "          [ 0.2008, -0.1236, -0.1167, -0.0485, -0.2017],\n",
      "          [ 0.0075,  0.1203,  0.1463,  0.1102,  0.2008],\n",
      "          [ 0.1607,  0.0987,  0.1726,  0.1617, -0.1414],\n",
      "          [ 0.2336, -0.0606, -0.1322,  0.1067, -0.1322]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1610,  0.1389, -0.1359,  0.1826, -0.0117],\n",
      "          [ 0.1951,  0.2412,  0.2236,  0.1284, -0.0325],\n",
      "          [ 0.1164, -0.1455, -0.0687,  0.0780,  0.0011],\n",
      "          [ 0.0872,  0.0874, -0.0834, -0.1183, -0.1861],\n",
      "          [-0.0710, -0.1174, -0.0541, -0.1283,  0.0560]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1656,  0.0900,  0.1720, -0.1492, -0.1609],\n",
      "          [ 0.0581,  0.1254, -0.0758, -0.0985, -0.0491],\n",
      "          [ 0.0385, -0.0426,  0.1291, -0.1667,  0.0530],\n",
      "          [-0.1847,  0.1817, -0.1761, -0.0490,  0.1052],\n",
      "          [-0.0158, -0.0972, -0.0699, -0.0585,  0.0422]]],\n",
      "\n",
      "\n",
      "        [[[-0.1290,  0.0740, -0.1960,  0.1197, -0.2121],\n",
      "          [-0.1132, -0.0993, -0.0719,  0.1210, -0.1927],\n",
      "          [-0.1608,  0.0266, -0.1144, -0.1234,  0.1449],\n",
      "          [-0.0518,  0.0473, -0.0158,  0.0591, -0.1861],\n",
      "          [-0.1444, -0.1597,  0.1668, -0.0949,  0.0619]]],\n",
      "\n",
      "\n",
      "        [[[-0.0260, -0.1610, -0.0087,  0.1878, -0.0471],\n",
      "          [ 0.0208, -0.1951, -0.0533,  0.0065, -0.1599],\n",
      "          [-0.0274, -0.0910, -0.0093, -0.1148, -0.0837],\n",
      "          [-0.1454,  0.1133, -0.0086,  0.0076, -0.0154],\n",
      "          [ 0.1372,  0.0088, -0.1366,  0.0757, -0.1980]]],\n",
      "\n",
      "\n",
      "        [[[-0.1191, -0.1079, -0.0525, -0.2092, -0.1821],\n",
      "          [-0.2208,  0.1350, -0.1045,  0.0367, -0.0656],\n",
      "          [ 0.2145, -0.1643,  0.1399, -0.0457,  0.0070],\n",
      "          [ 0.0715, -0.0375,  0.2151, -0.1547,  0.1216],\n",
      "          [ 0.1202,  0.0983,  0.1527,  0.1357,  0.1246]]]], requires_grad=True): {'old_init': tensor([[[[-0.0152,  0.0386, -0.1857, -0.1259,  0.0449],\n",
      "          [ 0.0759,  0.0275,  0.0646, -0.0870, -0.1584],\n",
      "          [-0.1323,  0.0762,  0.0831,  0.1156,  0.0071],\n",
      "          [ 0.1702,  0.0632,  0.0493,  0.0004,  0.1970],\n",
      "          [ 0.1274, -0.1625, -0.1157,  0.0324,  0.1151]]],\n",
      "\n",
      "\n",
      "        [[[-0.0372,  0.1566,  0.0055,  0.1552,  0.1695],\n",
      "          [-0.1298,  0.1493,  0.0488,  0.1698, -0.0087],\n",
      "          [-0.1463, -0.0848, -0.0599, -0.1671,  0.0008],\n",
      "          [ 0.0395, -0.0326, -0.0025, -0.1419, -0.1266],\n",
      "          [ 0.0562,  0.1891,  0.1971,  0.0958,  0.1444]]],\n",
      "\n",
      "\n",
      "        [[[-0.0024, -0.0018, -0.1555,  0.0271,  0.0251],\n",
      "          [ 0.0881,  0.0476,  0.1273, -0.0854,  0.1086],\n",
      "          [ 0.0024,  0.1491, -0.0637,  0.0970,  0.0140],\n",
      "          [-0.0280, -0.0347,  0.1469, -0.1742, -0.0478],\n",
      "          [-0.0275, -0.0190, -0.0903,  0.1244,  0.0652]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0477, -0.1364, -0.1304, -0.0934,  0.1813],\n",
      "          [ 0.1439, -0.1863,  0.0850, -0.1653,  0.2010],\n",
      "          [-0.1756, -0.1918, -0.0653, -0.0690, -0.1052],\n",
      "          [-0.1325, -0.1737, -0.1309, -0.1341,  0.1612],\n",
      "          [ 0.0635,  0.1815, -0.1735, -0.0317,  0.0330]]],\n",
      "\n",
      "\n",
      "        [[[-0.1073, -0.1985,  0.0102, -0.1667, -0.1728],\n",
      "          [ 0.1718, -0.1388,  0.1744, -0.1905,  0.0423],\n",
      "          [ 0.0574, -0.1110,  0.1356, -0.1725,  0.1017],\n",
      "          [-0.1617, -0.0941, -0.1831, -0.0623, -0.0877],\n",
      "          [ 0.0744, -0.0582, -0.0429,  0.0141,  0.1858]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0595,  0.1826, -0.0433, -0.1696, -0.0952],\n",
      "          [ 0.1020,  0.1536, -0.0785, -0.1544,  0.0721],\n",
      "          [ 0.1073,  0.0370, -0.0289,  0.2015,  0.0399],\n",
      "          [-0.0380, -0.0941, -0.1910,  0.0866, -0.1735],\n",
      "          [ 0.1529,  0.1513,  0.1084,  0.0184,  0.0508]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0230, -0.1833, -0.1282,  0.1670, -0.1448],\n",
      "          [-0.0429, -0.1925,  0.0490, -0.1237,  0.0517],\n",
      "          [ 0.1884,  0.0144, -0.0966,  0.0894,  0.1150],\n",
      "          [-0.1623, -0.0973,  0.1976,  0.0590,  0.1257],\n",
      "          [-0.1922,  0.0811, -0.1917,  0.1439,  0.0464]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1886,  0.1154, -0.0527,  0.1223,  0.1283],\n",
      "          [ 0.0274,  0.0996,  0.0391, -0.0656,  0.0809],\n",
      "          [-0.0360,  0.1981, -0.1783,  0.1901, -0.0071],\n",
      "          [ 0.0025, -0.1209, -0.1283, -0.0763,  0.0456],\n",
      "          [ 0.0456,  0.1856,  0.1684, -0.1269, -0.0896]]],\n",
      "\n",
      "\n",
      "        [[[-0.1803,  0.0844,  0.0176,  0.0300,  0.0136],\n",
      "          [ 0.1944, -0.1746, -0.1113,  0.0008, -0.1678],\n",
      "          [-0.1079,  0.0400, -0.1120, -0.0939,  0.1270],\n",
      "          [ 0.0797,  0.0765, -0.1396, -0.0967,  0.1382],\n",
      "          [-0.0492, -0.0358,  0.1660, -0.0980,  0.1805]]],\n",
      "\n",
      "\n",
      "        [[[-0.0905,  0.0672,  0.1725, -0.1804,  0.0064],\n",
      "          [ 0.1572, -0.1944,  0.0274,  0.0042, -0.1084],\n",
      "          [ 0.1831,  0.0645,  0.0250, -0.0237,  0.1637],\n",
      "          [ 0.1474,  0.1325,  0.0963, -0.0480, -0.1729],\n",
      "          [ 0.1320, -0.0456, -0.0085,  0.0557, -0.0415]]],\n",
      "\n",
      "\n",
      "        [[[-0.0431,  0.0749,  0.0567,  0.0913, -0.0173],\n",
      "          [-0.1927,  0.1948,  0.1634,  0.1316, -0.1074],\n",
      "          [-0.0176,  0.0320,  0.1703, -0.0569,  0.0806],\n",
      "          [-0.0866,  0.0385,  0.1417,  0.1258,  0.0583],\n",
      "          [-0.1024,  0.1575,  0.0050,  0.0361, -0.0351]]],\n",
      "\n",
      "\n",
      "        [[[-0.1710,  0.0959,  0.0859,  0.1934, -0.1124],\n",
      "          [ 0.0389, -0.1318,  0.1484, -0.0972,  0.0633],\n",
      "          [ 0.0069, -0.1007,  0.1038,  0.0829, -0.1468],\n",
      "          [ 0.1986,  0.0411,  0.1542, -0.1694,  0.1883],\n",
      "          [-0.1018, -0.1374,  0.0124,  0.1893,  0.0081]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1251, -0.1244,  0.1572, -0.0280,  0.0052],\n",
      "          [-0.1216, -0.0892, -0.1198, -0.0778, -0.1979],\n",
      "          [ 0.0866, -0.1720,  0.0683, -0.1439, -0.1753],\n",
      "          [-0.1781,  0.1923, -0.1146,  0.1725, -0.1119],\n",
      "          [ 0.0117, -0.0818,  0.0193,  0.1374,  0.0325]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1084, -0.1389,  0.1719, -0.1924, -0.1475],\n",
      "          [ 0.0411, -0.1488,  0.1454,  0.0603, -0.0269],\n",
      "          [-0.0699,  0.1974, -0.0028,  0.0197, -0.0368],\n",
      "          [-0.0175,  0.1351,  0.0616, -0.0519, -0.1321],\n",
      "          [-0.1060, -0.1300, -0.1506,  0.0127, -0.1086]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0201,  0.0771,  0.1142, -0.0948, -0.0440],\n",
      "          [ 0.0595, -0.0318,  0.1449,  0.0009, -0.0008],\n",
      "          [-0.0471, -0.1449,  0.1396,  0.0340,  0.1216],\n",
      "          [ 0.1542, -0.0669,  0.0037, -0.0425,  0.0316],\n",
      "          [ 0.0299, -0.1717, -0.1425,  0.0301, -0.0656]]],\n",
      "\n",
      "\n",
      "        [[[-0.1616, -0.1006,  0.0118, -0.0837, -0.0611],\n",
      "          [ 0.0457, -0.0545, -0.1394,  0.0267,  0.1666],\n",
      "          [-0.1407,  0.0643,  0.0728, -0.0080,  0.1470],\n",
      "          [ 0.0779,  0.1736, -0.1783,  0.0114,  0.1641],\n",
      "          [ 0.1532, -0.0358, -0.1317, -0.1161, -0.0059]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1487,  0.1396,  0.1898,  0.2061,  0.1425],\n",
      "          [ 0.0374,  0.0989,  0.1337, -0.0301, -0.0937],\n",
      "          [ 0.0058, -0.0086,  0.1076, -0.0974, -0.1181],\n",
      "          [-0.1817,  0.1607,  0.1002, -0.0092,  0.1923],\n",
      "          [-0.0049, -0.0880,  0.1694, -0.1278,  0.1519]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0508,  0.1124,  0.1443, -0.1432,  0.1463],\n",
      "          [-0.1509,  0.1322,  0.1359, -0.0763,  0.1929],\n",
      "          [ 0.1529, -0.0030,  0.0661,  0.0419, -0.0780],\n",
      "          [-0.0642,  0.0051, -0.1784, -0.0909, -0.0187],\n",
      "          [ 0.0708, -0.0608, -0.1534, -0.1253,  0.1936]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1965, -0.1381,  0.1828,  0.1172,  0.0639],\n",
      "          [-0.0640,  0.0604, -0.0892,  0.1004,  0.0318],\n",
      "          [-0.0540,  0.0097, -0.1111, -0.1160,  0.1498],\n",
      "          [-0.0576, -0.1234,  0.1718, -0.1387, -0.0076],\n",
      "          [-0.1519, -0.1910,  0.0961,  0.1909,  0.1467]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0140,  0.1391,  0.0101,  0.0170, -0.1416],\n",
      "          [-0.0431,  0.0249, -0.0728,  0.1794,  0.0083],\n",
      "          [ 0.0748, -0.1763,  0.1295,  0.1052, -0.1135],\n",
      "          [ 0.1456,  0.0483, -0.0860, -0.0580,  0.1848],\n",
      "          [-0.0545,  0.1899, -0.1692,  0.1652,  0.1359]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1929,  0.1725, -0.1544, -0.1523,  0.1434],\n",
      "          [ 0.0150,  0.0705,  0.0660,  0.1043,  0.1766],\n",
      "          [ 0.0908, -0.1699, -0.0748, -0.0482, -0.0769],\n",
      "          [-0.1621, -0.1562, -0.0464, -0.1193, -0.1743],\n",
      "          [ 0.0320, -0.0532, -0.0827,  0.0716,  0.1355]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0324, -0.1384, -0.1822, -0.0865, -0.0304],\n",
      "          [-0.0874,  0.0720,  0.0351, -0.0851,  0.0638],\n",
      "          [ 0.0008,  0.0268, -0.0026,  0.1466,  0.1324],\n",
      "          [-0.0324,  0.0514, -0.1477, -0.0563,  0.1445],\n",
      "          [-0.1676, -0.1892,  0.1035,  0.0217, -0.0269]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1566,  0.1982,  0.2012,  0.0467,  0.1341],\n",
      "          [-0.1758, -0.0684, -0.1491, -0.0833, -0.1503],\n",
      "          [ 0.0382, -0.0608,  0.1872,  0.1754, -0.1834],\n",
      "          [-0.0984,  0.0626,  0.0218, -0.1976,  0.1927],\n",
      "          [ 0.0284, -0.0061,  0.0373,  0.1648, -0.1281]]],\n",
      "\n",
      "\n",
      "        [[[-0.0862,  0.0757, -0.0994, -0.0675, -0.0109],\n",
      "          [ 0.0647, -0.1737,  0.0891,  0.0442,  0.0137],\n",
      "          [ 0.0670, -0.0391, -0.0472, -0.1127,  0.1518],\n",
      "          [ 0.1914, -0.0186, -0.0848,  0.1347,  0.1913],\n",
      "          [-0.1766, -0.0806, -0.0013, -0.0791,  0.0682]]],\n",
      "\n",
      "\n",
      "        [[[-0.0282, -0.1343, -0.1901,  0.0987,  0.0981],\n",
      "          [-0.1866,  0.1324, -0.0892,  0.1291, -0.1765],\n",
      "          [-0.1439,  0.1072, -0.0331, -0.0277, -0.1493],\n",
      "          [-0.1698, -0.0214,  0.1714,  0.1197, -0.0188],\n",
      "          [-0.1590, -0.1142, -0.1661,  0.0675, -0.1524]]],\n",
      "\n",
      "\n",
      "        [[[-0.1539,  0.1004,  0.0761,  0.0871,  0.0096],\n",
      "          [ 0.0486,  0.0213, -0.0878,  0.2010,  0.0486],\n",
      "          [ 0.0754,  0.0430, -0.1219, -0.1061, -0.1713],\n",
      "          [-0.0520, -0.0800,  0.1372,  0.0812,  0.0217],\n",
      "          [-0.0214, -0.1069,  0.0513,  0.1281,  0.1611]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0779, -0.1636, -0.0921,  0.0246, -0.1633],\n",
      "          [ 0.1965, -0.1053, -0.0969, -0.0293, -0.1930],\n",
      "          [-0.0015,  0.1042,  0.1207,  0.0950,  0.1729],\n",
      "          [ 0.1402,  0.0450,  0.1160,  0.1089, -0.1816],\n",
      "          [ 0.2011, -0.1109, -0.1863,  0.0595, -0.1705]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1029,  0.0774, -0.1909,  0.1519, -0.0276],\n",
      "          [ 0.1548,  0.1980,  0.1968,  0.1334, -0.0076],\n",
      "          [ 0.0989, -0.1721, -0.0771,  0.0728,  0.0091],\n",
      "          [ 0.0727,  0.0696, -0.0844, -0.1031, -0.1809],\n",
      "          [-0.0808, -0.1465, -0.0490, -0.1040,  0.0738]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1688,  0.0992,  0.1947, -0.1457, -0.1433],\n",
      "          [ 0.0727,  0.1356, -0.0551, -0.1076, -0.0527],\n",
      "          [ 0.0629, -0.0227,  0.1369, -0.1776,  0.0321],\n",
      "          [-0.1736,  0.1888, -0.1870, -0.0627,  0.0882],\n",
      "          [-0.0228, -0.1034, -0.0808, -0.0636,  0.0443]]],\n",
      "\n",
      "\n",
      "        [[[-0.1285,  0.0774, -0.1589,  0.1692, -0.1979],\n",
      "          [-0.1155, -0.1007, -0.0506,  0.1736, -0.1556],\n",
      "          [-0.1636,  0.0238, -0.1127, -0.0875,  0.1941],\n",
      "          [-0.0541,  0.0484, -0.0071,  0.0744, -0.1486],\n",
      "          [-0.1375, -0.1526,  0.1776, -0.0893,  0.0776]]],\n",
      "\n",
      "\n",
      "        [[[-0.0146, -0.1678, -0.0129,  0.1797, -0.0573],\n",
      "          [ 0.0424, -0.1938, -0.0580, -0.0025, -0.1767],\n",
      "          [ 0.0003, -0.0705, -0.0041, -0.1216, -0.0830],\n",
      "          [-0.1024,  0.1557,  0.0191,  0.0182, -0.0061],\n",
      "          [ 0.1597,  0.0347, -0.1171,  0.0839, -0.1919]]],\n",
      "\n",
      "\n",
      "        [[[-0.0687, -0.0541, -0.0125, -0.1856, -0.1628],\n",
      "          [-0.1945,  0.1688, -0.0685,  0.0450, -0.0609],\n",
      "          [ 0.1902, -0.1606,  0.1492, -0.0622, -0.0159],\n",
      "          [ 0.0233, -0.0643,  0.1896, -0.1938,  0.0679],\n",
      "          [ 0.0488,  0.0222,  0.0765,  0.0598,  0.0466]]]])}, Parameter containing:\n",
      "tensor([ 0.1843, -0.0959,  0.0776,  0.0679,  0.2058, -0.0646, -0.0948, -0.0818,\n",
      "        -0.1473,  0.1604, -0.1550, -0.0956,  0.0592,  0.0685, -0.0744, -0.0407,\n",
      "        -0.1664, -0.0991, -0.0954, -0.0941, -0.1528,  0.0499,  0.1375, -0.1847,\n",
      "        -0.0059, -0.1087, -0.0830, -0.0058,  0.0077, -0.0898,  0.0126, -0.0229],\n",
      "       requires_grad=True): {'old_init': tensor([ 0.1537, -0.1086,  0.0854,  0.0634,  0.1898, -0.0667, -0.1316, -0.0806,\n",
      "        -0.1724,  0.1963, -0.1622, -0.1082,  0.0140,  0.0677, -0.0783, -0.0614,\n",
      "        -0.1944, -0.1073, -0.1193, -0.1313, -0.1802,  0.0343,  0.1196, -0.1539,\n",
      "        -0.0257, -0.1086, -0.1013, -0.0185,  0.0322, -0.0832,  0.0260, -0.0789])}, Parameter containing:\n",
      "tensor([[[[ 1.6701e-02,  8.7833e-03,  4.1980e-02,  3.7858e-02, -6.0850e-03],\n",
      "          [-1.5669e-02, -1.2226e-02, -3.8945e-02,  3.5946e-02, -2.9957e-02],\n",
      "          [-5.1612e-02,  5.0463e-02,  7.8135e-02,  5.3092e-03,  4.8493e-02],\n",
      "          [ 7.4844e-04,  2.2790e-02,  6.3969e-02,  2.7960e-02,  1.7805e-02],\n",
      "          [ 4.1365e-02,  2.7963e-02,  7.2958e-04, -1.2172e-03,  4.3596e-02]],\n",
      "\n",
      "         [[-5.5195e-04,  3.5474e-02,  4.2482e-02, -4.0888e-03,  2.3075e-02],\n",
      "          [-1.8875e-02, -8.1097e-04,  6.3287e-02,  5.8074e-02,  1.6550e-02],\n",
      "          [-1.2617e-02, -4.1441e-03, -1.2805e-02, -1.0872e-02,  3.0019e-02],\n",
      "          [-3.6194e-02, -1.3910e-02,  1.7109e-02,  5.1690e-02,  4.5130e-02],\n",
      "          [ 2.9172e-02, -4.9641e-03,  1.7892e-02,  6.9318e-02,  2.4951e-02]],\n",
      "\n",
      "         [[ 3.1722e-02,  5.2252e-02,  2.3147e-02,  3.0718e-02,  3.9214e-03],\n",
      "          [ 2.5326e-02, -1.5305e-03,  2.9193e-02,  8.8923e-03,  2.7550e-02],\n",
      "          [ 1.0631e-02, -2.8263e-02,  3.8224e-02,  3.5795e-02,  1.9196e-03],\n",
      "          [-1.2026e-02,  7.9063e-03,  1.0707e-02,  4.1354e-02,  2.2981e-02],\n",
      "          [ 2.4357e-02, -7.6240e-03,  1.7462e-02,  1.6239e-02,  2.0307e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.0148e-02, -1.2774e-02, -2.1658e-02,  2.4022e-02,  3.6424e-03],\n",
      "          [-1.0152e-02, -1.7737e-02,  3.6860e-02,  1.3165e-02, -3.2402e-02],\n",
      "          [ 7.0377e-03,  1.4510e-02, -1.5025e-02, -1.2067e-02,  2.4068e-02],\n",
      "          [ 2.1438e-02,  4.0998e-02,  3.1523e-02, -3.9569e-02,  3.4165e-03],\n",
      "          [-1.6811e-02, -1.0333e-02, -3.4366e-03, -2.9226e-02, -1.3677e-02]],\n",
      "\n",
      "         [[ 2.2554e-02, -3.0753e-02,  2.5018e-02, -2.7895e-02, -2.5701e-02],\n",
      "          [-1.0768e-03, -2.1928e-02,  3.2152e-02, -1.4097e-02,  2.0152e-02],\n",
      "          [-2.7385e-02, -3.1126e-02,  7.2339e-03, -4.3070e-03, -1.0107e-02],\n",
      "          [-1.7624e-02,  1.0794e-02, -1.6516e-02,  1.5664e-03,  1.1380e-02],\n",
      "          [ 2.7031e-02,  9.8262e-04, -2.6768e-02, -8.7899e-03,  3.3382e-02]],\n",
      "\n",
      "         [[-2.5464e-02, -1.9484e-02, -8.5022e-03, -9.7516e-03,  2.5068e-02],\n",
      "          [-3.5801e-02, -1.8048e-02,  7.0868e-03, -2.5862e-02,  1.8760e-03],\n",
      "          [ 2.9388e-02,  7.6436e-03,  4.2682e-02,  3.8518e-02,  1.7517e-02],\n",
      "          [ 1.7322e-02, -2.2063e-02,  1.9047e-03,  2.5052e-02,  4.4638e-03],\n",
      "          [-3.1992e-02,  4.0722e-02, -2.1561e-02, -1.5278e-02,  1.6855e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.5766e-02,  3.9926e-02, -2.1767e-02, -7.4832e-02,  2.2026e-02],\n",
      "          [ 3.6630e-02,  2.4216e-02,  3.0323e-02, -2.8563e-02, -3.5186e-02],\n",
      "          [ 1.4701e-02, -7.5331e-05, -2.3219e-02, -2.0965e-02, -3.1268e-02],\n",
      "          [-4.8909e-02, -2.9488e-02, -3.9316e-02, -3.3757e-02, -4.1905e-03],\n",
      "          [-3.0306e-02, -2.8605e-02,  8.9910e-03,  2.5681e-02, -1.5436e-02]],\n",
      "\n",
      "         [[-2.7259e-02, -6.5152e-03, -3.4704e-02, -6.2945e-03, -1.5469e-02],\n",
      "          [ 3.0203e-02, -2.5674e-02, -5.4877e-04, -3.4844e-02, -2.9931e-02],\n",
      "          [-5.3138e-03,  2.5839e-02, -3.6758e-02,  8.9743e-03, -2.8170e-02],\n",
      "          [-9.5193e-03, -2.5296e-02, -2.0372e-02, -4.1195e-03, -3.2060e-02],\n",
      "          [-2.1134e-02, -1.9379e-02, -6.6554e-02, -2.0431e-03,  9.3959e-03]],\n",
      "\n",
      "         [[ 5.2276e-03, -4.1530e-02, -2.8019e-02, -4.3656e-02, -3.4915e-03],\n",
      "          [ 3.9160e-02,  1.3785e-02, -6.2577e-03, -8.8990e-03,  1.2652e-02],\n",
      "          [ 3.7255e-02, -2.0245e-02,  3.4209e-02, -3.7810e-02, -2.8125e-03],\n",
      "          [-1.8318e-02, -4.4881e-02, -2.5931e-02, -5.1157e-02, -1.1654e-02],\n",
      "          [-5.5285e-02, -4.9060e-02, -4.8782e-02, -3.9164e-02,  5.2110e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.8893e-02,  9.8774e-03, -9.4403e-03, -7.1816e-03, -2.0629e-02],\n",
      "          [ 1.4333e-02, -3.7053e-02, -4.9528e-02,  1.9381e-02, -7.9162e-03],\n",
      "          [-1.0022e-02, -2.1747e-02,  2.2375e-03, -1.2178e-03,  2.2664e-02],\n",
      "          [ 2.0728e-02,  7.5902e-03,  2.0427e-02, -1.2019e-02, -1.7492e-02],\n",
      "          [ 9.1529e-03, -1.2740e-02,  2.4201e-02, -1.3157e-02, -2.8677e-02]],\n",
      "\n",
      "         [[ 2.5423e-02, -7.1705e-03,  9.6423e-03,  1.7813e-02,  2.4837e-02],\n",
      "          [-2.8538e-02,  2.6121e-03,  3.1367e-02,  3.9089e-02,  1.3789e-02],\n",
      "          [ 7.8296e-03,  2.3037e-02, -1.1186e-02, -1.7479e-02,  6.1687e-03],\n",
      "          [-1.8788e-02,  1.1597e-02, -1.7674e-02, -2.2384e-02,  2.1971e-02],\n",
      "          [-3.1496e-03,  2.4127e-02,  1.3948e-02, -7.7528e-03, -1.4848e-02]],\n",
      "\n",
      "         [[ 1.2994e-02, -1.2133e-03,  1.4922e-02,  1.2423e-02, -5.9958e-03],\n",
      "          [-2.0267e-02,  4.0791e-02, -6.0847e-03, -3.4430e-02, -4.8108e-02],\n",
      "          [-3.8602e-02, -2.2341e-02, -1.3367e-02,  1.0562e-02,  2.6229e-02],\n",
      "          [-4.5766e-02, -5.2034e-02, -2.3326e-02, -2.8351e-03,  3.6054e-02],\n",
      "          [-1.0711e-02, -2.6859e-02, -3.0706e-02,  1.1889e-02,  2.4134e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 7.5486e-02,  6.7498e-02,  6.2558e-02,  8.7704e-02,  7.7898e-02],\n",
      "          [ 1.3064e-03,  7.1163e-02,  4.6591e-02,  9.3389e-03,  3.4769e-02],\n",
      "          [ 7.8726e-03, -4.6812e-04,  7.8602e-02,  3.9835e-02,  1.7684e-02],\n",
      "          [ 5.3654e-02,  1.1084e-02,  4.2242e-02,  9.3810e-02,  3.8566e-02],\n",
      "          [ 5.0470e-02,  8.5136e-02,  3.1466e-02,  5.9155e-02,  9.5728e-02]],\n",
      "\n",
      "         [[ 4.9700e-02,  4.4671e-02,  7.9067e-02,  4.8174e-02, -2.1819e-03],\n",
      "          [ 4.9305e-02,  3.0071e-02,  6.3161e-02,  7.6519e-02,  5.8998e-02],\n",
      "          [ 9.6251e-03,  6.3592e-02,  2.3504e-02,  6.9000e-02,  4.5552e-02],\n",
      "          [ 2.0726e-02,  7.5719e-02,  8.0766e-02,  5.4224e-02,  8.3376e-02],\n",
      "          [ 3.8435e-02,  7.0363e-02,  4.5328e-02,  3.2340e-02,  1.4881e-02]],\n",
      "\n",
      "         [[ 3.5907e-02,  4.0328e-02,  2.5759e-02,  6.4826e-02,  3.3397e-02],\n",
      "          [ 6.3424e-02,  2.1796e-02,  5.8838e-02,  4.3417e-02, -6.7552e-03],\n",
      "          [-8.1796e-03,  2.9062e-02,  3.6528e-02,  4.3180e-02,  2.5438e-02],\n",
      "          [-6.5667e-03, -2.0411e-03,  2.8392e-02,  1.0146e-02,  2.5474e-02],\n",
      "          [ 5.4668e-03,  7.4851e-02,  4.7259e-02,  2.2703e-02,  2.8898e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.8396e-02, -1.9422e-03, -3.9727e-03, -4.5143e-03, -5.2103e-03],\n",
      "          [ 1.5672e-02,  1.3098e-03,  2.3505e-02, -1.6070e-02,  2.1796e-02],\n",
      "          [ 3.8295e-02,  3.2013e-02,  7.2800e-03,  2.1183e-02,  3.4831e-02],\n",
      "          [ 9.0370e-03,  5.5142e-02,  1.3368e-02,  8.1202e-03,  3.5873e-02],\n",
      "          [-1.2659e-02,  2.1763e-02, -8.3004e-03,  2.0973e-04,  1.7948e-02]],\n",
      "\n",
      "         [[ 2.1720e-02, -1.9555e-02,  2.7172e-02, -1.2684e-02, -2.3608e-02],\n",
      "          [ 4.0483e-02,  3.4415e-02, -1.0544e-02,  1.5613e-02,  3.5787e-02],\n",
      "          [ 3.4318e-03,  3.6490e-02, -1.0694e-02,  1.3899e-02,  2.7772e-02],\n",
      "          [ 4.7515e-03,  6.1966e-02,  5.5944e-02,  5.8417e-03,  1.2195e-02],\n",
      "          [ 1.9169e-02,  9.3469e-04, -1.0369e-02,  4.8194e-02,  3.9572e-02]],\n",
      "\n",
      "         [[-1.2776e-02,  3.4195e-02,  5.2614e-02,  3.4921e-02,  6.8041e-02],\n",
      "          [ 2.5679e-02,  3.0943e-02, -2.1855e-02, -1.6246e-02,  3.8805e-02],\n",
      "          [ 3.5221e-02,  1.3969e-02, -1.4071e-02,  3.1711e-02,  5.7022e-02],\n",
      "          [ 3.1370e-02,  6.3395e-02,  4.6080e-02,  5.6186e-02,  4.6229e-02],\n",
      "          [-7.7025e-03,  6.9363e-02,  4.9700e-02,  3.6046e-02,  3.1270e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.7058e-02, -5.2795e-02, -3.4652e-02, -2.3286e-03, -7.4170e-03],\n",
      "          [-8.9162e-03, -6.7074e-03, -3.8416e-02,  1.9568e-02, -3.8761e-02],\n",
      "          [ 2.3069e-02,  5.2704e-02,  5.2155e-03,  4.8273e-02, -1.0729e-02],\n",
      "          [-5.6284e-03,  3.4518e-02,  2.6500e-03,  1.4114e-02, -1.2313e-02],\n",
      "          [-5.5497e-02, -3.9384e-02, -3.0044e-03,  1.9407e-02, -2.7139e-02]],\n",
      "\n",
      "         [[-4.5080e-03,  1.0727e-02, -6.3163e-02, -4.7308e-02, -2.4307e-02],\n",
      "          [-1.9856e-02, -2.0083e-02, -1.9645e-02,  8.8463e-03, -2.0419e-04],\n",
      "          [-2.1994e-03, -2.1192e-02,  3.2458e-02,  1.2301e-02, -3.5353e-02],\n",
      "          [-7.6475e-03,  1.1326e-02, -7.6018e-03,  3.3649e-02,  1.9614e-02],\n",
      "          [ 1.9621e-02,  1.3456e-02,  4.3862e-02,  2.1138e-02, -1.8012e-02]],\n",
      "\n",
      "         [[ 1.9028e-02, -3.6069e-02, -7.9748e-03, -5.0456e-03, -2.5015e-02],\n",
      "          [-6.9235e-03, -4.0440e-02, -6.0963e-04,  2.5022e-03,  1.5958e-02],\n",
      "          [ 1.0367e-02, -2.4537e-02,  3.4420e-02,  2.4649e-02, -3.5087e-02],\n",
      "          [-3.5065e-02, -2.8464e-02,  2.4531e-02,  2.6704e-03,  4.3844e-03],\n",
      "          [-2.9114e-02,  7.5812e-03,  2.3086e-02,  2.7288e-02, -1.4773e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.8403e-02,  1.2903e-02, -5.6831e-03,  1.9427e-02, -3.2759e-02],\n",
      "          [ 2.8646e-02,  1.2029e-02, -1.2850e-02,  1.4479e-02,  2.8229e-02],\n",
      "          [-3.1469e-02,  4.4504e-03, -2.3946e-02, -3.2359e-02,  2.9300e-02],\n",
      "          [ 3.2756e-02,  1.7864e-02,  2.3634e-02,  2.1147e-02, -2.9838e-02],\n",
      "          [ 1.8764e-02,  3.6638e-03, -1.6790e-02, -1.4154e-02, -3.1937e-02]],\n",
      "\n",
      "         [[-9.4800e-03, -9.8383e-03, -2.8863e-02, -3.1875e-02, -7.3504e-03],\n",
      "          [ 2.6102e-02,  1.4085e-02,  1.9846e-02, -6.4607e-03,  1.6943e-03],\n",
      "          [ 2.1051e-03,  2.3550e-02, -2.1308e-02,  1.1342e-03,  2.5361e-02],\n",
      "          [-3.8917e-02, -8.7427e-03, -3.9521e-02,  3.5069e-02,  3.2605e-02],\n",
      "          [-2.3650e-02, -4.2749e-02, -3.5875e-02,  1.6359e-02,  2.5714e-02]],\n",
      "\n",
      "         [[ 3.2827e-03,  5.0575e-05, -1.4786e-02, -1.8725e-02, -1.9671e-03],\n",
      "          [-1.2127e-02,  2.7171e-02,  3.6657e-02, -2.0267e-02,  5.7942e-03],\n",
      "          [-9.3062e-03,  2.1211e-02,  4.7076e-02,  1.3863e-02, -6.6550e-03],\n",
      "          [ 1.0039e-02, -3.9061e-02, -3.1022e-02,  3.7258e-02,  2.9949e-02],\n",
      "          [ 6.4180e-03, -2.7931e-02, -3.4090e-02, -4.2421e-02,  2.7882e-02]]],\n",
      "\n",
      "\n",
      "        [[[-2.2366e-02,  2.9919e-02, -8.3848e-03,  9.2272e-03, -7.5370e-03],\n",
      "          [ 5.0282e-02,  4.7428e-02,  4.1480e-02,  4.0492e-02,  4.7914e-02],\n",
      "          [ 6.3274e-02,  9.4855e-02,  8.0329e-02,  6.3571e-02,  1.1703e-02],\n",
      "          [ 2.3097e-02, -1.3911e-02,  2.7665e-02,  2.9644e-02,  5.9628e-03],\n",
      "          [-3.7817e-02, -1.6567e-02,  4.0527e-02, -2.4802e-02,  5.6563e-02]],\n",
      "\n",
      "         [[-3.5349e-02, -5.3724e-03, -1.1715e-02, -3.3646e-03,  5.6507e-02],\n",
      "          [ 3.8989e-02,  6.6448e-02,  6.1516e-02,  7.5144e-02,  6.5767e-02],\n",
      "          [ 5.3751e-03, -2.6550e-02, -4.0083e-02,  3.8167e-02, -1.2395e-02],\n",
      "          [ 1.2117e-03,  6.6735e-02,  4.4510e-02,  2.7192e-02, -4.1008e-03],\n",
      "          [-1.2178e-02,  1.4865e-02,  2.6927e-02, -4.2599e-03,  3.3515e-02]],\n",
      "\n",
      "         [[-4.4922e-02, -2.4109e-02,  3.5538e-03,  1.9545e-02, -1.0128e-02],\n",
      "          [ 3.4883e-03, -2.2908e-03, -3.6823e-02, -2.0715e-02,  5.6279e-03],\n",
      "          [ 3.0970e-02, -8.5952e-03,  1.0413e-02,  4.5054e-02,  2.8178e-02],\n",
      "          [ 7.6876e-02,  7.3889e-02,  2.2496e-02,  4.3884e-02,  2.4221e-02],\n",
      "          [-1.6576e-02,  8.0335e-03,  3.2250e-02, -1.3278e-03,  1.1933e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-6.8328e-03,  1.5858e-02, -1.4926e-02,  1.7709e-02, -2.4187e-02],\n",
      "          [ 3.1522e-03,  3.5438e-02,  1.8905e-02, -1.4843e-02,  2.6914e-02],\n",
      "          [-8.9474e-03, -3.8492e-02,  7.1632e-03,  2.2829e-03,  1.5085e-02],\n",
      "          [-2.2545e-03,  4.0681e-03, -1.3269e-02,  1.8366e-02, -5.4624e-03],\n",
      "          [ 1.4978e-02, -2.0123e-02,  1.0244e-02, -2.1811e-02, -2.2933e-02]],\n",
      "\n",
      "         [[-1.2696e-03,  1.8283e-02,  2.4881e-02, -5.6405e-03, -1.0816e-02],\n",
      "          [ 2.1288e-02,  5.1898e-03,  7.3735e-04,  4.3487e-02,  3.3601e-02],\n",
      "          [ 2.1421e-02, -7.5332e-03, -1.9243e-02, -2.8647e-02,  1.5879e-02],\n",
      "          [ 2.4400e-02, -2.6828e-02,  3.3001e-02,  6.6647e-03, -2.1903e-02],\n",
      "          [-1.3811e-02, -2.4667e-02, -1.0836e-02,  3.6637e-02,  2.4798e-02]],\n",
      "\n",
      "         [[-4.3620e-03, -3.4383e-03,  1.8035e-03,  2.4731e-02, -5.6520e-03],\n",
      "          [ 3.9523e-02,  2.2889e-02,  3.9929e-02,  3.9909e-02,  5.6753e-02],\n",
      "          [ 2.6256e-03,  5.8318e-02,  2.4715e-02,  1.1824e-02,  3.1545e-02],\n",
      "          [-3.3405e-02,  1.7741e-02, -1.8027e-02, -9.0935e-03, -3.4300e-02],\n",
      "          [ 1.1639e-02,  2.1164e-02,  1.0904e-02, -3.8901e-03, -2.0859e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 2.5332e-02,  8.0130e-02,  9.4993e-02,  1.0870e-01,  4.4522e-02],\n",
      "          [ 2.6898e-02,  7.3872e-02,  6.2344e-02,  6.3539e-02,  6.4537e-02],\n",
      "          [ 2.8654e-02,  2.9506e-02,  6.0525e-02,  4.5299e-02,  4.7288e-02],\n",
      "          [ 1.3750e-02,  2.5594e-02,  2.3160e-02,  5.9934e-02, -1.0789e-02],\n",
      "          [ 8.5313e-02,  1.2587e-02,  1.3390e-03,  5.3953e-03,  3.4275e-02]],\n",
      "\n",
      "         [[ 5.8431e-02,  6.5170e-02,  2.9313e-02,  2.4955e-02,  1.0869e-02],\n",
      "          [ 1.3354e-02,  4.6082e-02,  7.9081e-02,  9.6011e-02,  4.7767e-02],\n",
      "          [ 4.6133e-02,  2.0277e-03,  5.5257e-02,  8.1858e-02,  6.2910e-03],\n",
      "          [ 2.5126e-02,  3.9356e-02,  4.9666e-02,  1.4565e-02,  1.9015e-02],\n",
      "          [ 8.7123e-03,  2.8684e-02,  6.5328e-02,  6.3735e-03,  6.9387e-03]],\n",
      "\n",
      "         [[ 3.3657e-02, -4.3515e-03,  2.3293e-02,  4.6264e-02,  4.7670e-02],\n",
      "          [ 5.6113e-02,  3.1452e-02,  6.8932e-03,  6.5525e-02,  4.1323e-02],\n",
      "          [-6.0547e-04,  2.3368e-02, -7.7998e-03,  2.8468e-02,  3.9617e-02],\n",
      "          [-1.1232e-02,  1.3823e-04,  1.2489e-02,  3.7703e-02,  5.2310e-02],\n",
      "          [ 4.0039e-02,  3.8251e-03,  3.5750e-02, -1.2510e-02,  6.0444e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.6183e-02, -1.3638e-02,  3.3930e-02,  3.1793e-02,  2.7209e-02],\n",
      "          [-2.2973e-02, -1.3569e-02,  1.5023e-02,  2.6491e-02,  9.1223e-03],\n",
      "          [ 3.5701e-02,  1.3964e-03,  1.1409e-02,  1.6065e-02, -1.3533e-02],\n",
      "          [-4.3766e-03, -2.4820e-02, -2.6851e-02,  5.4070e-03,  2.6896e-03],\n",
      "          [ 2.5636e-02, -9.6206e-04, -1.9188e-02,  2.6695e-02,  1.8975e-02]],\n",
      "\n",
      "         [[ 2.8120e-02,  4.4768e-02,  4.4436e-02,  1.5505e-02,  1.1827e-02],\n",
      "          [ 3.9214e-02, -9.9129e-03,  1.5771e-02, -9.2354e-04,  5.9653e-03],\n",
      "          [ 2.0886e-02, -1.0248e-03,  3.2420e-02, -5.8766e-03,  3.1024e-02],\n",
      "          [ 2.8268e-02,  3.7177e-02, -1.5416e-02,  1.2821e-02,  3.1573e-02],\n",
      "          [ 2.3242e-02, -7.3273e-03,  2.5613e-02,  4.7608e-02,  7.9237e-04]],\n",
      "\n",
      "         [[ 4.9121e-02,  1.7367e-02,  3.7813e-02,  4.2962e-02,  3.9087e-02],\n",
      "          [ 1.0069e-02,  5.8485e-03,  3.0465e-02,  2.6392e-02,  2.0558e-02],\n",
      "          [-6.4752e-03,  7.9696e-03,  5.1215e-02,  4.1546e-02, -2.1053e-02],\n",
      "          [-2.2668e-03, -1.3547e-02,  3.0162e-02, -3.2775e-03, -2.7632e-02],\n",
      "          [ 3.5585e-02,  2.5011e-02, -1.7435e-02, -3.6892e-02,  2.7566e-02]]]],\n",
      "       requires_grad=True): {'old_init': tensor([[[[-6.4765e-03,  7.3803e-03,  2.5480e-02,  2.4371e-02, -3.2968e-03],\n",
      "          [ 1.6886e-02, -1.5386e-02, -2.3004e-02,  3.6043e-02, -2.4326e-02],\n",
      "          [-1.5699e-02,  3.2207e-02,  2.1078e-02, -2.2518e-02,  1.4364e-02],\n",
      "          [-2.1528e-02,  1.6049e-02,  2.7127e-02, -1.0879e-02,  7.4092e-04],\n",
      "          [ 3.5284e-02,  1.4860e-02,  1.0671e-02, -8.9182e-03,  2.9019e-02]],\n",
      "\n",
      "         [[-1.5163e-02,  2.1042e-02,  2.1625e-02, -8.6181e-03,  2.1281e-02],\n",
      "          [-2.0626e-02, -1.6276e-02,  2.6069e-02,  2.1782e-02, -1.0269e-02],\n",
      "          [-2.3562e-02, -1.8923e-02, -1.1663e-02, -2.3557e-02,  1.7797e-02],\n",
      "          [-1.0999e-02, -1.7866e-02, -3.6086e-03,  2.8493e-02,  3.2828e-02],\n",
      "          [ 2.7833e-02, -2.3068e-02, -2.2699e-02,  1.9343e-02, -6.7620e-03]],\n",
      "\n",
      "         [[ 8.7043e-03,  2.8378e-02,  1.5344e-03,  9.3049e-03, -1.5086e-02],\n",
      "          [ 2.4666e-02, -1.6046e-02,  1.7028e-02,  4.4643e-03,  2.9780e-02],\n",
      "          [ 2.8843e-02, -1.8926e-02,  2.6302e-02,  2.2758e-02, -6.4968e-03],\n",
      "          [-1.0843e-02, -5.7582e-04, -9.3330e-03,  3.8648e-03, -3.6924e-03],\n",
      "          [ 1.6116e-02, -2.1310e-02,  7.4524e-03,  7.5554e-03,  1.0581e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.6336e-02, -1.3085e-02, -1.9149e-02,  2.6332e-02,  6.2570e-03],\n",
      "          [-1.9044e-02, -2.0538e-02,  3.3376e-02,  1.4146e-02, -2.9431e-02],\n",
      "          [-7.0705e-03,  1.5687e-03, -1.8393e-02, -1.3124e-02,  3.3533e-02],\n",
      "          [ 1.7270e-02,  3.1502e-02,  2.8758e-02, -3.3917e-02,  7.7323e-03],\n",
      "          [-2.2208e-02, -8.9644e-03, -1.1637e-03, -2.5621e-02, -7.4421e-03]],\n",
      "\n",
      "         [[ 2.9049e-02, -2.0815e-02,  3.2771e-02, -2.0675e-02, -2.5956e-02],\n",
      "          [-2.0361e-03, -1.0301e-02,  3.3802e-02, -1.7982e-02,  1.8264e-02],\n",
      "          [-3.2010e-02, -3.2081e-02,  8.9745e-03, -6.1530e-03, -2.1910e-02],\n",
      "          [-2.1739e-02,  7.6884e-03, -9.5918e-03,  1.4216e-02,  5.8183e-03],\n",
      "          [ 3.0430e-02,  7.7848e-03, -2.3972e-02, -4.7580e-03,  3.0861e-02]],\n",
      "\n",
      "         [[-2.5172e-02, -1.2841e-02, -1.3402e-04, -7.0567e-03,  3.0288e-02],\n",
      "          [-2.1337e-02, -1.1768e-02,  1.2127e-02, -3.1116e-02, -4.0152e-03],\n",
      "          [ 2.9474e-02,  1.2945e-02,  3.1943e-02,  1.9314e-02, -3.4510e-03],\n",
      "          [ 4.7502e-03, -2.7253e-02,  4.9221e-03,  1.8963e-02, -1.0191e-02],\n",
      "          [-3.1305e-02,  3.3076e-02, -1.7243e-02, -1.8134e-02,  1.2656e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 9.7386e-03,  3.1115e-02,  1.4883e-03, -2.2214e-02,  2.6598e-02],\n",
      "          [-2.5231e-03,  4.5168e-03,  1.2560e-02, -9.5379e-04,  2.5871e-02],\n",
      "          [ 1.6114e-02,  2.3466e-02,  2.6132e-02,  1.6679e-02,  9.7405e-03],\n",
      "          [ 5.9619e-03,  2.5204e-02,  1.4597e-03, -7.0344e-03, -1.2587e-02],\n",
      "          [ 1.4554e-02, -5.9191e-03,  2.8579e-03,  3.9118e-03, -2.7873e-02]],\n",
      "\n",
      "         [[-2.1218e-02,  5.5684e-03, -2.5642e-02, -2.0813e-02,  5.2744e-03],\n",
      "          [ 1.6015e-02, -2.0167e-02,  2.7381e-02,  5.7627e-03, -1.5412e-02],\n",
      "          [ 6.0552e-03,  3.3538e-02, -2.4540e-02,  3.6778e-02,  3.0433e-03],\n",
      "          [-2.8597e-03, -1.1054e-02, -1.2775e-02,  8.8965e-03,  6.9100e-04],\n",
      "          [ 5.3940e-03,  1.8982e-02, -2.8642e-02,  3.1442e-02,  1.9357e-02]],\n",
      "\n",
      "         [[ 1.0777e-02, -2.9332e-02, -7.3607e-03, -1.9722e-02,  1.1593e-02],\n",
      "          [ 3.4165e-02,  1.4648e-02, -5.1428e-03, -9.7038e-05,  2.2668e-02],\n",
      "          [ 3.0157e-02, -2.9571e-02,  3.0819e-02, -3.1313e-02,  1.8136e-02],\n",
      "          [-1.0003e-02, -2.7143e-02,  7.3819e-06, -9.1230e-03,  2.6130e-02],\n",
      "          [-2.1632e-02, -5.7312e-03, -1.2095e-02, -2.4602e-02,  5.8964e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.8896e-02,  1.8305e-02, -1.1762e-03, -7.6604e-03, -2.4187e-02],\n",
      "          [ 2.5074e-02, -2.6885e-02, -2.8741e-02,  2.7975e-02, -7.5754e-03],\n",
      "          [ 4.3817e-03, -3.5380e-03,  7.2200e-03, -9.8682e-04,  2.1347e-02],\n",
      "          [ 3.2435e-02,  7.0914e-03,  1.8297e-02, -1.9033e-02, -2.4947e-02],\n",
      "          [ 8.5207e-03, -2.6580e-02,  1.7665e-02, -1.3616e-02, -2.9976e-02]],\n",
      "\n",
      "         [[ 3.1580e-02, -9.2679e-03,  7.9612e-03,  2.5430e-02,  3.5598e-02],\n",
      "          [-2.2315e-02,  5.3176e-03,  2.7889e-02,  2.7077e-02,  7.4150e-04],\n",
      "          [ 1.7412e-02,  2.4214e-02, -1.2957e-02, -2.5158e-02,  1.4821e-03],\n",
      "          [-1.7061e-02,  1.1298e-02, -2.3807e-02, -2.6163e-02,  1.4597e-02],\n",
      "          [-1.5649e-02,  1.1737e-02,  1.3301e-02, -8.5664e-03, -1.6492e-02]],\n",
      "\n",
      "         [[ 7.1651e-03, -1.2860e-02,  4.2338e-03,  1.4608e-02,  8.8372e-03],\n",
      "          [-3.0759e-02,  3.2527e-02, -1.3637e-02, -2.9552e-02, -1.9300e-02],\n",
      "          [-2.6451e-02, -1.1982e-02,  9.6229e-03,  2.2007e-02,  2.8948e-02],\n",
      "          [-1.3134e-02, -1.7381e-02, -1.2446e-02, -6.5080e-03,  2.3202e-02],\n",
      "          [-4.0004e-03, -2.1153e-02, -2.9906e-02,  1.9116e-03,  1.2525e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.5912e-02,  2.5644e-02,  1.6845e-02,  3.2361e-02, -8.0392e-04],\n",
      "          [-2.0404e-02,  3.5999e-02, -5.0943e-04, -2.1996e-02, -1.8924e-03],\n",
      "          [-5.0052e-03, -2.8744e-02,  6.6564e-03, -1.7576e-02, -1.1915e-02],\n",
      "          [-3.0298e-03, -2.0843e-02, -2.1406e-02,  1.8311e-02, -2.5649e-02],\n",
      "          [-1.9538e-02,  1.6992e-02, -1.3277e-02, -4.4005e-03,  2.8155e-02]],\n",
      "\n",
      "         [[ 2.4967e-02, -7.4592e-03,  1.7214e-02, -9.4680e-03, -3.3234e-02],\n",
      "          [-6.2667e-03, -2.2482e-02,  1.0467e-03,  4.0862e-03,  8.8141e-03],\n",
      "          [-2.9593e-02,  1.8445e-02, -2.3995e-02,  2.7193e-02, -2.3845e-02],\n",
      "          [ 1.6483e-03,  2.2617e-02,  2.2852e-02,  4.9465e-03,  2.5905e-02],\n",
      "          [ 1.1215e-02,  3.2864e-02, -1.7415e-02, -2.9119e-02, -2.4314e-02]],\n",
      "\n",
      "         [[-4.8192e-03, -1.0755e-02, -1.8126e-02,  1.2408e-02, -2.6602e-02],\n",
      "          [ 3.6243e-02, -1.7791e-02,  1.5483e-02,  1.4506e-02, -3.1753e-02],\n",
      "          [-2.8108e-02,  5.5812e-03,  2.5248e-03,  7.7505e-03, -7.7679e-03],\n",
      "          [-3.4085e-02, -2.2088e-02, -6.0926e-03, -3.1214e-02, -1.6638e-02],\n",
      "          [-3.0399e-02,  3.5309e-02,  1.1770e-02, -1.6084e-02, -2.2689e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.3697e-02, -1.5514e-02, -2.6044e-02, -1.8893e-02, -1.7718e-02],\n",
      "          [ 7.9850e-04, -1.7364e-02,  1.0010e-02, -2.9764e-02,  1.5068e-03],\n",
      "          [ 1.7697e-02,  4.1919e-03, -5.7509e-03,  8.0726e-03,  2.1880e-02],\n",
      "          [-7.9957e-03,  3.3958e-02, -4.6956e-03, -5.7286e-03,  2.3461e-02],\n",
      "          [-3.1616e-02, -6.1991e-04, -2.1386e-02, -7.0456e-03,  2.0383e-03]],\n",
      "\n",
      "         [[ 7.6357e-03, -3.1610e-02,  1.4652e-02, -2.8345e-02, -3.4317e-02],\n",
      "          [ 2.5474e-02,  2.4856e-02, -3.3664e-02, -1.1766e-02,  1.7557e-02],\n",
      "          [-2.3341e-02,  1.1364e-02, -3.2827e-02, -1.1180e-02,  4.5724e-03],\n",
      "          [-2.1247e-02,  3.5715e-02,  3.3791e-02, -6.9703e-03, -4.6835e-03],\n",
      "          [ 2.5025e-03, -8.0132e-03, -3.0140e-02,  3.2396e-02,  2.8228e-02]],\n",
      "\n",
      "         [[-3.1015e-02,  8.6954e-04,  3.0233e-02,  1.2910e-02,  3.0693e-02],\n",
      "          [ 2.5673e-02,  2.1395e-02, -3.2082e-02, -3.0025e-02,  2.2918e-02],\n",
      "          [ 2.9168e-02,  2.6770e-03, -3.1049e-02,  2.8896e-03,  3.1129e-02],\n",
      "          [ 1.4136e-02,  3.1871e-02,  1.7151e-02,  2.7551e-02,  4.3400e-03],\n",
      "          [-2.5131e-02,  3.3901e-02,  1.8218e-02,  1.5048e-02,  1.0890e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 1.4716e-02, -2.2143e-02, -3.6502e-03,  3.0318e-02,  2.6728e-02],\n",
      "          [-1.1427e-02,  3.9544e-04, -1.4263e-02,  3.0105e-02, -2.7696e-02],\n",
      "          [ 3.0435e-03,  1.3124e-02, -9.2074e-03,  3.5127e-02, -2.5197e-02],\n",
      "          [ 1.9656e-02,  2.5296e-02, -1.6595e-02, -2.2175e-02, -1.7066e-02],\n",
      "          [-1.7627e-02, -1.2471e-02,  9.9230e-03,  1.6380e-02, -2.9572e-02]],\n",
      "\n",
      "         [[ 1.0276e-03,  1.9763e-02, -3.2049e-02, -8.8715e-03, -9.8684e-04],\n",
      "          [-1.7739e-02, -3.1129e-02, -1.9319e-02,  2.8255e-02,  6.2310e-03],\n",
      "          [ 1.6226e-02,  3.0233e-03,  3.2545e-02,  9.7953e-03, -3.2409e-02],\n",
      "          [-1.7988e-02,  5.8788e-03, -1.3417e-02,  3.3443e-02,  1.9630e-02],\n",
      "          [ 2.8107e-02,  1.7346e-02,  3.2670e-02,  6.7683e-03, -2.6963e-02]],\n",
      "\n",
      "         [[ 1.5295e-02, -2.8931e-02, -3.6818e-04,  9.0291e-03, -4.9684e-03],\n",
      "          [-1.0127e-03, -2.1928e-02,  1.5418e-02,  1.8065e-02,  3.1323e-02],\n",
      "          [ 1.3859e-02, -2.9815e-02,  3.3795e-02,  2.5123e-02, -3.3395e-02],\n",
      "          [-2.1410e-02, -2.9726e-02,  8.1874e-03, -2.4161e-02, -7.1336e-03],\n",
      "          [-7.4539e-03,  1.1745e-02,  2.0077e-02,  1.8796e-02, -3.3333e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.7897e-02,  9.2699e-03, -5.8838e-03,  1.4298e-02, -3.3184e-02],\n",
      "          [ 3.2171e-02,  1.5883e-02, -1.4480e-02,  7.3942e-03,  2.3492e-02],\n",
      "          [-3.2962e-02,  8.5822e-03, -2.2200e-02, -3.0359e-02,  3.5307e-02],\n",
      "          [ 3.0021e-02,  2.0275e-02,  2.5176e-02,  3.2214e-02, -2.4345e-02],\n",
      "          [ 2.2023e-02,  6.1325e-03, -1.8183e-02, -1.0806e-02, -2.6265e-02]],\n",
      "\n",
      "         [[-8.9088e-03, -1.1938e-02, -2.3920e-02, -2.0218e-02,  3.3494e-03],\n",
      "          [ 2.1808e-02,  2.9903e-03,  1.2929e-02, -7.1921e-03, -8.6155e-04],\n",
      "          [ 7.3093e-03,  3.3507e-02, -2.0559e-02, -7.6818e-03,  2.1437e-02],\n",
      "          [-2.7079e-02,  1.2166e-02, -2.6912e-02,  3.3846e-02,  3.2919e-02],\n",
      "          [-2.0459e-02, -3.2542e-02, -1.9270e-02,  2.5683e-02,  3.2329e-02]],\n",
      "\n",
      "         [[ 2.2814e-02,  2.4612e-02,  4.3857e-03, -6.8587e-03,  9.8731e-03],\n",
      "          [-2.8499e-03,  3.1085e-02,  3.2933e-02, -1.7173e-02,  1.0082e-02],\n",
      "          [-7.8298e-03,  8.1880e-03,  2.7524e-02, -1.2697e-03, -1.2738e-02],\n",
      "          [ 1.7330e-02, -3.1127e-02, -3.3328e-02,  2.9830e-02,  2.8117e-02],\n",
      "          [ 1.0975e-02,  4.3098e-03, -9.5597e-03, -3.1703e-02,  2.6570e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.0247e-02,  2.4433e-02, -3.1254e-02, -2.5882e-04, -2.0991e-02],\n",
      "          [ 2.7699e-02,  3.0406e-02,  6.3280e-03, -1.0705e-02, -8.0694e-03],\n",
      "          [ 1.0974e-02,  2.8271e-02,  3.4289e-02,  2.9054e-02, -1.5746e-02],\n",
      "          [ 7.6998e-03, -6.9666e-03,  3.3261e-02,  3.6444e-02,  1.6827e-02],\n",
      "          [-2.5832e-02, -2.5433e-02,  2.7657e-02, -1.8335e-02,  3.6226e-02]],\n",
      "\n",
      "         [[-2.2847e-02,  2.4851e-03, -1.6967e-03, -2.2638e-02,  1.9089e-02],\n",
      "          [-4.9100e-03,  9.5264e-04, -7.5413e-03,  1.9800e-02,  1.9482e-02],\n",
      "          [-3.2956e-03, -1.0052e-03, -1.7342e-02,  2.8279e-02, -2.5974e-02],\n",
      "          [-2.3040e-02,  2.3144e-02, -1.5051e-02, -6.7854e-03, -3.0288e-02],\n",
      "          [-3.0375e-02, -4.7977e-03,  8.2173e-03, -2.8436e-02,  1.5528e-02]],\n",
      "\n",
      "         [[-2.8951e-02, -2.2104e-02, -1.2484e-02,  1.3150e-03, -2.6106e-02],\n",
      "          [-3.8694e-03, -7.3457e-03, -3.3010e-02, -2.2188e-02, -1.9457e-03],\n",
      "          [ 1.2922e-02, -2.9930e-02, -2.4929e-02,  5.0370e-03, -3.4934e-03],\n",
      "          [ 3.1209e-02,  3.3459e-02, -2.5886e-03,  2.4277e-02,  1.1975e-02],\n",
      "          [-1.7441e-02,  8.5301e-03,  3.2265e-02, -2.6756e-04,  7.6204e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.6734e-02,  2.7943e-03, -2.1488e-02,  7.5067e-03, -2.8257e-02],\n",
      "          [-2.0154e-03,  3.1972e-02,  1.7628e-02, -1.1881e-02,  3.0670e-02],\n",
      "          [-1.0915e-02, -3.4270e-02,  4.1807e-03,  4.0919e-03,  1.7673e-02],\n",
      "          [ 2.2012e-03,  3.5177e-03, -1.2619e-02,  1.8139e-02, -7.5688e-04],\n",
      "          [ 1.0546e-02, -2.1666e-02,  1.0210e-02, -3.4645e-02, -2.4606e-02]],\n",
      "\n",
      "         [[-7.9341e-03,  1.6419e-02,  3.2406e-02, -5.2596e-03, -1.9960e-02],\n",
      "          [ 1.9685e-02,  3.5113e-05, -4.7014e-03,  3.2646e-02,  1.5750e-02],\n",
      "          [ 2.6760e-02, -5.1776e-03, -1.2283e-02, -1.6588e-02,  2.4779e-02],\n",
      "          [ 2.3985e-02, -2.8311e-02,  3.3890e-02,  8.9372e-03, -3.0605e-02],\n",
      "          [-2.5061e-02, -3.2054e-02, -2.2602e-02,  1.7085e-02,  6.3267e-03]],\n",
      "\n",
      "         [[-9.5434e-03, -6.2564e-03, -6.5762e-03,  1.3195e-02, -2.3713e-02],\n",
      "          [ 9.7663e-03, -6.1634e-03,  1.7146e-02,  1.6164e-02,  2.7665e-02],\n",
      "          [-2.7604e-02,  3.1297e-02,  8.6385e-03, -1.4510e-02,  1.9065e-02],\n",
      "          [-2.5287e-02,  3.4426e-02, -1.3923e-02, -8.6089e-03, -3.4147e-02],\n",
      "          [ 1.7924e-02,  1.7088e-02,  5.1606e-03, -7.7061e-03, -2.7030e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.4038e-02,  2.1872e-02,  1.4401e-02,  3.4174e-02,  1.7075e-02],\n",
      "          [-6.6800e-03,  2.3080e-02,  2.2958e-02,  1.8068e-02,  3.1013e-02],\n",
      "          [ 3.7927e-03,  3.1411e-03,  1.3602e-02,  2.2425e-02,  2.3308e-02],\n",
      "          [-2.4274e-02,  1.0510e-02, -3.9224e-03,  2.2499e-02, -2.0603e-02],\n",
      "          [ 1.0033e-02, -1.3719e-02,  4.7322e-03, -6.3079e-03,  1.7086e-02]],\n",
      "\n",
      "         [[ 1.9785e-02,  3.3728e-02,  1.4909e-03,  1.9525e-02,  4.8421e-03],\n",
      "          [-1.2396e-02,  1.2213e-02,  1.5297e-02,  3.2702e-02,  2.0292e-02],\n",
      "          [-6.4176e-04, -3.1666e-02,  6.1417e-03,  2.7249e-02, -1.9256e-02],\n",
      "          [-1.0917e-02, -1.4140e-03,  1.8151e-02, -5.9190e-03,  8.6410e-03],\n",
      "          [-1.0431e-02, -2.1148e-02,  1.7659e-02, -1.4099e-02,  1.5785e-02]],\n",
      "\n",
      "         [[ 1.7286e-02, -2.1858e-02,  1.3856e-03,  2.5440e-02,  3.0624e-02],\n",
      "          [ 2.7891e-02, -9.2770e-03, -2.4855e-02,  3.3810e-02,  1.1739e-02],\n",
      "          [-2.1978e-02,  1.8993e-03, -3.3958e-02,  1.3043e-03,  2.8383e-02],\n",
      "          [-3.3752e-02, -1.6549e-02, -1.8692e-02,  5.8130e-03,  3.2414e-02],\n",
      "          [-4.9668e-03, -3.2946e-02,  2.2190e-02, -1.8773e-02, -5.7675e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.7133e-03, -2.5594e-02,  3.2101e-02,  3.0481e-02,  2.4964e-02],\n",
      "          [-3.3496e-02, -1.6708e-02,  9.7357e-03,  2.6209e-02,  1.0731e-02],\n",
      "          [ 2.8822e-02, -3.8109e-03,  1.0688e-02,  1.6347e-02, -1.2344e-02],\n",
      "          [-5.7336e-03, -3.1138e-02, -3.4224e-02,  2.2714e-03,  4.3142e-03],\n",
      "          [ 2.5517e-02,  5.8749e-03, -2.4409e-02,  2.1977e-02,  1.2157e-02]],\n",
      "\n",
      "         [[ 2.0194e-02,  3.3823e-02,  3.4043e-02, -5.2277e-03, -2.7872e-02],\n",
      "          [ 3.4212e-02, -1.2996e-02,  2.3665e-02, -5.4606e-03, -5.6746e-03],\n",
      "          [ 9.8889e-03, -1.7201e-05,  3.3181e-02, -5.6078e-03,  1.2751e-02],\n",
      "          [ 1.2507e-02,  1.9545e-02, -1.8658e-02,  1.1137e-02,  2.0438e-02],\n",
      "          [ 1.7684e-02, -2.4177e-02,  4.1943e-03,  3.2646e-02, -6.7360e-03]],\n",
      "\n",
      "         [[ 2.5854e-02, -1.9834e-03,  5.1506e-03,  1.0732e-03,  2.1432e-04],\n",
      "          [-1.4423e-02, -1.9094e-02,  2.0224e-02,  3.2925e-03, -7.1479e-03],\n",
      "          [-2.2512e-02, -1.0585e-03,  3.1557e-02,  3.2594e-02, -3.1875e-02],\n",
      "          [-2.4404e-02, -3.0969e-02,  3.0404e-02, -5.5568e-03, -3.3785e-02],\n",
      "          [ 1.5672e-02, -3.7798e-03, -3.1876e-02, -2.8159e-02,  2.7748e-02]]]])}, Parameter containing:\n",
      "tensor([-0.0086, -0.0050,  0.0671,  0.0075, -0.0331, -0.0085, -0.0060,  0.0211,\n",
      "         0.0524, -0.0165,  0.0098,  0.0172,  0.0577,  0.0368, -0.0037,  0.0301,\n",
      "        -0.0512, -0.0067,  0.0054,  0.0366,  0.0627, -0.0030,  0.0747,  0.0203,\n",
      "        -0.0367, -0.0036,  0.0029, -0.0175, -0.0325,  0.0059, -0.0207,  0.0118,\n",
      "         0.0087, -0.0414,  0.0089, -0.0358, -0.0226, -0.0476, -0.0461,  0.0544,\n",
      "         0.0262,  0.0791, -0.0087,  0.1035, -0.0034,  0.0345,  0.0177,  0.0893,\n",
      "         0.0303, -0.0075, -0.0356, -0.0125, -0.0052, -0.0408,  0.0493,  0.0371,\n",
      "         0.0066, -0.0313,  0.0316, -0.0299, -0.0064,  0.0071,  0.0341,  0.0145],\n",
      "       requires_grad=True): {'old_init': tensor([-1.7455e-02,  1.7160e-02, -1.7602e-02, -1.8584e-02, -1.0697e-02,\n",
      "         3.6759e-02,  1.6384e-02,  1.1206e-02,  2.0863e-02,  2.8052e-03,\n",
      "         2.0454e-02,  1.9863e-02,  2.5893e-02,  2.7057e-02, -1.6745e-02,\n",
      "         3.6302e-02, -2.3533e-02, -4.7361e-03, -2.8667e-02, -5.4040e-03,\n",
      "         2.8941e-02, -1.2287e-03, -1.3834e-02,  2.1900e-02, -2.2312e-02,\n",
      "        -1.2252e-02,  2.1842e-02, -3.1324e-02, -1.1247e-02,  1.7694e-02,\n",
      "         3.6313e-03,  1.5252e-02,  8.0387e-03, -3.4393e-02,  3.5438e-02,\n",
      "         4.7281e-03, -3.3815e-02, -2.4289e-02, -3.3264e-02,  2.3605e-02,\n",
      "         3.2010e-02, -1.1581e-03, -1.7528e-03,  3.5894e-02,  5.4654e-03,\n",
      "        -6.5955e-03,  3.6671e-02,  3.5254e-02,  3.1197e-05, -1.7742e-02,\n",
      "        -1.5700e-02,  9.3243e-03, -2.9625e-02, -3.2409e-02,  2.9405e-02,\n",
      "         1.4401e-02,  1.1839e-04, -1.7342e-02,  3.6220e-02,  1.7631e-03,\n",
      "        -9.6436e-03,  1.7688e-02,  1.3177e-02, -2.6239e-02])}, Parameter containing:\n",
      "tensor([[ 0.0142, -0.0080,  0.0280,  ...,  0.0124, -0.0010,  0.0051],\n",
      "        [ 0.0230,  0.0118,  0.0018,  ..., -0.0264,  0.0098, -0.0267],\n",
      "        [ 0.0042, -0.0228, -0.0268,  ...,  0.0050,  0.0061, -0.0181],\n",
      "        ...,\n",
      "        [ 0.0271,  0.0123,  0.0531,  ...,  0.0073,  0.0659,  0.0254],\n",
      "        [ 0.0038,  0.0177, -0.0115,  ..., -0.0097, -0.0264, -0.0060],\n",
      "        [ 0.0036, -0.0089,  0.0155,  ...,  0.0073,  0.0039,  0.0016]],\n",
      "       requires_grad=True): {'old_init': tensor([[ 0.0142, -0.0080,  0.0280,  ...,  0.0124, -0.0010,  0.0051],\n",
      "        [ 0.0230,  0.0118,  0.0018,  ..., -0.0264,  0.0098, -0.0267],\n",
      "        [ 0.0042, -0.0228, -0.0268,  ...,  0.0050,  0.0061, -0.0181],\n",
      "        ...,\n",
      "        [ 0.0123, -0.0099,  0.0153,  ..., -0.0284,  0.0308,  0.0064],\n",
      "        [ 0.0038,  0.0177, -0.0115,  ..., -0.0097, -0.0264, -0.0060],\n",
      "        [-0.0095, -0.0286, -0.0180,  ..., -0.0245, -0.0273, -0.0153]])}, Parameter containing:\n",
      "tensor([-1.5014e-02,  2.7167e-02, -1.8762e-02,  5.6500e-03,  3.5492e-02,\n",
      "        -2.5427e-02, -1.9837e-02,  5.9704e-04,  1.5771e-02, -1.0484e-02,\n",
      "        -2.7753e-02,  2.8411e-02, -1.8881e-02, -1.2449e-02, -2.8969e-02,\n",
      "         2.2648e-02, -3.2879e-02, -3.8309e-02,  3.0655e-02,  3.2975e-02,\n",
      "         2.0601e-02,  2.6068e-02, -1.4796e-02,  2.8815e-02, -1.4164e-02,\n",
      "        -1.2534e-02,  2.5051e-02, -9.8884e-03, -5.5921e-02,  1.2249e-02,\n",
      "        -4.5575e-03, -1.1136e-02, -2.0198e-03, -2.7706e-02,  2.2427e-03,\n",
      "        -2.8352e-02, -2.2383e-02, -2.2084e-02,  2.0964e-02, -2.7535e-02,\n",
      "         8.2531e-04,  2.9925e-02, -5.4893e-03, -7.8679e-03, -1.9892e-02,\n",
      "         4.5041e-02, -5.9190e-02, -2.3236e-02,  1.2650e-02, -3.0488e-02,\n",
      "         1.6560e-02,  7.3937e-03,  3.0317e-02,  5.1569e-02,  9.8904e-03,\n",
      "         4.2360e-03, -4.3738e-03,  5.4619e-03, -1.2048e-02, -1.2202e-02,\n",
      "         1.9142e-02,  3.1533e-02, -4.4089e-03, -1.2862e-03,  1.9433e-02,\n",
      "        -3.2537e-02,  2.4227e-02, -4.2608e-02, -5.6595e-03, -1.0659e-02,\n",
      "         3.6608e-02,  3.9588e-02,  1.1275e-02,  4.2755e-04, -3.1279e-02,\n",
      "        -2.0162e-02,  9.6216e-03,  3.3339e-03,  2.8276e-02,  3.2140e-02,\n",
      "        -2.7536e-02,  2.7046e-03,  8.9967e-03, -2.4494e-02,  5.1284e-02,\n",
      "         4.3938e-02, -2.6627e-02,  2.4252e-03,  3.7267e-02,  3.5251e-03,\n",
      "         3.4622e-03, -4.1719e-02,  9.6295e-03, -2.0162e-02,  1.6560e-02,\n",
      "         3.4536e-03,  2.0336e-02,  8.0655e-03, -1.3707e-02,  1.9437e-02,\n",
      "        -3.1173e-02, -2.9105e-03,  9.8901e-03, -1.4592e-03, -2.1270e-02,\n",
      "        -2.5483e-02,  4.7283e-02,  7.9949e-03, -4.4258e-02,  2.2646e-02,\n",
      "        -1.5364e-05, -2.9589e-02,  2.5826e-02,  4.6207e-03,  7.5396e-03,\n",
      "        -1.9409e-02,  1.0388e-02,  2.3470e-02,  8.2717e-03, -2.4235e-02,\n",
      "         4.1296e-04, -2.4931e-02, -2.4303e-02,  3.4489e-03,  9.2502e-03,\n",
      "         2.9593e-02, -3.3982e-02,  5.4559e-02, -1.0745e-02, -6.2671e-03,\n",
      "        -3.3826e-03,  7.0952e-04, -1.2624e-02, -2.1085e-02, -1.8246e-02,\n",
      "        -2.1612e-03, -4.6226e-02,  3.8946e-03,  2.0734e-02, -2.6878e-02,\n",
      "         5.8546e-02, -1.6473e-02,  2.7820e-02, -1.4705e-02, -2.3284e-02,\n",
      "         3.2901e-02,  5.6613e-03, -1.7120e-03,  2.7828e-02,  5.6509e-03,\n",
      "         1.0548e-03, -2.5234e-02,  3.0478e-03,  2.3420e-02,  4.9480e-03,\n",
      "         9.3008e-03,  2.9692e-02, -7.8338e-03, -3.0175e-02,  2.4516e-02,\n",
      "        -4.5736e-03,  1.0236e-03,  2.3455e-02,  2.3512e-02, -5.8713e-02,\n",
      "        -5.8948e-05, -8.0037e-03, -2.9973e-02, -4.0582e-02, -2.1649e-02,\n",
      "        -5.2034e-03,  1.6393e-02,  3.8271e-02, -1.0325e-02, -8.9941e-04,\n",
      "         3.7573e-02, -1.4876e-03,  7.4907e-03, -5.6352e-02,  1.2375e-02,\n",
      "        -2.5127e-02,  1.6934e-02,  1.2279e-02,  1.4983e-02, -1.0046e-02,\n",
      "         2.6173e-02, -2.7371e-02, -1.0122e-02,  1.3628e-02, -2.3223e-02,\n",
      "         1.5142e-02,  3.3925e-02,  2.6067e-02,  7.2719e-04,  3.5799e-02,\n",
      "         1.8271e-02, -4.2072e-02,  2.4237e-02, -1.4649e-02,  3.0413e-02,\n",
      "        -1.1212e-02, -1.3550e-02,  5.3686e-03, -3.8544e-03, -2.7703e-02,\n",
      "         1.3730e-03,  2.7088e-02,  3.3572e-02, -4.7206e-03,  3.4348e-02,\n",
      "        -1.6543e-02, -3.5779e-02,  4.2226e-03, -1.3541e-02,  1.2995e-02,\n",
      "        -2.5492e-03, -1.1319e-02,  6.8318e-03,  3.0808e-02, -7.1886e-03,\n",
      "        -8.1466e-03,  2.3612e-02,  8.5978e-03, -1.4416e-02,  3.0363e-02,\n",
      "         3.6567e-02,  3.6466e-03, -4.1062e-02, -9.7187e-03,  7.9166e-03,\n",
      "        -5.4961e-03,  2.5511e-02,  3.1521e-03,  1.0108e-02, -2.1276e-02,\n",
      "         1.2740e-02, -3.4038e-02,  3.1485e-02, -7.0251e-03, -1.2462e-02,\n",
      "         1.7823e-02,  3.1059e-03,  3.0977e-02, -1.1046e-02,  1.0528e-02,\n",
      "         1.7728e-02, -2.0551e-02, -8.9893e-03,  2.5751e-02,  9.8601e-03,\n",
      "         2.6759e-02, -3.3643e-02,  3.3517e-03, -1.8854e-03,  1.1989e-02,\n",
      "         8.9923e-03,  2.9821e-02,  1.2768e-02,  6.6306e-03,  9.0862e-03,\n",
      "         1.7279e-02, -1.4506e-03,  4.5298e-03, -3.6835e-02,  1.7464e-02,\n",
      "        -3.5694e-02,  3.0902e-03,  1.6251e-02, -2.4803e-02, -1.3117e-02,\n",
      "         5.0269e-02,  1.4800e-02, -1.3869e-02,  8.2642e-03, -1.1383e-02,\n",
      "        -7.1375e-03,  3.0514e-02,  3.0014e-02, -1.2239e-02,  2.0372e-02,\n",
      "        -2.6510e-03,  4.6620e-03,  1.9598e-02,  3.7020e-02,  4.1387e-03,\n",
      "         2.7625e-02, -4.1916e-02, -3.2156e-04, -2.3301e-02, -2.8727e-02,\n",
      "         5.3031e-02,  1.5077e-02,  3.0326e-02, -1.1303e-02,  9.6344e-03,\n",
      "         9.4463e-03, -1.0202e-02, -2.1429e-02,  1.7104e-02, -1.1573e-02,\n",
      "        -1.7551e-02, -4.5423e-03,  1.2794e-02,  1.3429e-02, -1.6031e-02,\n",
      "         3.0786e-02,  1.0059e-02,  2.9071e-02, -5.2034e-03,  1.8610e-02,\n",
      "         2.8382e-02, -2.2175e-02, -1.2631e-02,  2.2675e-03, -1.8024e-02,\n",
      "         3.2747e-02, -7.7838e-03, -3.6409e-02, -1.3737e-02,  1.7313e-02,\n",
      "        -2.2260e-02,  4.0410e-03, -2.9277e-02,  2.9143e-02,  9.6328e-03,\n",
      "         3.0377e-02,  1.0184e-02,  3.1339e-02,  2.8429e-02, -1.9923e-02,\n",
      "        -2.6908e-02,  3.2130e-02, -3.7146e-02, -5.5803e-03,  2.8247e-02,\n",
      "         5.8501e-02, -9.4723e-03,  4.7224e-02,  6.2875e-02, -1.1709e-02,\n",
      "         2.6759e-02, -1.6174e-02,  1.5638e-02,  3.0513e-02, -9.6240e-03,\n",
      "        -1.2581e-02, -1.4626e-02, -2.0443e-02, -1.5347e-02, -2.5652e-02,\n",
      "        -4.7095e-02,  4.0725e-02, -1.9763e-02,  4.2171e-02,  1.2461e-02,\n",
      "        -2.1856e-02,  4.8239e-02, -1.5329e-02, -1.2033e-02, -4.4894e-03,\n",
      "         9.8144e-03,  2.7972e-02,  4.3753e-03,  1.4404e-03, -2.8469e-02,\n",
      "         2.6880e-03,  4.4949e-02,  2.6290e-02, -5.7661e-03,  1.5920e-02,\n",
      "         3.7575e-02,  2.7437e-02, -1.1881e-02,  8.0718e-03,  2.3094e-02,\n",
      "        -3.1469e-03,  5.0671e-03,  1.0800e-03,  8.4571e-03, -2.5469e-02,\n",
      "         3.0509e-02,  1.4297e-02, -1.2508e-03, -1.2880e-02, -2.3735e-02,\n",
      "        -2.7488e-02,  2.7652e-03, -3.1078e-02,  5.0980e-02,  3.1905e-03,\n",
      "         2.6874e-02, -3.9199e-03, -7.2583e-04,  4.6227e-03, -1.5888e-02,\n",
      "         5.7330e-03,  9.0155e-03,  1.1233e-02,  5.0582e-02, -2.2725e-02,\n",
      "        -1.9572e-02, -1.8308e-02, -2.0142e-02, -4.5095e-02, -1.3661e-02,\n",
      "        -3.8651e-02, -8.1463e-03, -1.7586e-02, -2.6175e-02, -7.6208e-03,\n",
      "        -1.3342e-02, -1.8077e-03, -1.5290e-02, -3.2309e-04,  5.3769e-03,\n",
      "        -2.7386e-02, -1.8501e-02,  2.9320e-02,  2.9647e-02,  1.6486e-02,\n",
      "        -2.2416e-02, -1.0107e-02, -2.7156e-02, -8.3679e-03, -3.6506e-02,\n",
      "        -9.1898e-03, -5.5672e-03,  2.7053e-02, -1.2564e-04,  3.0508e-02,\n",
      "         2.9714e-02, -4.8171e-03,  1.3303e-02,  3.5581e-02,  1.5961e-02,\n",
      "         6.3135e-02, -6.0026e-04,  4.2822e-03, -2.0111e-02,  1.1368e-02,\n",
      "        -9.1601e-03, -2.1328e-02, -1.8180e-02, -3.9077e-02,  1.5738e-02,\n",
      "         9.7244e-03, -1.9936e-02,  3.5810e-02,  1.0389e-02,  1.1538e-02,\n",
      "         1.0562e-02, -3.8671e-03, -4.5352e-02, -2.3273e-02,  2.4189e-02,\n",
      "         7.8327e-03, -4.1984e-02,  1.3262e-02,  1.5655e-02,  1.3456e-02,\n",
      "         2.1307e-02, -2.1197e-02,  2.8025e-02,  8.9569e-03, -4.7258e-03,\n",
      "        -1.8840e-02,  6.4759e-02, -1.1695e-02,  2.5326e-02, -4.1348e-02,\n",
      "        -3.1514e-03,  4.5137e-03, -3.0435e-03,  5.3983e-03,  1.9520e-02,\n",
      "         2.6670e-02,  7.4768e-03, -6.7055e-03, -2.7095e-02, -9.9242e-03,\n",
      "        -1.0577e-03,  9.9071e-03, -3.0285e-02,  1.4458e-02, -2.7577e-02,\n",
      "        -2.2822e-02,  3.1006e-02, -2.9068e-02, -3.1995e-02,  3.8711e-02,\n",
      "        -5.8949e-05, -2.8406e-02, -5.9847e-02,  8.9855e-03, -7.2919e-03,\n",
      "         5.3985e-04, -6.0643e-02, -1.6122e-02, -2.1111e-02,  1.1811e-02,\n",
      "         3.1476e-04,  2.7534e-02,  2.0474e-02,  2.3548e-02, -4.3580e-02,\n",
      "        -3.1332e-03, -2.4816e-02,  2.5337e-02, -7.9936e-03,  2.6133e-02,\n",
      "        -1.1696e-02,  1.1851e-02], requires_grad=True): {'old_init': tensor([-1.5014e-02,  2.7167e-02, -1.8762e-02,  5.6500e-03,  1.4499e-02,\n",
      "        -2.5427e-02, -1.9837e-02,  2.8037e-02,  1.5771e-02,  2.5556e-02,\n",
      "        -2.7753e-02,  1.0693e-02, -1.4378e-02, -1.2449e-02, -2.8969e-02,\n",
      "         2.2648e-02, -1.0110e-02,  1.5648e-03,  1.8669e-02,  1.5947e-02,\n",
      "         2.0601e-02,  2.2351e-02, -2.5566e-02,  2.8815e-02,  2.8233e-03,\n",
      "         9.4683e-03, -5.7406e-03, -1.7145e-02, -3.0800e-02,  7.5810e-03,\n",
      "         1.8427e-02, -9.9543e-03, -1.9393e-02, -4.4331e-03, -5.2725e-04,\n",
      "         6.0428e-03, -2.2383e-02, -2.2084e-02,  2.0964e-02, -2.7535e-02,\n",
      "         2.1296e-03,  2.9925e-02, -4.4046e-03, -7.8679e-03,  4.0044e-03,\n",
      "         2.0593e-02, -2.8865e-02, -1.8988e-03, -4.8475e-03, -5.1270e-03,\n",
      "        -1.3273e-02, -2.1193e-02,  3.0317e-02,  1.8631e-02,  9.8904e-03,\n",
      "         8.6854e-03,  2.7088e-03,  7.1278e-03,  6.6975e-04,  6.4567e-03,\n",
      "         1.9010e-03,  3.1533e-02, -1.2234e-02, -7.4569e-03,  1.2005e-03,\n",
      "        -2.9698e-02,  2.4227e-02, -1.6231e-02, -3.7629e-03,  2.8782e-02,\n",
      "         2.0791e-02,  2.9137e-03, -2.4259e-04,  2.3043e-02, -2.4406e-03,\n",
      "        -2.0162e-02, -1.3594e-02, -2.7089e-02, -1.4817e-02, -2.4663e-03,\n",
      "        -2.7536e-02,  9.8017e-03,  3.0601e-02, -2.4494e-02,  2.8439e-02,\n",
      "         1.3651e-02,  1.6093e-02,  2.4252e-03,  9.7166e-03,  1.7910e-02,\n",
      "         2.7103e-02,  7.0640e-03,  9.6295e-03, -2.0162e-02,  1.6560e-02,\n",
      "        -2.9480e-02,  2.0336e-02,  2.0934e-02,  9.6272e-03,  1.9437e-02,\n",
      "        -3.1173e-02,  1.0894e-02,  1.2025e-02,  2.9650e-02, -8.9165e-03,\n",
      "        -2.5483e-02,  1.9586e-02, -2.5953e-02, -1.3280e-02,  2.2646e-02,\n",
      "        -8.6839e-03, -8.1217e-03, -7.7251e-03,  3.0489e-02,  1.1796e-02,\n",
      "         2.7719e-03,  1.0388e-02,  5.0912e-04,  8.2717e-03, -2.4235e-02,\n",
      "         4.1296e-04, -2.4931e-02, -2.4303e-02, -2.4467e-02,  9.2502e-03,\n",
      "         2.9593e-02, -2.3354e-02,  2.7224e-02, -2.8442e-02,  2.8858e-03,\n",
      "         1.9770e-02,  7.0952e-04, -1.2624e-02, -8.9014e-03,  1.7690e-02,\n",
      "        -2.1612e-03, -9.0146e-03,  3.8946e-03,  2.0734e-02, -2.6878e-02,\n",
      "         1.9740e-02, -1.7267e-02,  2.7820e-02, -3.0578e-02, -2.3284e-02,\n",
      "         1.8327e-02,  5.8799e-03,  1.3788e-02,  2.7828e-02,  5.6509e-03,\n",
      "         2.6653e-02, -2.5234e-02,  2.2746e-02,  2.1466e-02,  1.3518e-02,\n",
      "        -1.1779e-02,  2.9692e-02,  1.7413e-02, -3.0175e-02,  2.4516e-02,\n",
      "        -4.5736e-03, -2.3534e-02,  3.9101e-03, -8.1206e-03, -2.4113e-02,\n",
      "        -5.8948e-05,  8.1280e-03, -2.9973e-02,  2.6981e-03, -2.1649e-02,\n",
      "        -5.2034e-03,  1.6393e-02,  1.7285e-02, -1.0325e-02, -8.9941e-04,\n",
      "         1.6351e-02, -1.4876e-03,  7.4907e-03, -1.6232e-02,  1.2375e-02,\n",
      "         1.3516e-02, -8.4792e-03,  9.9183e-04,  1.3733e-02, -1.0046e-02,\n",
      "         2.6173e-02, -1.7727e-02, -2.5072e-02,  1.3628e-02, -2.3223e-02,\n",
      "         1.5142e-02,  4.6196e-03,  2.6067e-02, -2.8333e-02,  2.6980e-03,\n",
      "         1.8271e-02, -2.9107e-02,  1.7515e-02,  2.6501e-03,  2.4082e-03,\n",
      "         1.3195e-02, -1.3550e-02,  5.3686e-03, -3.8544e-03, -2.7703e-02,\n",
      "        -9.9638e-03,  2.9883e-02,  1.1202e-02, -4.7206e-03,  2.6638e-02,\n",
      "         3.1548e-02, -2.7510e-02,  2.3287e-02, -1.3541e-02,  2.5258e-02,\n",
      "         2.2571e-02, -1.1319e-02,  6.8318e-03, -1.0684e-03, -2.9729e-02,\n",
      "        -8.1466e-03,  2.3612e-02,  5.3561e-03, -1.4416e-02,  1.4951e-02,\n",
      "         4.8485e-03, -1.0835e-02, -3.0201e-02, -1.2508e-02,  7.9166e-03,\n",
      "        -5.4961e-03,  2.5511e-02, -9.8319e-03,  7.3466e-03, -2.1276e-02,\n",
      "        -2.4707e-03, -2.8731e-02,  8.9284e-03, -2.9435e-02, -6.9808e-03,\n",
      "         3.1149e-02,  1.9256e-02,  3.0977e-02, -1.1046e-02, -2.0336e-03,\n",
      "         1.7728e-02, -2.0551e-02, -8.9893e-03, -2.0188e-02,  9.8601e-03,\n",
      "         1.4382e-02, -9.7616e-03,  3.3517e-03,  3.0172e-02,  1.2788e-03,\n",
      "         8.9923e-03,  3.0905e-02, -1.1204e-02,  1.7559e-02,  9.2670e-03,\n",
      "         1.8647e-02, -7.5381e-03,  4.5298e-03, -4.7707e-03,  1.7464e-02,\n",
      "        -5.4314e-03,  3.0902e-03, -1.0862e-02, -6.3151e-03,  3.0907e-02,\n",
      "         2.6289e-02, -7.4894e-03, -1.3869e-02,  8.2642e-03,  9.5242e-03,\n",
      "        -7.1375e-03,  3.0514e-02,  3.0014e-02, -1.2239e-02,  2.0372e-02,\n",
      "        -2.6510e-03,  2.6170e-02,  1.9598e-02,  1.5768e-02,  1.3751e-02,\n",
      "         7.0122e-03, -1.2491e-03, -3.2156e-04,  1.7394e-02, -2.8727e-02,\n",
      "         2.2173e-02,  1.5077e-02,  3.0326e-02, -1.1303e-02,  9.6344e-03,\n",
      "         2.7738e-02, -1.0202e-02, -2.1429e-02, -2.4327e-02, -1.1573e-02,\n",
      "        -1.0157e-02, -3.0016e-02,  9.6249e-03, -1.4120e-02, -1.6249e-02,\n",
      "         2.5027e-03,  9.2210e-03,  2.9071e-02, -5.2034e-03,  1.8610e-02,\n",
      "         4.0471e-03, -2.2175e-02, -1.2631e-02,  2.2675e-03, -1.8024e-02,\n",
      "         1.8861e-02, -7.7838e-03, -2.0410e-02, -1.3737e-02,  1.7313e-02,\n",
      "        -2.2260e-02,  4.0410e-03,  1.2430e-03,  2.9143e-02,  9.6328e-03,\n",
      "         3.0377e-02,  1.0184e-02, -1.2263e-03,  2.8429e-02, -1.5753e-02,\n",
      "        -1.8337e-02,  1.8916e-02, -1.8174e-02, -5.5803e-03, -6.2378e-03,\n",
      "         2.3051e-02, -9.4723e-03,  1.0870e-02,  2.6438e-02, -1.1709e-02,\n",
      "         2.6759e-02, -3.2321e-03, -2.1983e-02, -9.8302e-03, -9.6240e-03,\n",
      "        -1.2581e-02, -1.4626e-02, -2.0443e-02,  2.7917e-02, -2.0975e-02,\n",
      "        -1.8253e-02,  2.5667e-02, -7.4054e-03,  5.8630e-03,  1.2461e-02,\n",
      "        -2.4177e-02,  6.8143e-04, -3.0441e-02, -1.2033e-02, -2.6718e-04,\n",
      "        -5.7481e-03,  4.8646e-03,  4.3753e-03,  1.4404e-03, -1.3721e-02,\n",
      "         6.3026e-03,  2.6411e-02,  3.3430e-03,  2.3780e-02,  1.5920e-02,\n",
      "        -1.8508e-03,  2.7437e-02,  2.3359e-02, -7.8485e-03, -2.0523e-02,\n",
      "         8.3155e-03,  5.0671e-03, -1.7455e-02,  8.4571e-03, -2.5225e-02,\n",
      "         3.0509e-02,  1.8750e-02, -9.1895e-03, -1.2880e-02, -1.9107e-02,\n",
      "        -2.7488e-02,  2.7652e-03, -1.2752e-02,  1.5209e-02,  2.5557e-02,\n",
      "         1.4776e-02,  2.6717e-04, -7.2583e-04,  4.6227e-03, -1.5888e-02,\n",
      "        -1.8479e-02,  9.0155e-03, -1.9291e-02,  1.9815e-02,  3.2629e-03,\n",
      "        -1.9572e-02,  1.9327e-02, -1.7160e-02, -9.5998e-03,  1.3684e-02,\n",
      "        -1.5304e-02, -2.7585e-02, -8.9294e-03, -2.6175e-02, -7.6208e-03,\n",
      "         1.1418e-03, -1.8077e-03, -3.0466e-04, -1.5505e-02,  5.3769e-03,\n",
      "        -2.7386e-02, -2.4769e-02,  2.9320e-02,  2.9647e-02,  1.6486e-02,\n",
      "        -2.8118e-02, -1.0107e-02, -2.7156e-02, -8.3679e-03, -3.0004e-02,\n",
      "         6.5497e-03, -5.5672e-03, -3.1488e-04, -1.2564e-04, -1.4423e-02,\n",
      "        -4.0789e-03, -2.0847e-02,  1.3303e-02,  9.6718e-03,  1.1784e-02,\n",
      "         1.7530e-02,  2.1290e-02,  4.2822e-03, -2.0111e-02,  1.1368e-02,\n",
      "        -9.1601e-03, -2.1328e-02, -2.7466e-02, -1.6885e-02, -1.6091e-02,\n",
      "         9.7244e-03, -1.9936e-02,  1.2929e-02,  1.0389e-02,  1.1538e-02,\n",
      "        -2.1175e-02, -6.5031e-03, -3.0190e-02, -2.3273e-02,  2.0548e-02,\n",
      "         7.8327e-03, -1.8466e-02,  1.3262e-02,  1.5655e-02,  1.3456e-02,\n",
      "         2.1307e-02,  9.7154e-03,  2.8025e-02, -3.0610e-03,  2.1344e-02,\n",
      "        -1.8840e-02,  3.0141e-02, -1.1695e-02, -5.4149e-03, -7.0828e-03,\n",
      "         8.4438e-04,  4.5137e-03,  2.0981e-02,  5.3983e-03, -9.9910e-03,\n",
      "         9.3201e-03,  7.4768e-03,  2.6612e-02, -2.7095e-02,  2.9950e-02,\n",
      "         1.3759e-02,  9.9071e-03, -3.0285e-02,  1.4458e-02, -2.7577e-02,\n",
      "        -2.2822e-02,  1.6508e-02, -2.9068e-02, -1.6515e-03,  2.4786e-02,\n",
      "         2.7606e-02, -2.8406e-02, -1.3199e-02,  8.9855e-03,  2.2281e-03,\n",
      "        -2.1551e-02, -2.5784e-02, -1.6122e-02, -2.1111e-02,  1.1811e-02,\n",
      "         3.2591e-04,  1.9587e-02, -8.2446e-03,  2.3548e-02, -3.0504e-02,\n",
      "        -3.1332e-03, -5.5919e-04,  2.5337e-02,  3.0130e-02, -6.7615e-03,\n",
      "        -1.1696e-02, -1.7320e-02])}, Parameter containing:\n",
      "tensor([[-0.0277, -0.0181, -0.0108,  ..., -0.0538,  0.0203,  0.0191],\n",
      "        [ 0.0089, -0.0076, -0.0139,  ..., -0.0477,  0.0035, -0.0482],\n",
      "        [-0.0173,  0.0101, -0.0397,  ..., -0.1093, -0.0136, -0.0085],\n",
      "        ...,\n",
      "        [ 0.0339,  0.0080,  0.0308,  ..., -0.0766,  0.0043, -0.0093],\n",
      "        [-0.0417,  0.0071,  0.0091,  ..., -0.0197,  0.0193, -0.0354],\n",
      "        [-0.0422,  0.0207,  0.0240,  ..., -0.0659, -0.0279,  0.0017]],\n",
      "       requires_grad=True): {'old_init': tensor([[-0.0277, -0.0181, -0.0108,  ..., -0.0011,  0.0203,  0.0395],\n",
      "        [ 0.0089, -0.0076, -0.0139,  ..., -0.0091,  0.0035, -0.0333],\n",
      "        [-0.0173,  0.0101, -0.0397,  ..., -0.0374, -0.0136,  0.0193],\n",
      "        ...,\n",
      "        [ 0.0339,  0.0080,  0.0308,  ..., -0.0311,  0.0043,  0.0083],\n",
      "        [-0.0417,  0.0071,  0.0091,  ...,  0.0259,  0.0193, -0.0177],\n",
      "        [-0.0422,  0.0207,  0.0240,  ..., -0.0110, -0.0279,  0.0229]])}, Parameter containing:\n",
      "tensor([-0.1434, -0.1015, -0.1162, -0.0842, -0.0748,  0.8578, -0.0959, -0.0589,\n",
      "        -0.1204, -0.0854], requires_grad=True): {'old_init': tensor([-0.0406, -0.0262,  0.0240,  0.0101, -0.0002, -0.0005, -0.0099,  0.0299,\n",
      "        -0.0314,  0.0218])}, 'tmp': {}})\n",
      "param_state\n",
      "param_state\n",
      "param_state\n",
      "param_state\n",
      "param_state\n",
      "param_state\n",
      "param_state\n",
      "param_state\n"
     ]
    }
   ],
   "source": [
    "tmp.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211732b5",
   "metadata": {},
   "source": [
    "###### a = defaultdict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1250,
   "id": "c50dcb59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(dict,\n",
       "            {Parameter containing:\n",
       "             tensor([[[[ 0.0135,  0.0834, -0.1635, -0.1240,  0.0150],\n",
       "                       [ 0.1076,  0.0894,  0.1221, -0.0690, -0.1478],\n",
       "                       [-0.1232,  0.1202,  0.1382,  0.1460,  0.0215],\n",
       "                       [ 0.1577,  0.0469,  0.0594,  0.0215,  0.1838],\n",
       "                       [ 0.1121, -0.2022, -0.1584,  0.0095,  0.0925]]],\n",
       "             \n",
       "             \n",
       "                     [[[-0.0625,  0.1570,  0.0061,  0.1393,  0.1369],\n",
       "                       [-0.1410,  0.1502,  0.0581,  0.1548, -0.0272],\n",
       "                       [-0.1242, -0.0803, -0.0600, -0.1837, -0.0414],\n",
       "                       [ 0.0703, -0.0023,  0.0040, -0.1584, -0.1531],\n",
       "                       [ 0.0853,  0.2344,  0.2251,  0.0915,  0.1444]]],\n",
       "             \n",
       "             \n",
       "                     [[[ 0.0254,  0.0159, -0.1718,  0.0243,  0.0359],\n",
       "                       [ 0.1028,  0.0580,  0.1281, -0.1112,  0.0933],\n",
       "                       [ 0.0104,  0.1602, -0.0586,  0.0608, -0.0146],\n",
       "                       [-0.0350, -0.0496,  0.1493, -0.1740, -0.0495],\n",
       "                       [-0.0526, -0.0541, -0.1318,  0.1012,  0.0460]]],\n",
       "             \n",
       "             \n",
       "                     [[[ 0.0523, -0.1491, -0.1451, -0.1036,  0.1483],\n",
       "                       [ 0.1593, -0.1866,  0.0807, -0.1690,  0.1838],\n",
       "                       [-0.1610, -0.1925, -0.0687, -0.0771, -0.1243],\n",
       "                       [-0.1310, -0.1723, -0.1332, -0.1387,  0.1657],\n",
       "                       [ 0.0673,  0.1843, -0.1727, -0.0221,  0.0475]]],\n",
       "             \n",
       "             \n",
       "                     [[[-0.1166, -0.2003,  0.0023, -0.1722, -0.1807],\n",
       "                       [ 0.1743, -0.1324,  0.1766, -0.1933,  0.0421],\n",
       "                       [ 0.0664, -0.1061,  0.1350, -0.1796,  0.0959],\n",
       "                       [-0.1545, -0.0873, -0.1879, -0.0640, -0.0866],\n",
       "                       [ 0.0948, -0.0368, -0.0359,  0.0155,  0.1910]]],\n",
       "             \n",
       "             \n",
       "                     [[[ 0.0258,  0.1585, -0.0605, -0.1808, -0.1016],\n",
       "                       [ 0.1053,  0.1681, -0.0658, -0.1539,  0.0900],\n",
       "                       [ 0.1305,  0.0858,  0.0231,  0.2134,  0.0425],\n",
       "                       [-0.0314, -0.0843, -0.1412,  0.1217, -0.1732],\n",
       "                       [ 0.1602,  0.1400,  0.1269,  0.0630,  0.0827]]],\n",
       "             \n",
       "             \n",
       "                     [[[ 0.0358, -0.1917, -0.1120,  0.2049, -0.1158],\n",
       "                       [-0.0061, -0.1651,  0.0857, -0.0805,  0.1048],\n",
       "                       [ 0.2273,  0.0632, -0.0317,  0.1479,  0.1683],\n",
       "                       [-0.1713, -0.0821,  0.2332,  0.1289,  0.1782],\n",
       "                       [-0.2157,  0.0590, -0.1970,  0.1742,  0.1031]]],\n",
       "             \n",
       "             \n",
       "                     [[[ 0.1946,  0.1226, -0.0379,  0.1271,  0.1310],\n",
       "                       [ 0.0521,  0.1187,  0.0522, -0.0669,  0.0973],\n",
       "                       [ 0.0018,  0.2133, -0.1862,  0.1631, -0.0204],\n",
       "                       [ 0.0216, -0.1197, -0.1432, -0.0932,  0.0281],\n",
       "                       [ 0.0581,  0.1875,  0.1514, -0.1499, -0.1222]]],\n",
       "             \n",
       "             \n",
       "                     [[[-0.1871,  0.0648, -0.0124,  0.0088, -0.0019],\n",
       "                       [ 0.1854, -0.1833, -0.1256, -0.0211, -0.2010],\n",
       "                       [-0.1089,  0.0371, -0.1193, -0.1104,  0.0947],\n",
       "                       [ 0.0985,  0.0924, -0.1460, -0.1077,  0.1423],\n",
       "                       [-0.0402, -0.0136,  0.1983, -0.0739,  0.2171]]],\n",
       "             \n",
       "             \n",
       "                     [[[-0.0755,  0.0595,  0.1407, -0.2088, -0.0349],\n",
       "                       [ 0.1426, -0.2198, -0.0138, -0.0581, -0.1707],\n",
       "                       [ 0.1371,  0.0209, -0.0216, -0.0762,  0.1080],\n",
       "                       [ 0.0938,  0.0840,  0.0349, -0.1152, -0.2312],\n",
       "                       [ 0.0812, -0.0777, -0.0365,  0.0252, -0.0585]]],\n",
       "             \n",
       "             \n",
       "                     [[[-0.0267,  0.0877,  0.0532,  0.0864, -0.0226],\n",
       "                       [-0.1922,  0.2107,  0.1760,  0.1296, -0.1210],\n",
       "                       [ 0.0003,  0.0457,  0.1716, -0.0477,  0.0795],\n",
       "                       [-0.0709,  0.0425,  0.1391,  0.1358,  0.0675],\n",
       "                       [-0.0902,  0.1600, -0.0145,  0.0142, -0.0277]]],\n",
       "             \n",
       "             \n",
       "                     [[[-0.1873,  0.1259,  0.1329,  0.2461, -0.0952],\n",
       "                       [ 0.0395, -0.1034,  0.2120, -0.0411,  0.0971],\n",
       "                       [ 0.0161, -0.0857,  0.1236,  0.1347, -0.0836],\n",
       "                       [ 0.1901,  0.0359,  0.1728, -0.1359,  0.2418],\n",
       "                       [-0.1364, -0.1452,  0.0169,  0.2173,  0.0485]]],\n",
       "             \n",
       "             \n",
       "                     [[[ 0.1180, -0.1290,  0.1516, -0.0370,  0.0083],\n",
       "                       [-0.1335, -0.1046, -0.1384, -0.0997, -0.2158],\n",
       "                       [ 0.0763, -0.1950,  0.0490, -0.1629, -0.1906],\n",
       "                       [-0.1712,  0.1970, -0.1103,  0.1659, -0.1250],\n",
       "                       [ 0.0452, -0.0650,  0.0322,  0.1534,  0.0450]]],\n",
       "             \n",
       "             \n",
       "                     [[[ 0.1303, -0.0990,  0.1991, -0.1845, -0.1459],\n",
       "                       [ 0.0317, -0.1254,  0.1798,  0.0878, -0.0153],\n",
       "                       [-0.0719,  0.2238,  0.0349,  0.0481, -0.0195],\n",
       "                       [-0.0012,  0.1490,  0.0777, -0.0401, -0.1298],\n",
       "                       [-0.0932, -0.1288, -0.1446,  0.0063, -0.1167]]],\n",
       "             \n",
       "             \n",
       "                     [[[-0.0092,  0.0875,  0.1393, -0.0748, -0.0498],\n",
       "                       [ 0.0256, -0.0404,  0.1814,  0.0301,  0.0224],\n",
       "                       [-0.0543, -0.1303,  0.1696,  0.0547,  0.1267],\n",
       "                       [ 0.1552, -0.0524,  0.0304, -0.0342,  0.0414],\n",
       "                       [ 0.0382, -0.1561, -0.1196,  0.0560, -0.0426]]],\n",
       "             \n",
       "             \n",
       "                     [[[-0.1724, -0.0990,  0.0374, -0.0619, -0.0433],\n",
       "                       [ 0.0275, -0.0777, -0.1589,  0.0274,  0.1841],\n",
       "                       [-0.1549,  0.0612,  0.0661, -0.0064,  0.1761],\n",
       "                       [ 0.0730,  0.1787, -0.1636,  0.0290,  0.1941],\n",
       "                       [ 0.1649, -0.0269, -0.1209, -0.0951,  0.0009]]],\n",
       "             \n",
       "             \n",
       "                     [[[ 0.2331,  0.2258,  0.2711,  0.2831,  0.1886],\n",
       "                       [ 0.0851,  0.1380,  0.1660,  0.0121, -0.0609],\n",
       "                       [ 0.0124, -0.0006,  0.0990, -0.1002, -0.1323],\n",
       "                       [-0.2045,  0.1496,  0.0704, -0.0544,  0.1827],\n",
       "                       [-0.0494, -0.1172,  0.1525, -0.1420,  0.1531]]],\n",
       "             \n",
       "             \n",
       "                     [[[ 0.0690,  0.1429,  0.1603, -0.1402,  0.1642],\n",
       "                       [-0.1474,  0.1451,  0.1482, -0.0824,  0.2107],\n",
       "                       [ 0.1589, -0.0015,  0.0661,  0.0453, -0.0813],\n",
       "                       [-0.0687,  0.0027, -0.1749, -0.0774, -0.0202],\n",
       "                       [ 0.0614, -0.0686, -0.1603, -0.1345,  0.1668]]],\n",
       "             \n",
       "             \n",
       "                     [[[ 0.2234, -0.1157,  0.2208,  0.1601,  0.0766],\n",
       "                       [-0.0649,  0.0621, -0.0811,  0.1370,  0.0545],\n",
       "                       [-0.0655, -0.0088, -0.1224, -0.1125,  0.1804],\n",
       "                       [-0.0673, -0.1320,  0.1635, -0.1471,  0.0069],\n",
       "                       [-0.1517, -0.2006,  0.0924,  0.1809,  0.1496]]],\n",
       "             \n",
       "             \n",
       "                     [[[-0.0025,  0.1443,  0.0174,  0.0232, -0.1461],\n",
       "                       [-0.0627,  0.0182, -0.0657,  0.2101,  0.0402],\n",
       "                       [ 0.0805, -0.1545,  0.1720,  0.1626, -0.0382],\n",
       "                       [ 0.1976,  0.0982, -0.0351,  0.0006,  0.2624],\n",
       "                       [ 0.0055,  0.2480, -0.1111,  0.2202,  0.1832]]],\n",
       "             \n",
       "             \n",
       "                     [[[ 0.3022,  0.2427, -0.1028, -0.0976,  0.1901],\n",
       "                       [ 0.1156,  0.1059,  0.0971,  0.1397,  0.1990],\n",
       "                       [ 0.1251, -0.1811, -0.0711, -0.0224, -0.0822],\n",
       "                       [-0.1492, -0.1688, -0.0462, -0.1098, -0.1868],\n",
       "                       [ 0.0509, -0.0629, -0.0965,  0.0679,  0.1303]]],\n",
       "             \n",
       "             \n",
       "                     [[[ 0.0047, -0.1515, -0.1914, -0.0853, -0.0127],\n",
       "                       [-0.1211,  0.0330, -0.0010, -0.1034,  0.0737],\n",
       "                       [-0.0227,  0.0031, -0.0279,  0.1258,  0.1343],\n",
       "                       [-0.0298,  0.0576, -0.1494, -0.0595,  0.1605],\n",
       "                       [-0.1548, -0.1767,  0.1322,  0.0387, -0.0155]]],\n",
       "             \n",
       "             \n",
       "                     [[[ 0.2051,  0.2821,  0.2756,  0.0940,  0.1375],\n",
       "                       [-0.1472, -0.0164, -0.1026, -0.0379, -0.1374],\n",
       "                       [ 0.0634, -0.0502,  0.1892,  0.1894, -0.1735],\n",
       "                       [-0.1031,  0.0435, -0.0016, -0.2340,  0.1660],\n",
       "                       [ 0.0055, -0.0299,  0.0141,  0.1227, -0.1556]]],\n",
       "             \n",
       "             \n",
       "                     [[[-0.0964,  0.0513, -0.1124, -0.0549, -0.0088],\n",
       "                       [ 0.0687, -0.2043,  0.0323, -0.0136, -0.0300],\n",
       "                       [ 0.0765, -0.0577, -0.1009, -0.2061,  0.0690],\n",
       "                       [ 0.1918, -0.0174, -0.1005,  0.0680,  0.1170],\n",
       "                       [-0.1754, -0.0726, -0.0102, -0.1012,  0.0426]]],\n",
       "             \n",
       "             \n",
       "                     [[[-0.0140, -0.1365, -0.2059,  0.0698,  0.0709],\n",
       "                       [-0.1770,  0.1414, -0.0972,  0.1095, -0.2063],\n",
       "                       [-0.1484,  0.1153, -0.0299, -0.0252, -0.1552],\n",
       "                       [-0.1791, -0.0278,  0.1730,  0.1276, -0.0038],\n",
       "                       [-0.1634, -0.1221, -0.1781,  0.0704, -0.1323]]],\n",
       "             \n",
       "             \n",
       "                     [[[-0.1827,  0.0661,  0.0416,  0.0590, -0.0038],\n",
       "                       [ 0.0322, -0.0074, -0.1251,  0.1627,  0.0389],\n",
       "                       [ 0.0722, -0.0004, -0.1702, -0.1344, -0.1649],\n",
       "                       [-0.0498, -0.0869,  0.1245,  0.0755,  0.0189],\n",
       "                       [ 0.0202, -0.0826,  0.0735,  0.1341,  0.1714]]],\n",
       "             \n",
       "             \n",
       "                     [[[ 0.0910, -0.1679, -0.1144,  0.0028, -0.1899],\n",
       "                       [ 0.2008, -0.1236, -0.1167, -0.0485, -0.2017],\n",
       "                       [ 0.0075,  0.1203,  0.1463,  0.1102,  0.2008],\n",
       "                       [ 0.1607,  0.0987,  0.1726,  0.1617, -0.1414],\n",
       "                       [ 0.2336, -0.0606, -0.1322,  0.1067, -0.1322]]],\n",
       "             \n",
       "             \n",
       "                     [[[ 0.1610,  0.1389, -0.1359,  0.1826, -0.0117],\n",
       "                       [ 0.1951,  0.2412,  0.2236,  0.1284, -0.0325],\n",
       "                       [ 0.1164, -0.1455, -0.0687,  0.0780,  0.0011],\n",
       "                       [ 0.0872,  0.0874, -0.0834, -0.1183, -0.1861],\n",
       "                       [-0.0710, -0.1174, -0.0541, -0.1283,  0.0560]]],\n",
       "             \n",
       "             \n",
       "                     [[[ 0.1656,  0.0900,  0.1720, -0.1492, -0.1609],\n",
       "                       [ 0.0581,  0.1254, -0.0758, -0.0985, -0.0491],\n",
       "                       [ 0.0385, -0.0426,  0.1291, -0.1667,  0.0530],\n",
       "                       [-0.1847,  0.1817, -0.1761, -0.0490,  0.1052],\n",
       "                       [-0.0158, -0.0972, -0.0699, -0.0585,  0.0422]]],\n",
       "             \n",
       "             \n",
       "                     [[[-0.1290,  0.0740, -0.1960,  0.1197, -0.2121],\n",
       "                       [-0.1132, -0.0993, -0.0719,  0.1210, -0.1927],\n",
       "                       [-0.1608,  0.0266, -0.1144, -0.1234,  0.1449],\n",
       "                       [-0.0518,  0.0473, -0.0158,  0.0591, -0.1861],\n",
       "                       [-0.1444, -0.1597,  0.1668, -0.0949,  0.0619]]],\n",
       "             \n",
       "             \n",
       "                     [[[-0.0260, -0.1610, -0.0087,  0.1878, -0.0471],\n",
       "                       [ 0.0208, -0.1951, -0.0533,  0.0065, -0.1599],\n",
       "                       [-0.0274, -0.0910, -0.0093, -0.1148, -0.0837],\n",
       "                       [-0.1454,  0.1133, -0.0086,  0.0076, -0.0154],\n",
       "                       [ 0.1372,  0.0088, -0.1366,  0.0757, -0.1980]]],\n",
       "             \n",
       "             \n",
       "                     [[[-0.1191, -0.1079, -0.0525, -0.2092, -0.1821],\n",
       "                       [-0.2208,  0.1350, -0.1045,  0.0367, -0.0656],\n",
       "                       [ 0.2145, -0.1643,  0.1399, -0.0457,  0.0070],\n",
       "                       [ 0.0715, -0.0375,  0.2151, -0.1547,  0.1216],\n",
       "                       [ 0.1202,  0.0983,  0.1527,  0.1357,  0.1246]]]], requires_grad=True): {'old_init': tensor([[[[-0.0152,  0.0386, -0.1857, -0.1259,  0.0449],\n",
       "                        [ 0.0759,  0.0275,  0.0646, -0.0870, -0.1584],\n",
       "                        [-0.1323,  0.0762,  0.0831,  0.1156,  0.0071],\n",
       "                        [ 0.1702,  0.0632,  0.0493,  0.0004,  0.1970],\n",
       "                        [ 0.1274, -0.1625, -0.1157,  0.0324,  0.1151]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0372,  0.1566,  0.0055,  0.1552,  0.1695],\n",
       "                        [-0.1298,  0.1493,  0.0488,  0.1698, -0.0087],\n",
       "                        [-0.1463, -0.0848, -0.0599, -0.1671,  0.0008],\n",
       "                        [ 0.0395, -0.0326, -0.0025, -0.1419, -0.1266],\n",
       "                        [ 0.0562,  0.1891,  0.1971,  0.0958,  0.1444]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0024, -0.0018, -0.1555,  0.0271,  0.0251],\n",
       "                        [ 0.0881,  0.0476,  0.1273, -0.0854,  0.1086],\n",
       "                        [ 0.0024,  0.1491, -0.0637,  0.0970,  0.0140],\n",
       "                        [-0.0280, -0.0347,  0.1469, -0.1742, -0.0478],\n",
       "                        [-0.0275, -0.0190, -0.0903,  0.1244,  0.0652]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0477, -0.1364, -0.1304, -0.0934,  0.1813],\n",
       "                        [ 0.1439, -0.1863,  0.0850, -0.1653,  0.2010],\n",
       "                        [-0.1756, -0.1918, -0.0653, -0.0690, -0.1052],\n",
       "                        [-0.1325, -0.1737, -0.1309, -0.1341,  0.1612],\n",
       "                        [ 0.0635,  0.1815, -0.1735, -0.0317,  0.0330]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1073, -0.1985,  0.0102, -0.1667, -0.1728],\n",
       "                        [ 0.1718, -0.1388,  0.1744, -0.1905,  0.0423],\n",
       "                        [ 0.0574, -0.1110,  0.1356, -0.1725,  0.1017],\n",
       "                        [-0.1617, -0.0941, -0.1831, -0.0623, -0.0877],\n",
       "                        [ 0.0744, -0.0582, -0.0429,  0.0141,  0.1858]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0595,  0.1826, -0.0433, -0.1696, -0.0952],\n",
       "                        [ 0.1020,  0.1536, -0.0785, -0.1544,  0.0721],\n",
       "                        [ 0.1073,  0.0370, -0.0289,  0.2015,  0.0399],\n",
       "                        [-0.0380, -0.0941, -0.1910,  0.0866, -0.1735],\n",
       "                        [ 0.1529,  0.1513,  0.1084,  0.0184,  0.0508]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0230, -0.1833, -0.1282,  0.1670, -0.1448],\n",
       "                        [-0.0429, -0.1925,  0.0490, -0.1237,  0.0517],\n",
       "                        [ 0.1884,  0.0144, -0.0966,  0.0894,  0.1150],\n",
       "                        [-0.1623, -0.0973,  0.1976,  0.0590,  0.1257],\n",
       "                        [-0.1922,  0.0811, -0.1917,  0.1439,  0.0464]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1886,  0.1154, -0.0527,  0.1223,  0.1283],\n",
       "                        [ 0.0274,  0.0996,  0.0391, -0.0656,  0.0809],\n",
       "                        [-0.0360,  0.1981, -0.1783,  0.1901, -0.0071],\n",
       "                        [ 0.0025, -0.1209, -0.1283, -0.0763,  0.0456],\n",
       "                        [ 0.0456,  0.1856,  0.1684, -0.1269, -0.0896]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1803,  0.0844,  0.0176,  0.0300,  0.0136],\n",
       "                        [ 0.1944, -0.1746, -0.1113,  0.0008, -0.1678],\n",
       "                        [-0.1079,  0.0400, -0.1120, -0.0939,  0.1270],\n",
       "                        [ 0.0797,  0.0765, -0.1396, -0.0967,  0.1382],\n",
       "                        [-0.0492, -0.0358,  0.1660, -0.0980,  0.1805]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0905,  0.0672,  0.1725, -0.1804,  0.0064],\n",
       "                        [ 0.1572, -0.1944,  0.0274,  0.0042, -0.1084],\n",
       "                        [ 0.1831,  0.0645,  0.0250, -0.0237,  0.1637],\n",
       "                        [ 0.1474,  0.1325,  0.0963, -0.0480, -0.1729],\n",
       "                        [ 0.1320, -0.0456, -0.0085,  0.0557, -0.0415]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0431,  0.0749,  0.0567,  0.0913, -0.0173],\n",
       "                        [-0.1927,  0.1948,  0.1634,  0.1316, -0.1074],\n",
       "                        [-0.0176,  0.0320,  0.1703, -0.0569,  0.0806],\n",
       "                        [-0.0866,  0.0385,  0.1417,  0.1258,  0.0583],\n",
       "                        [-0.1024,  0.1575,  0.0050,  0.0361, -0.0351]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1710,  0.0959,  0.0859,  0.1934, -0.1124],\n",
       "                        [ 0.0389, -0.1318,  0.1484, -0.0972,  0.0633],\n",
       "                        [ 0.0069, -0.1007,  0.1038,  0.0829, -0.1468],\n",
       "                        [ 0.1986,  0.0411,  0.1542, -0.1694,  0.1883],\n",
       "                        [-0.1018, -0.1374,  0.0124,  0.1893,  0.0081]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1251, -0.1244,  0.1572, -0.0280,  0.0052],\n",
       "                        [-0.1216, -0.0892, -0.1198, -0.0778, -0.1979],\n",
       "                        [ 0.0866, -0.1720,  0.0683, -0.1439, -0.1753],\n",
       "                        [-0.1781,  0.1923, -0.1146,  0.1725, -0.1119],\n",
       "                        [ 0.0117, -0.0818,  0.0193,  0.1374,  0.0325]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1084, -0.1389,  0.1719, -0.1924, -0.1475],\n",
       "                        [ 0.0411, -0.1488,  0.1454,  0.0603, -0.0269],\n",
       "                        [-0.0699,  0.1974, -0.0028,  0.0197, -0.0368],\n",
       "                        [-0.0175,  0.1351,  0.0616, -0.0519, -0.1321],\n",
       "                        [-0.1060, -0.1300, -0.1506,  0.0127, -0.1086]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0201,  0.0771,  0.1142, -0.0948, -0.0440],\n",
       "                        [ 0.0595, -0.0318,  0.1449,  0.0009, -0.0008],\n",
       "                        [-0.0471, -0.1449,  0.1396,  0.0340,  0.1216],\n",
       "                        [ 0.1542, -0.0669,  0.0037, -0.0425,  0.0316],\n",
       "                        [ 0.0299, -0.1717, -0.1425,  0.0301, -0.0656]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1616, -0.1006,  0.0118, -0.0837, -0.0611],\n",
       "                        [ 0.0457, -0.0545, -0.1394,  0.0267,  0.1666],\n",
       "                        [-0.1407,  0.0643,  0.0728, -0.0080,  0.1470],\n",
       "                        [ 0.0779,  0.1736, -0.1783,  0.0114,  0.1641],\n",
       "                        [ 0.1532, -0.0358, -0.1317, -0.1161, -0.0059]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1487,  0.1396,  0.1898,  0.2061,  0.1425],\n",
       "                        [ 0.0374,  0.0989,  0.1337, -0.0301, -0.0937],\n",
       "                        [ 0.0058, -0.0086,  0.1076, -0.0974, -0.1181],\n",
       "                        [-0.1817,  0.1607,  0.1002, -0.0092,  0.1923],\n",
       "                        [-0.0049, -0.0880,  0.1694, -0.1278,  0.1519]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0508,  0.1124,  0.1443, -0.1432,  0.1463],\n",
       "                        [-0.1509,  0.1322,  0.1359, -0.0763,  0.1929],\n",
       "                        [ 0.1529, -0.0030,  0.0661,  0.0419, -0.0780],\n",
       "                        [-0.0642,  0.0051, -0.1784, -0.0909, -0.0187],\n",
       "                        [ 0.0708, -0.0608, -0.1534, -0.1253,  0.1936]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1965, -0.1381,  0.1828,  0.1172,  0.0639],\n",
       "                        [-0.0640,  0.0604, -0.0892,  0.1004,  0.0318],\n",
       "                        [-0.0540,  0.0097, -0.1111, -0.1160,  0.1498],\n",
       "                        [-0.0576, -0.1234,  0.1718, -0.1387, -0.0076],\n",
       "                        [-0.1519, -0.1910,  0.0961,  0.1909,  0.1467]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0140,  0.1391,  0.0101,  0.0170, -0.1416],\n",
       "                        [-0.0431,  0.0249, -0.0728,  0.1794,  0.0083],\n",
       "                        [ 0.0748, -0.1763,  0.1295,  0.1052, -0.1135],\n",
       "                        [ 0.1456,  0.0483, -0.0860, -0.0580,  0.1848],\n",
       "                        [-0.0545,  0.1899, -0.1692,  0.1652,  0.1359]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1929,  0.1725, -0.1544, -0.1523,  0.1434],\n",
       "                        [ 0.0150,  0.0705,  0.0660,  0.1043,  0.1766],\n",
       "                        [ 0.0908, -0.1699, -0.0748, -0.0482, -0.0769],\n",
       "                        [-0.1621, -0.1562, -0.0464, -0.1193, -0.1743],\n",
       "                        [ 0.0320, -0.0532, -0.0827,  0.0716,  0.1355]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0324, -0.1384, -0.1822, -0.0865, -0.0304],\n",
       "                        [-0.0874,  0.0720,  0.0351, -0.0851,  0.0638],\n",
       "                        [ 0.0008,  0.0268, -0.0026,  0.1466,  0.1324],\n",
       "                        [-0.0324,  0.0514, -0.1477, -0.0563,  0.1445],\n",
       "                        [-0.1676, -0.1892,  0.1035,  0.0217, -0.0269]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1566,  0.1982,  0.2012,  0.0467,  0.1341],\n",
       "                        [-0.1758, -0.0684, -0.1491, -0.0833, -0.1503],\n",
       "                        [ 0.0382, -0.0608,  0.1872,  0.1754, -0.1834],\n",
       "                        [-0.0984,  0.0626,  0.0218, -0.1976,  0.1927],\n",
       "                        [ 0.0284, -0.0061,  0.0373,  0.1648, -0.1281]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0862,  0.0757, -0.0994, -0.0675, -0.0109],\n",
       "                        [ 0.0647, -0.1737,  0.0891,  0.0442,  0.0137],\n",
       "                        [ 0.0670, -0.0391, -0.0472, -0.1127,  0.1518],\n",
       "                        [ 0.1914, -0.0186, -0.0848,  0.1347,  0.1913],\n",
       "                        [-0.1766, -0.0806, -0.0013, -0.0791,  0.0682]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0282, -0.1343, -0.1901,  0.0987,  0.0981],\n",
       "                        [-0.1866,  0.1324, -0.0892,  0.1291, -0.1765],\n",
       "                        [-0.1439,  0.1072, -0.0331, -0.0277, -0.1493],\n",
       "                        [-0.1698, -0.0214,  0.1714,  0.1197, -0.0188],\n",
       "                        [-0.1590, -0.1142, -0.1661,  0.0675, -0.1524]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1539,  0.1004,  0.0761,  0.0871,  0.0096],\n",
       "                        [ 0.0486,  0.0213, -0.0878,  0.2010,  0.0486],\n",
       "                        [ 0.0754,  0.0430, -0.1219, -0.1061, -0.1713],\n",
       "                        [-0.0520, -0.0800,  0.1372,  0.0812,  0.0217],\n",
       "                        [-0.0214, -0.1069,  0.0513,  0.1281,  0.1611]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0779, -0.1636, -0.0921,  0.0246, -0.1633],\n",
       "                        [ 0.1965, -0.1053, -0.0969, -0.0293, -0.1930],\n",
       "                        [-0.0015,  0.1042,  0.1207,  0.0950,  0.1729],\n",
       "                        [ 0.1402,  0.0450,  0.1160,  0.1089, -0.1816],\n",
       "                        [ 0.2011, -0.1109, -0.1863,  0.0595, -0.1705]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1029,  0.0774, -0.1909,  0.1519, -0.0276],\n",
       "                        [ 0.1548,  0.1980,  0.1968,  0.1334, -0.0076],\n",
       "                        [ 0.0989, -0.1721, -0.0771,  0.0728,  0.0091],\n",
       "                        [ 0.0727,  0.0696, -0.0844, -0.1031, -0.1809],\n",
       "                        [-0.0808, -0.1465, -0.0490, -0.1040,  0.0738]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1688,  0.0992,  0.1947, -0.1457, -0.1433],\n",
       "                        [ 0.0727,  0.1356, -0.0551, -0.1076, -0.0527],\n",
       "                        [ 0.0629, -0.0227,  0.1369, -0.1776,  0.0321],\n",
       "                        [-0.1736,  0.1888, -0.1870, -0.0627,  0.0882],\n",
       "                        [-0.0228, -0.1034, -0.0808, -0.0636,  0.0443]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.1285,  0.0774, -0.1589,  0.1692, -0.1979],\n",
       "                        [-0.1155, -0.1007, -0.0506,  0.1736, -0.1556],\n",
       "                        [-0.1636,  0.0238, -0.1127, -0.0875,  0.1941],\n",
       "                        [-0.0541,  0.0484, -0.0071,  0.0744, -0.1486],\n",
       "                        [-0.1375, -0.1526,  0.1776, -0.0893,  0.0776]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0146, -0.1678, -0.0129,  0.1797, -0.0573],\n",
       "                        [ 0.0424, -0.1938, -0.0580, -0.0025, -0.1767],\n",
       "                        [ 0.0003, -0.0705, -0.0041, -0.1216, -0.0830],\n",
       "                        [-0.1024,  0.1557,  0.0191,  0.0182, -0.0061],\n",
       "                        [ 0.1597,  0.0347, -0.1171,  0.0839, -0.1919]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0687, -0.0541, -0.0125, -0.1856, -0.1628],\n",
       "                        [-0.1945,  0.1688, -0.0685,  0.0450, -0.0609],\n",
       "                        [ 0.1902, -0.1606,  0.1492, -0.0622, -0.0159],\n",
       "                        [ 0.0233, -0.0643,  0.1896, -0.1938,  0.0679],\n",
       "                        [ 0.0488,  0.0222,  0.0765,  0.0598,  0.0466]]]])},\n",
       "             Parameter containing:\n",
       "             tensor([ 0.1843, -0.0959,  0.0776,  0.0679,  0.2058, -0.0646, -0.0948, -0.0818,\n",
       "                     -0.1473,  0.1604, -0.1550, -0.0956,  0.0592,  0.0685, -0.0744, -0.0407,\n",
       "                     -0.1664, -0.0991, -0.0954, -0.0941, -0.1528,  0.0499,  0.1375, -0.1847,\n",
       "                     -0.0059, -0.1087, -0.0830, -0.0058,  0.0077, -0.0898,  0.0126, -0.0229],\n",
       "                    requires_grad=True): {'old_init': tensor([ 0.1537, -0.1086,  0.0854,  0.0634,  0.1898, -0.0667, -0.1316, -0.0806,\n",
       "                      -0.1724,  0.1963, -0.1622, -0.1082,  0.0140,  0.0677, -0.0783, -0.0614,\n",
       "                      -0.1944, -0.1073, -0.1193, -0.1313, -0.1802,  0.0343,  0.1196, -0.1539,\n",
       "                      -0.0257, -0.1086, -0.1013, -0.0185,  0.0322, -0.0832,  0.0260, -0.0789])},\n",
       "             Parameter containing:\n",
       "             tensor([[[[ 1.6701e-02,  8.7833e-03,  4.1980e-02,  3.7858e-02, -6.0850e-03],\n",
       "                       [-1.5669e-02, -1.2226e-02, -3.8945e-02,  3.5946e-02, -2.9957e-02],\n",
       "                       [-5.1612e-02,  5.0463e-02,  7.8135e-02,  5.3092e-03,  4.8493e-02],\n",
       "                       [ 7.4844e-04,  2.2790e-02,  6.3969e-02,  2.7960e-02,  1.7805e-02],\n",
       "                       [ 4.1365e-02,  2.7963e-02,  7.2958e-04, -1.2172e-03,  4.3596e-02]],\n",
       "             \n",
       "                      [[-5.5195e-04,  3.5474e-02,  4.2482e-02, -4.0888e-03,  2.3075e-02],\n",
       "                       [-1.8875e-02, -8.1097e-04,  6.3287e-02,  5.8074e-02,  1.6550e-02],\n",
       "                       [-1.2617e-02, -4.1441e-03, -1.2805e-02, -1.0872e-02,  3.0019e-02],\n",
       "                       [-3.6194e-02, -1.3910e-02,  1.7109e-02,  5.1690e-02,  4.5130e-02],\n",
       "                       [ 2.9172e-02, -4.9641e-03,  1.7892e-02,  6.9318e-02,  2.4951e-02]],\n",
       "             \n",
       "                      [[ 3.1722e-02,  5.2252e-02,  2.3147e-02,  3.0718e-02,  3.9214e-03],\n",
       "                       [ 2.5326e-02, -1.5305e-03,  2.9193e-02,  8.8923e-03,  2.7550e-02],\n",
       "                       [ 1.0631e-02, -2.8263e-02,  3.8224e-02,  3.5795e-02,  1.9196e-03],\n",
       "                       [-1.2026e-02,  7.9063e-03,  1.0707e-02,  4.1354e-02,  2.2981e-02],\n",
       "                       [ 2.4357e-02, -7.6240e-03,  1.7462e-02,  1.6239e-02,  2.0307e-02]],\n",
       "             \n",
       "                      ...,\n",
       "             \n",
       "                      [[-3.0148e-02, -1.2774e-02, -2.1658e-02,  2.4022e-02,  3.6424e-03],\n",
       "                       [-1.0152e-02, -1.7737e-02,  3.6860e-02,  1.3165e-02, -3.2402e-02],\n",
       "                       [ 7.0377e-03,  1.4510e-02, -1.5025e-02, -1.2067e-02,  2.4068e-02],\n",
       "                       [ 2.1438e-02,  4.0998e-02,  3.1523e-02, -3.9569e-02,  3.4165e-03],\n",
       "                       [-1.6811e-02, -1.0333e-02, -3.4366e-03, -2.9226e-02, -1.3677e-02]],\n",
       "             \n",
       "                      [[ 2.2554e-02, -3.0753e-02,  2.5018e-02, -2.7895e-02, -2.5701e-02],\n",
       "                       [-1.0768e-03, -2.1928e-02,  3.2152e-02, -1.4097e-02,  2.0152e-02],\n",
       "                       [-2.7385e-02, -3.1126e-02,  7.2339e-03, -4.3070e-03, -1.0107e-02],\n",
       "                       [-1.7624e-02,  1.0794e-02, -1.6516e-02,  1.5664e-03,  1.1380e-02],\n",
       "                       [ 2.7031e-02,  9.8262e-04, -2.6768e-02, -8.7899e-03,  3.3382e-02]],\n",
       "             \n",
       "                      [[-2.5464e-02, -1.9484e-02, -8.5022e-03, -9.7516e-03,  2.5068e-02],\n",
       "                       [-3.5801e-02, -1.8048e-02,  7.0868e-03, -2.5862e-02,  1.8760e-03],\n",
       "                       [ 2.9388e-02,  7.6436e-03,  4.2682e-02,  3.8518e-02,  1.7517e-02],\n",
       "                       [ 1.7322e-02, -2.2063e-02,  1.9047e-03,  2.5052e-02,  4.4638e-03],\n",
       "                       [-3.1992e-02,  4.0722e-02, -2.1561e-02, -1.5278e-02,  1.6855e-02]]],\n",
       "             \n",
       "             \n",
       "                     [[[ 1.5766e-02,  3.9926e-02, -2.1767e-02, -7.4832e-02,  2.2026e-02],\n",
       "                       [ 3.6630e-02,  2.4216e-02,  3.0323e-02, -2.8563e-02, -3.5186e-02],\n",
       "                       [ 1.4701e-02, -7.5331e-05, -2.3219e-02, -2.0965e-02, -3.1268e-02],\n",
       "                       [-4.8909e-02, -2.9488e-02, -3.9316e-02, -3.3757e-02, -4.1905e-03],\n",
       "                       [-3.0306e-02, -2.8605e-02,  8.9910e-03,  2.5681e-02, -1.5436e-02]],\n",
       "             \n",
       "                      [[-2.7259e-02, -6.5152e-03, -3.4704e-02, -6.2945e-03, -1.5469e-02],\n",
       "                       [ 3.0203e-02, -2.5674e-02, -5.4877e-04, -3.4844e-02, -2.9931e-02],\n",
       "                       [-5.3138e-03,  2.5839e-02, -3.6758e-02,  8.9743e-03, -2.8170e-02],\n",
       "                       [-9.5193e-03, -2.5296e-02, -2.0372e-02, -4.1195e-03, -3.2060e-02],\n",
       "                       [-2.1134e-02, -1.9379e-02, -6.6554e-02, -2.0431e-03,  9.3959e-03]],\n",
       "             \n",
       "                      [[ 5.2276e-03, -4.1530e-02, -2.8019e-02, -4.3656e-02, -3.4915e-03],\n",
       "                       [ 3.9160e-02,  1.3785e-02, -6.2577e-03, -8.8990e-03,  1.2652e-02],\n",
       "                       [ 3.7255e-02, -2.0245e-02,  3.4209e-02, -3.7810e-02, -2.8125e-03],\n",
       "                       [-1.8318e-02, -4.4881e-02, -2.5931e-02, -5.1157e-02, -1.1654e-02],\n",
       "                       [-5.5285e-02, -4.9060e-02, -4.8782e-02, -3.9164e-02,  5.2110e-03]],\n",
       "             \n",
       "                      ...,\n",
       "             \n",
       "                      [[-1.8893e-02,  9.8774e-03, -9.4403e-03, -7.1816e-03, -2.0629e-02],\n",
       "                       [ 1.4333e-02, -3.7053e-02, -4.9528e-02,  1.9381e-02, -7.9162e-03],\n",
       "                       [-1.0022e-02, -2.1747e-02,  2.2375e-03, -1.2178e-03,  2.2664e-02],\n",
       "                       [ 2.0728e-02,  7.5902e-03,  2.0427e-02, -1.2019e-02, -1.7492e-02],\n",
       "                       [ 9.1529e-03, -1.2740e-02,  2.4201e-02, -1.3157e-02, -2.8677e-02]],\n",
       "             \n",
       "                      [[ 2.5423e-02, -7.1705e-03,  9.6423e-03,  1.7813e-02,  2.4837e-02],\n",
       "                       [-2.8538e-02,  2.6121e-03,  3.1367e-02,  3.9089e-02,  1.3789e-02],\n",
       "                       [ 7.8296e-03,  2.3037e-02, -1.1186e-02, -1.7479e-02,  6.1687e-03],\n",
       "                       [-1.8788e-02,  1.1597e-02, -1.7674e-02, -2.2384e-02,  2.1971e-02],\n",
       "                       [-3.1496e-03,  2.4127e-02,  1.3948e-02, -7.7528e-03, -1.4848e-02]],\n",
       "             \n",
       "                      [[ 1.2994e-02, -1.2133e-03,  1.4922e-02,  1.2423e-02, -5.9958e-03],\n",
       "                       [-2.0267e-02,  4.0791e-02, -6.0847e-03, -3.4430e-02, -4.8108e-02],\n",
       "                       [-3.8602e-02, -2.2341e-02, -1.3367e-02,  1.0562e-02,  2.6229e-02],\n",
       "                       [-4.5766e-02, -5.2034e-02, -2.3326e-02, -2.8351e-03,  3.6054e-02],\n",
       "                       [-1.0711e-02, -2.6859e-02, -3.0706e-02,  1.1889e-02,  2.4134e-02]]],\n",
       "             \n",
       "             \n",
       "                     [[[ 7.5486e-02,  6.7498e-02,  6.2558e-02,  8.7704e-02,  7.7898e-02],\n",
       "                       [ 1.3064e-03,  7.1163e-02,  4.6591e-02,  9.3389e-03,  3.4769e-02],\n",
       "                       [ 7.8726e-03, -4.6812e-04,  7.8602e-02,  3.9835e-02,  1.7684e-02],\n",
       "                       [ 5.3654e-02,  1.1084e-02,  4.2242e-02,  9.3810e-02,  3.8566e-02],\n",
       "                       [ 5.0470e-02,  8.5136e-02,  3.1466e-02,  5.9155e-02,  9.5728e-02]],\n",
       "             \n",
       "                      [[ 4.9700e-02,  4.4671e-02,  7.9067e-02,  4.8174e-02, -2.1819e-03],\n",
       "                       [ 4.9305e-02,  3.0071e-02,  6.3161e-02,  7.6519e-02,  5.8998e-02],\n",
       "                       [ 9.6251e-03,  6.3592e-02,  2.3504e-02,  6.9000e-02,  4.5552e-02],\n",
       "                       [ 2.0726e-02,  7.5719e-02,  8.0766e-02,  5.4224e-02,  8.3376e-02],\n",
       "                       [ 3.8435e-02,  7.0363e-02,  4.5328e-02,  3.2340e-02,  1.4881e-02]],\n",
       "             \n",
       "                      [[ 3.5907e-02,  4.0328e-02,  2.5759e-02,  6.4826e-02,  3.3397e-02],\n",
       "                       [ 6.3424e-02,  2.1796e-02,  5.8838e-02,  4.3417e-02, -6.7552e-03],\n",
       "                       [-8.1796e-03,  2.9062e-02,  3.6528e-02,  4.3180e-02,  2.5438e-02],\n",
       "                       [-6.5667e-03, -2.0411e-03,  2.8392e-02,  1.0146e-02,  2.5474e-02],\n",
       "                       [ 5.4668e-03,  7.4851e-02,  4.7259e-02,  2.2703e-02,  2.8898e-02]],\n",
       "             \n",
       "                      ...,\n",
       "             \n",
       "                      [[-1.8396e-02, -1.9422e-03, -3.9727e-03, -4.5143e-03, -5.2103e-03],\n",
       "                       [ 1.5672e-02,  1.3098e-03,  2.3505e-02, -1.6070e-02,  2.1796e-02],\n",
       "                       [ 3.8295e-02,  3.2013e-02,  7.2800e-03,  2.1183e-02,  3.4831e-02],\n",
       "                       [ 9.0370e-03,  5.5142e-02,  1.3368e-02,  8.1202e-03,  3.5873e-02],\n",
       "                       [-1.2659e-02,  2.1763e-02, -8.3004e-03,  2.0973e-04,  1.7948e-02]],\n",
       "             \n",
       "                      [[ 2.1720e-02, -1.9555e-02,  2.7172e-02, -1.2684e-02, -2.3608e-02],\n",
       "                       [ 4.0483e-02,  3.4415e-02, -1.0544e-02,  1.5613e-02,  3.5787e-02],\n",
       "                       [ 3.4318e-03,  3.6490e-02, -1.0694e-02,  1.3899e-02,  2.7772e-02],\n",
       "                       [ 4.7515e-03,  6.1966e-02,  5.5944e-02,  5.8417e-03,  1.2195e-02],\n",
       "                       [ 1.9169e-02,  9.3469e-04, -1.0369e-02,  4.8194e-02,  3.9572e-02]],\n",
       "             \n",
       "                      [[-1.2776e-02,  3.4195e-02,  5.2614e-02,  3.4921e-02,  6.8041e-02],\n",
       "                       [ 2.5679e-02,  3.0943e-02, -2.1855e-02, -1.6246e-02,  3.8805e-02],\n",
       "                       [ 3.5221e-02,  1.3969e-02, -1.4071e-02,  3.1711e-02,  5.7022e-02],\n",
       "                       [ 3.1370e-02,  6.3395e-02,  4.6080e-02,  5.6186e-02,  4.6229e-02],\n",
       "                       [-7.7025e-03,  6.9363e-02,  4.9700e-02,  3.6046e-02,  3.1270e-02]]],\n",
       "             \n",
       "             \n",
       "                     ...,\n",
       "             \n",
       "             \n",
       "                     [[[-1.7058e-02, -5.2795e-02, -3.4652e-02, -2.3286e-03, -7.4170e-03],\n",
       "                       [-8.9162e-03, -6.7074e-03, -3.8416e-02,  1.9568e-02, -3.8761e-02],\n",
       "                       [ 2.3069e-02,  5.2704e-02,  5.2155e-03,  4.8273e-02, -1.0729e-02],\n",
       "                       [-5.6284e-03,  3.4518e-02,  2.6500e-03,  1.4114e-02, -1.2313e-02],\n",
       "                       [-5.5497e-02, -3.9384e-02, -3.0044e-03,  1.9407e-02, -2.7139e-02]],\n",
       "             \n",
       "                      [[-4.5080e-03,  1.0727e-02, -6.3163e-02, -4.7308e-02, -2.4307e-02],\n",
       "                       [-1.9856e-02, -2.0083e-02, -1.9645e-02,  8.8463e-03, -2.0419e-04],\n",
       "                       [-2.1994e-03, -2.1192e-02,  3.2458e-02,  1.2301e-02, -3.5353e-02],\n",
       "                       [-7.6475e-03,  1.1326e-02, -7.6018e-03,  3.3649e-02,  1.9614e-02],\n",
       "                       [ 1.9621e-02,  1.3456e-02,  4.3862e-02,  2.1138e-02, -1.8012e-02]],\n",
       "             \n",
       "                      [[ 1.9028e-02, -3.6069e-02, -7.9748e-03, -5.0456e-03, -2.5015e-02],\n",
       "                       [-6.9235e-03, -4.0440e-02, -6.0963e-04,  2.5022e-03,  1.5958e-02],\n",
       "                       [ 1.0367e-02, -2.4537e-02,  3.4420e-02,  2.4649e-02, -3.5087e-02],\n",
       "                       [-3.5065e-02, -2.8464e-02,  2.4531e-02,  2.6704e-03,  4.3844e-03],\n",
       "                       [-2.9114e-02,  7.5812e-03,  2.3086e-02,  2.7288e-02, -1.4773e-03]],\n",
       "             \n",
       "                      ...,\n",
       "             \n",
       "                      [[ 1.8403e-02,  1.2903e-02, -5.6831e-03,  1.9427e-02, -3.2759e-02],\n",
       "                       [ 2.8646e-02,  1.2029e-02, -1.2850e-02,  1.4479e-02,  2.8229e-02],\n",
       "                       [-3.1469e-02,  4.4504e-03, -2.3946e-02, -3.2359e-02,  2.9300e-02],\n",
       "                       [ 3.2756e-02,  1.7864e-02,  2.3634e-02,  2.1147e-02, -2.9838e-02],\n",
       "                       [ 1.8764e-02,  3.6638e-03, -1.6790e-02, -1.4154e-02, -3.1937e-02]],\n",
       "             \n",
       "                      [[-9.4800e-03, -9.8383e-03, -2.8863e-02, -3.1875e-02, -7.3504e-03],\n",
       "                       [ 2.6102e-02,  1.4085e-02,  1.9846e-02, -6.4607e-03,  1.6943e-03],\n",
       "                       [ 2.1051e-03,  2.3550e-02, -2.1308e-02,  1.1342e-03,  2.5361e-02],\n",
       "                       [-3.8917e-02, -8.7427e-03, -3.9521e-02,  3.5069e-02,  3.2605e-02],\n",
       "                       [-2.3650e-02, -4.2749e-02, -3.5875e-02,  1.6359e-02,  2.5714e-02]],\n",
       "             \n",
       "                      [[ 3.2827e-03,  5.0575e-05, -1.4786e-02, -1.8725e-02, -1.9671e-03],\n",
       "                       [-1.2127e-02,  2.7171e-02,  3.6657e-02, -2.0267e-02,  5.7942e-03],\n",
       "                       [-9.3062e-03,  2.1211e-02,  4.7076e-02,  1.3863e-02, -6.6550e-03],\n",
       "                       [ 1.0039e-02, -3.9061e-02, -3.1022e-02,  3.7258e-02,  2.9949e-02],\n",
       "                       [ 6.4180e-03, -2.7931e-02, -3.4090e-02, -4.2421e-02,  2.7882e-02]]],\n",
       "             \n",
       "             \n",
       "                     [[[-2.2366e-02,  2.9919e-02, -8.3848e-03,  9.2272e-03, -7.5370e-03],\n",
       "                       [ 5.0282e-02,  4.7428e-02,  4.1480e-02,  4.0492e-02,  4.7914e-02],\n",
       "                       [ 6.3274e-02,  9.4855e-02,  8.0329e-02,  6.3571e-02,  1.1703e-02],\n",
       "                       [ 2.3097e-02, -1.3911e-02,  2.7665e-02,  2.9644e-02,  5.9628e-03],\n",
       "                       [-3.7817e-02, -1.6567e-02,  4.0527e-02, -2.4802e-02,  5.6563e-02]],\n",
       "             \n",
       "                      [[-3.5349e-02, -5.3724e-03, -1.1715e-02, -3.3646e-03,  5.6507e-02],\n",
       "                       [ 3.8989e-02,  6.6448e-02,  6.1516e-02,  7.5144e-02,  6.5767e-02],\n",
       "                       [ 5.3751e-03, -2.6550e-02, -4.0083e-02,  3.8167e-02, -1.2395e-02],\n",
       "                       [ 1.2117e-03,  6.6735e-02,  4.4510e-02,  2.7192e-02, -4.1008e-03],\n",
       "                       [-1.2178e-02,  1.4865e-02,  2.6927e-02, -4.2599e-03,  3.3515e-02]],\n",
       "             \n",
       "                      [[-4.4922e-02, -2.4109e-02,  3.5538e-03,  1.9545e-02, -1.0128e-02],\n",
       "                       [ 3.4883e-03, -2.2908e-03, -3.6823e-02, -2.0715e-02,  5.6279e-03],\n",
       "                       [ 3.0970e-02, -8.5952e-03,  1.0413e-02,  4.5054e-02,  2.8178e-02],\n",
       "                       [ 7.6876e-02,  7.3889e-02,  2.2496e-02,  4.3884e-02,  2.4221e-02],\n",
       "                       [-1.6576e-02,  8.0335e-03,  3.2250e-02, -1.3278e-03,  1.1933e-02]],\n",
       "             \n",
       "                      ...,\n",
       "             \n",
       "                      [[-6.8328e-03,  1.5858e-02, -1.4926e-02,  1.7709e-02, -2.4187e-02],\n",
       "                       [ 3.1522e-03,  3.5438e-02,  1.8905e-02, -1.4843e-02,  2.6914e-02],\n",
       "                       [-8.9474e-03, -3.8492e-02,  7.1632e-03,  2.2829e-03,  1.5085e-02],\n",
       "                       [-2.2545e-03,  4.0681e-03, -1.3269e-02,  1.8366e-02, -5.4624e-03],\n",
       "                       [ 1.4978e-02, -2.0123e-02,  1.0244e-02, -2.1811e-02, -2.2933e-02]],\n",
       "             \n",
       "                      [[-1.2696e-03,  1.8283e-02,  2.4881e-02, -5.6405e-03, -1.0816e-02],\n",
       "                       [ 2.1288e-02,  5.1898e-03,  7.3735e-04,  4.3487e-02,  3.3601e-02],\n",
       "                       [ 2.1421e-02, -7.5332e-03, -1.9243e-02, -2.8647e-02,  1.5879e-02],\n",
       "                       [ 2.4400e-02, -2.6828e-02,  3.3001e-02,  6.6647e-03, -2.1903e-02],\n",
       "                       [-1.3811e-02, -2.4667e-02, -1.0836e-02,  3.6637e-02,  2.4798e-02]],\n",
       "             \n",
       "                      [[-4.3620e-03, -3.4383e-03,  1.8035e-03,  2.4731e-02, -5.6520e-03],\n",
       "                       [ 3.9523e-02,  2.2889e-02,  3.9929e-02,  3.9909e-02,  5.6753e-02],\n",
       "                       [ 2.6256e-03,  5.8318e-02,  2.4715e-02,  1.1824e-02,  3.1545e-02],\n",
       "                       [-3.3405e-02,  1.7741e-02, -1.8027e-02, -9.0935e-03, -3.4300e-02],\n",
       "                       [ 1.1639e-02,  2.1164e-02,  1.0904e-02, -3.8901e-03, -2.0859e-02]]],\n",
       "             \n",
       "             \n",
       "                     [[[ 2.5332e-02,  8.0130e-02,  9.4993e-02,  1.0870e-01,  4.4522e-02],\n",
       "                       [ 2.6898e-02,  7.3872e-02,  6.2344e-02,  6.3539e-02,  6.4537e-02],\n",
       "                       [ 2.8654e-02,  2.9506e-02,  6.0525e-02,  4.5299e-02,  4.7288e-02],\n",
       "                       [ 1.3750e-02,  2.5594e-02,  2.3160e-02,  5.9934e-02, -1.0789e-02],\n",
       "                       [ 8.5313e-02,  1.2587e-02,  1.3390e-03,  5.3953e-03,  3.4275e-02]],\n",
       "             \n",
       "                      [[ 5.8431e-02,  6.5170e-02,  2.9313e-02,  2.4955e-02,  1.0869e-02],\n",
       "                       [ 1.3354e-02,  4.6082e-02,  7.9081e-02,  9.6011e-02,  4.7767e-02],\n",
       "                       [ 4.6133e-02,  2.0277e-03,  5.5257e-02,  8.1858e-02,  6.2910e-03],\n",
       "                       [ 2.5126e-02,  3.9356e-02,  4.9666e-02,  1.4565e-02,  1.9015e-02],\n",
       "                       [ 8.7123e-03,  2.8684e-02,  6.5328e-02,  6.3735e-03,  6.9387e-03]],\n",
       "             \n",
       "                      [[ 3.3657e-02, -4.3515e-03,  2.3293e-02,  4.6264e-02,  4.7670e-02],\n",
       "                       [ 5.6113e-02,  3.1452e-02,  6.8932e-03,  6.5525e-02,  4.1323e-02],\n",
       "                       [-6.0547e-04,  2.3368e-02, -7.7998e-03,  2.8468e-02,  3.9617e-02],\n",
       "                       [-1.1232e-02,  1.3823e-04,  1.2489e-02,  3.7703e-02,  5.2310e-02],\n",
       "                       [ 4.0039e-02,  3.8251e-03,  3.5750e-02, -1.2510e-02,  6.0444e-03]],\n",
       "             \n",
       "                      ...,\n",
       "             \n",
       "                      [[ 1.6183e-02, -1.3638e-02,  3.3930e-02,  3.1793e-02,  2.7209e-02],\n",
       "                       [-2.2973e-02, -1.3569e-02,  1.5023e-02,  2.6491e-02,  9.1223e-03],\n",
       "                       [ 3.5701e-02,  1.3964e-03,  1.1409e-02,  1.6065e-02, -1.3533e-02],\n",
       "                       [-4.3766e-03, -2.4820e-02, -2.6851e-02,  5.4070e-03,  2.6896e-03],\n",
       "                       [ 2.5636e-02, -9.6206e-04, -1.9188e-02,  2.6695e-02,  1.8975e-02]],\n",
       "             \n",
       "                      [[ 2.8120e-02,  4.4768e-02,  4.4436e-02,  1.5505e-02,  1.1827e-02],\n",
       "                       [ 3.9214e-02, -9.9129e-03,  1.5771e-02, -9.2354e-04,  5.9653e-03],\n",
       "                       [ 2.0886e-02, -1.0248e-03,  3.2420e-02, -5.8766e-03,  3.1024e-02],\n",
       "                       [ 2.8268e-02,  3.7177e-02, -1.5416e-02,  1.2821e-02,  3.1573e-02],\n",
       "                       [ 2.3242e-02, -7.3273e-03,  2.5613e-02,  4.7608e-02,  7.9237e-04]],\n",
       "             \n",
       "                      [[ 4.9121e-02,  1.7367e-02,  3.7813e-02,  4.2962e-02,  3.9087e-02],\n",
       "                       [ 1.0069e-02,  5.8485e-03,  3.0465e-02,  2.6392e-02,  2.0558e-02],\n",
       "                       [-6.4752e-03,  7.9696e-03,  5.1215e-02,  4.1546e-02, -2.1053e-02],\n",
       "                       [-2.2668e-03, -1.3547e-02,  3.0162e-02, -3.2775e-03, -2.7632e-02],\n",
       "                       [ 3.5585e-02,  2.5011e-02, -1.7435e-02, -3.6892e-02,  2.7566e-02]]]],\n",
       "                    requires_grad=True): {'old_init': tensor([[[[-6.4765e-03,  7.3803e-03,  2.5480e-02,  2.4371e-02, -3.2968e-03],\n",
       "                        [ 1.6886e-02, -1.5386e-02, -2.3004e-02,  3.6043e-02, -2.4326e-02],\n",
       "                        [-1.5699e-02,  3.2207e-02,  2.1078e-02, -2.2518e-02,  1.4364e-02],\n",
       "                        [-2.1528e-02,  1.6049e-02,  2.7127e-02, -1.0879e-02,  7.4092e-04],\n",
       "                        [ 3.5284e-02,  1.4860e-02,  1.0671e-02, -8.9182e-03,  2.9019e-02]],\n",
       "              \n",
       "                       [[-1.5163e-02,  2.1042e-02,  2.1625e-02, -8.6181e-03,  2.1281e-02],\n",
       "                        [-2.0626e-02, -1.6276e-02,  2.6069e-02,  2.1782e-02, -1.0269e-02],\n",
       "                        [-2.3562e-02, -1.8923e-02, -1.1663e-02, -2.3557e-02,  1.7797e-02],\n",
       "                        [-1.0999e-02, -1.7866e-02, -3.6086e-03,  2.8493e-02,  3.2828e-02],\n",
       "                        [ 2.7833e-02, -2.3068e-02, -2.2699e-02,  1.9343e-02, -6.7620e-03]],\n",
       "              \n",
       "                       [[ 8.7043e-03,  2.8378e-02,  1.5344e-03,  9.3049e-03, -1.5086e-02],\n",
       "                        [ 2.4666e-02, -1.6046e-02,  1.7028e-02,  4.4643e-03,  2.9780e-02],\n",
       "                        [ 2.8843e-02, -1.8926e-02,  2.6302e-02,  2.2758e-02, -6.4968e-03],\n",
       "                        [-1.0843e-02, -5.7582e-04, -9.3330e-03,  3.8648e-03, -3.6924e-03],\n",
       "                        [ 1.6116e-02, -2.1310e-02,  7.4524e-03,  7.5554e-03,  1.0581e-04]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-2.6336e-02, -1.3085e-02, -1.9149e-02,  2.6332e-02,  6.2570e-03],\n",
       "                        [-1.9044e-02, -2.0538e-02,  3.3376e-02,  1.4146e-02, -2.9431e-02],\n",
       "                        [-7.0705e-03,  1.5687e-03, -1.8393e-02, -1.3124e-02,  3.3533e-02],\n",
       "                        [ 1.7270e-02,  3.1502e-02,  2.8758e-02, -3.3917e-02,  7.7323e-03],\n",
       "                        [-2.2208e-02, -8.9644e-03, -1.1637e-03, -2.5621e-02, -7.4421e-03]],\n",
       "              \n",
       "                       [[ 2.9049e-02, -2.0815e-02,  3.2771e-02, -2.0675e-02, -2.5956e-02],\n",
       "                        [-2.0361e-03, -1.0301e-02,  3.3802e-02, -1.7982e-02,  1.8264e-02],\n",
       "                        [-3.2010e-02, -3.2081e-02,  8.9745e-03, -6.1530e-03, -2.1910e-02],\n",
       "                        [-2.1739e-02,  7.6884e-03, -9.5918e-03,  1.4216e-02,  5.8183e-03],\n",
       "                        [ 3.0430e-02,  7.7848e-03, -2.3972e-02, -4.7580e-03,  3.0861e-02]],\n",
       "              \n",
       "                       [[-2.5172e-02, -1.2841e-02, -1.3402e-04, -7.0567e-03,  3.0288e-02],\n",
       "                        [-2.1337e-02, -1.1768e-02,  1.2127e-02, -3.1116e-02, -4.0152e-03],\n",
       "                        [ 2.9474e-02,  1.2945e-02,  3.1943e-02,  1.9314e-02, -3.4510e-03],\n",
       "                        [ 4.7502e-03, -2.7253e-02,  4.9221e-03,  1.8963e-02, -1.0191e-02],\n",
       "                        [-3.1305e-02,  3.3076e-02, -1.7243e-02, -1.8134e-02,  1.2656e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 9.7386e-03,  3.1115e-02,  1.4883e-03, -2.2214e-02,  2.6598e-02],\n",
       "                        [-2.5231e-03,  4.5168e-03,  1.2560e-02, -9.5379e-04,  2.5871e-02],\n",
       "                        [ 1.6114e-02,  2.3466e-02,  2.6132e-02,  1.6679e-02,  9.7405e-03],\n",
       "                        [ 5.9619e-03,  2.5204e-02,  1.4597e-03, -7.0344e-03, -1.2587e-02],\n",
       "                        [ 1.4554e-02, -5.9191e-03,  2.8579e-03,  3.9118e-03, -2.7873e-02]],\n",
       "              \n",
       "                       [[-2.1218e-02,  5.5684e-03, -2.5642e-02, -2.0813e-02,  5.2744e-03],\n",
       "                        [ 1.6015e-02, -2.0167e-02,  2.7381e-02,  5.7627e-03, -1.5412e-02],\n",
       "                        [ 6.0552e-03,  3.3538e-02, -2.4540e-02,  3.6778e-02,  3.0433e-03],\n",
       "                        [-2.8597e-03, -1.1054e-02, -1.2775e-02,  8.8965e-03,  6.9100e-04],\n",
       "                        [ 5.3940e-03,  1.8982e-02, -2.8642e-02,  3.1442e-02,  1.9357e-02]],\n",
       "              \n",
       "                       [[ 1.0777e-02, -2.9332e-02, -7.3607e-03, -1.9722e-02,  1.1593e-02],\n",
       "                        [ 3.4165e-02,  1.4648e-02, -5.1428e-03, -9.7038e-05,  2.2668e-02],\n",
       "                        [ 3.0157e-02, -2.9571e-02,  3.0819e-02, -3.1313e-02,  1.8136e-02],\n",
       "                        [-1.0003e-02, -2.7143e-02,  7.3819e-06, -9.1230e-03,  2.6130e-02],\n",
       "                        [-2.1632e-02, -5.7312e-03, -1.2095e-02, -2.4602e-02,  5.8964e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.8896e-02,  1.8305e-02, -1.1762e-03, -7.6604e-03, -2.4187e-02],\n",
       "                        [ 2.5074e-02, -2.6885e-02, -2.8741e-02,  2.7975e-02, -7.5754e-03],\n",
       "                        [ 4.3817e-03, -3.5380e-03,  7.2200e-03, -9.8682e-04,  2.1347e-02],\n",
       "                        [ 3.2435e-02,  7.0914e-03,  1.8297e-02, -1.9033e-02, -2.4947e-02],\n",
       "                        [ 8.5207e-03, -2.6580e-02,  1.7665e-02, -1.3616e-02, -2.9976e-02]],\n",
       "              \n",
       "                       [[ 3.1580e-02, -9.2679e-03,  7.9612e-03,  2.5430e-02,  3.5598e-02],\n",
       "                        [-2.2315e-02,  5.3176e-03,  2.7889e-02,  2.7077e-02,  7.4150e-04],\n",
       "                        [ 1.7412e-02,  2.4214e-02, -1.2957e-02, -2.5158e-02,  1.4821e-03],\n",
       "                        [-1.7061e-02,  1.1298e-02, -2.3807e-02, -2.6163e-02,  1.4597e-02],\n",
       "                        [-1.5649e-02,  1.1737e-02,  1.3301e-02, -8.5664e-03, -1.6492e-02]],\n",
       "              \n",
       "                       [[ 7.1651e-03, -1.2860e-02,  4.2338e-03,  1.4608e-02,  8.8372e-03],\n",
       "                        [-3.0759e-02,  3.2527e-02, -1.3637e-02, -2.9552e-02, -1.9300e-02],\n",
       "                        [-2.6451e-02, -1.1982e-02,  9.6229e-03,  2.2007e-02,  2.8948e-02],\n",
       "                        [-1.3134e-02, -1.7381e-02, -1.2446e-02, -6.5080e-03,  2.3202e-02],\n",
       "                        [-4.0004e-03, -2.1153e-02, -2.9906e-02,  1.9116e-03,  1.2525e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.5912e-02,  2.5644e-02,  1.6845e-02,  3.2361e-02, -8.0392e-04],\n",
       "                        [-2.0404e-02,  3.5999e-02, -5.0943e-04, -2.1996e-02, -1.8924e-03],\n",
       "                        [-5.0052e-03, -2.8744e-02,  6.6564e-03, -1.7576e-02, -1.1915e-02],\n",
       "                        [-3.0298e-03, -2.0843e-02, -2.1406e-02,  1.8311e-02, -2.5649e-02],\n",
       "                        [-1.9538e-02,  1.6992e-02, -1.3277e-02, -4.4005e-03,  2.8155e-02]],\n",
       "              \n",
       "                       [[ 2.4967e-02, -7.4592e-03,  1.7214e-02, -9.4680e-03, -3.3234e-02],\n",
       "                        [-6.2667e-03, -2.2482e-02,  1.0467e-03,  4.0862e-03,  8.8141e-03],\n",
       "                        [-2.9593e-02,  1.8445e-02, -2.3995e-02,  2.7193e-02, -2.3845e-02],\n",
       "                        [ 1.6483e-03,  2.2617e-02,  2.2852e-02,  4.9465e-03,  2.5905e-02],\n",
       "                        [ 1.1215e-02,  3.2864e-02, -1.7415e-02, -2.9119e-02, -2.4314e-02]],\n",
       "              \n",
       "                       [[-4.8192e-03, -1.0755e-02, -1.8126e-02,  1.2408e-02, -2.6602e-02],\n",
       "                        [ 3.6243e-02, -1.7791e-02,  1.5483e-02,  1.4506e-02, -3.1753e-02],\n",
       "                        [-2.8108e-02,  5.5812e-03,  2.5248e-03,  7.7505e-03, -7.7679e-03],\n",
       "                        [-3.4085e-02, -2.2088e-02, -6.0926e-03, -3.1214e-02, -1.6638e-02],\n",
       "                        [-3.0399e-02,  3.5309e-02,  1.1770e-02, -1.6084e-02, -2.2689e-02]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-3.3697e-02, -1.5514e-02, -2.6044e-02, -1.8893e-02, -1.7718e-02],\n",
       "                        [ 7.9850e-04, -1.7364e-02,  1.0010e-02, -2.9764e-02,  1.5068e-03],\n",
       "                        [ 1.7697e-02,  4.1919e-03, -5.7509e-03,  8.0726e-03,  2.1880e-02],\n",
       "                        [-7.9957e-03,  3.3958e-02, -4.6956e-03, -5.7286e-03,  2.3461e-02],\n",
       "                        [-3.1616e-02, -6.1991e-04, -2.1386e-02, -7.0456e-03,  2.0383e-03]],\n",
       "              \n",
       "                       [[ 7.6357e-03, -3.1610e-02,  1.4652e-02, -2.8345e-02, -3.4317e-02],\n",
       "                        [ 2.5474e-02,  2.4856e-02, -3.3664e-02, -1.1766e-02,  1.7557e-02],\n",
       "                        [-2.3341e-02,  1.1364e-02, -3.2827e-02, -1.1180e-02,  4.5724e-03],\n",
       "                        [-2.1247e-02,  3.5715e-02,  3.3791e-02, -6.9703e-03, -4.6835e-03],\n",
       "                        [ 2.5025e-03, -8.0132e-03, -3.0140e-02,  3.2396e-02,  2.8228e-02]],\n",
       "              \n",
       "                       [[-3.1015e-02,  8.6954e-04,  3.0233e-02,  1.2910e-02,  3.0693e-02],\n",
       "                        [ 2.5673e-02,  2.1395e-02, -3.2082e-02, -3.0025e-02,  2.2918e-02],\n",
       "                        [ 2.9168e-02,  2.6770e-03, -3.1049e-02,  2.8896e-03,  3.1129e-02],\n",
       "                        [ 1.4136e-02,  3.1871e-02,  1.7151e-02,  2.7551e-02,  4.3400e-03],\n",
       "                        [-2.5131e-02,  3.3901e-02,  1.8218e-02,  1.5048e-02,  1.0890e-02]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 1.4716e-02, -2.2143e-02, -3.6502e-03,  3.0318e-02,  2.6728e-02],\n",
       "                        [-1.1427e-02,  3.9544e-04, -1.4263e-02,  3.0105e-02, -2.7696e-02],\n",
       "                        [ 3.0435e-03,  1.3124e-02, -9.2074e-03,  3.5127e-02, -2.5197e-02],\n",
       "                        [ 1.9656e-02,  2.5296e-02, -1.6595e-02, -2.2175e-02, -1.7066e-02],\n",
       "                        [-1.7627e-02, -1.2471e-02,  9.9230e-03,  1.6380e-02, -2.9572e-02]],\n",
       "              \n",
       "                       [[ 1.0276e-03,  1.9763e-02, -3.2049e-02, -8.8715e-03, -9.8684e-04],\n",
       "                        [-1.7739e-02, -3.1129e-02, -1.9319e-02,  2.8255e-02,  6.2310e-03],\n",
       "                        [ 1.6226e-02,  3.0233e-03,  3.2545e-02,  9.7953e-03, -3.2409e-02],\n",
       "                        [-1.7988e-02,  5.8788e-03, -1.3417e-02,  3.3443e-02,  1.9630e-02],\n",
       "                        [ 2.8107e-02,  1.7346e-02,  3.2670e-02,  6.7683e-03, -2.6963e-02]],\n",
       "              \n",
       "                       [[ 1.5295e-02, -2.8931e-02, -3.6818e-04,  9.0291e-03, -4.9684e-03],\n",
       "                        [-1.0127e-03, -2.1928e-02,  1.5418e-02,  1.8065e-02,  3.1323e-02],\n",
       "                        [ 1.3859e-02, -2.9815e-02,  3.3795e-02,  2.5123e-02, -3.3395e-02],\n",
       "                        [-2.1410e-02, -2.9726e-02,  8.1874e-03, -2.4161e-02, -7.1336e-03],\n",
       "                        [-7.4539e-03,  1.1745e-02,  2.0077e-02,  1.8796e-02, -3.3333e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 1.7897e-02,  9.2699e-03, -5.8838e-03,  1.4298e-02, -3.3184e-02],\n",
       "                        [ 3.2171e-02,  1.5883e-02, -1.4480e-02,  7.3942e-03,  2.3492e-02],\n",
       "                        [-3.2962e-02,  8.5822e-03, -2.2200e-02, -3.0359e-02,  3.5307e-02],\n",
       "                        [ 3.0021e-02,  2.0275e-02,  2.5176e-02,  3.2214e-02, -2.4345e-02],\n",
       "                        [ 2.2023e-02,  6.1325e-03, -1.8183e-02, -1.0806e-02, -2.6265e-02]],\n",
       "              \n",
       "                       [[-8.9088e-03, -1.1938e-02, -2.3920e-02, -2.0218e-02,  3.3494e-03],\n",
       "                        [ 2.1808e-02,  2.9903e-03,  1.2929e-02, -7.1921e-03, -8.6155e-04],\n",
       "                        [ 7.3093e-03,  3.3507e-02, -2.0559e-02, -7.6818e-03,  2.1437e-02],\n",
       "                        [-2.7079e-02,  1.2166e-02, -2.6912e-02,  3.3846e-02,  3.2919e-02],\n",
       "                        [-2.0459e-02, -3.2542e-02, -1.9270e-02,  2.5683e-02,  3.2329e-02]],\n",
       "              \n",
       "                       [[ 2.2814e-02,  2.4612e-02,  4.3857e-03, -6.8587e-03,  9.8731e-03],\n",
       "                        [-2.8499e-03,  3.1085e-02,  3.2933e-02, -1.7173e-02,  1.0082e-02],\n",
       "                        [-7.8298e-03,  8.1880e-03,  2.7524e-02, -1.2697e-03, -1.2738e-02],\n",
       "                        [ 1.7330e-02, -3.1127e-02, -3.3328e-02,  2.9830e-02,  2.8117e-02],\n",
       "                        [ 1.0975e-02,  4.3098e-03, -9.5597e-03, -3.1703e-02,  2.6570e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.0247e-02,  2.4433e-02, -3.1254e-02, -2.5882e-04, -2.0991e-02],\n",
       "                        [ 2.7699e-02,  3.0406e-02,  6.3280e-03, -1.0705e-02, -8.0694e-03],\n",
       "                        [ 1.0974e-02,  2.8271e-02,  3.4289e-02,  2.9054e-02, -1.5746e-02],\n",
       "                        [ 7.6998e-03, -6.9666e-03,  3.3261e-02,  3.6444e-02,  1.6827e-02],\n",
       "                        [-2.5832e-02, -2.5433e-02,  2.7657e-02, -1.8335e-02,  3.6226e-02]],\n",
       "              \n",
       "                       [[-2.2847e-02,  2.4851e-03, -1.6967e-03, -2.2638e-02,  1.9089e-02],\n",
       "                        [-4.9100e-03,  9.5264e-04, -7.5413e-03,  1.9800e-02,  1.9482e-02],\n",
       "                        [-3.2956e-03, -1.0052e-03, -1.7342e-02,  2.8279e-02, -2.5974e-02],\n",
       "                        [-2.3040e-02,  2.3144e-02, -1.5051e-02, -6.7854e-03, -3.0288e-02],\n",
       "                        [-3.0375e-02, -4.7977e-03,  8.2173e-03, -2.8436e-02,  1.5528e-02]],\n",
       "              \n",
       "                       [[-2.8951e-02, -2.2104e-02, -1.2484e-02,  1.3150e-03, -2.6106e-02],\n",
       "                        [-3.8694e-03, -7.3457e-03, -3.3010e-02, -2.2188e-02, -1.9457e-03],\n",
       "                        [ 1.2922e-02, -2.9930e-02, -2.4929e-02,  5.0370e-03, -3.4934e-03],\n",
       "                        [ 3.1209e-02,  3.3459e-02, -2.5886e-03,  2.4277e-02,  1.1975e-02],\n",
       "                        [-1.7441e-02,  8.5301e-03,  3.2265e-02, -2.6756e-04,  7.6204e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[-1.6734e-02,  2.7943e-03, -2.1488e-02,  7.5067e-03, -2.8257e-02],\n",
       "                        [-2.0154e-03,  3.1972e-02,  1.7628e-02, -1.1881e-02,  3.0670e-02],\n",
       "                        [-1.0915e-02, -3.4270e-02,  4.1807e-03,  4.0919e-03,  1.7673e-02],\n",
       "                        [ 2.2012e-03,  3.5177e-03, -1.2619e-02,  1.8139e-02, -7.5688e-04],\n",
       "                        [ 1.0546e-02, -2.1666e-02,  1.0210e-02, -3.4645e-02, -2.4606e-02]],\n",
       "              \n",
       "                       [[-7.9341e-03,  1.6419e-02,  3.2406e-02, -5.2596e-03, -1.9960e-02],\n",
       "                        [ 1.9685e-02,  3.5113e-05, -4.7014e-03,  3.2646e-02,  1.5750e-02],\n",
       "                        [ 2.6760e-02, -5.1776e-03, -1.2283e-02, -1.6588e-02,  2.4779e-02],\n",
       "                        [ 2.3985e-02, -2.8311e-02,  3.3890e-02,  8.9372e-03, -3.0605e-02],\n",
       "                        [-2.5061e-02, -3.2054e-02, -2.2602e-02,  1.7085e-02,  6.3267e-03]],\n",
       "              \n",
       "                       [[-9.5434e-03, -6.2564e-03, -6.5762e-03,  1.3195e-02, -2.3713e-02],\n",
       "                        [ 9.7663e-03, -6.1634e-03,  1.7146e-02,  1.6164e-02,  2.7665e-02],\n",
       "                        [-2.7604e-02,  3.1297e-02,  8.6385e-03, -1.4510e-02,  1.9065e-02],\n",
       "                        [-2.5287e-02,  3.4426e-02, -1.3923e-02, -8.6089e-03, -3.4147e-02],\n",
       "                        [ 1.7924e-02,  1.7088e-02,  5.1606e-03, -7.7061e-03, -2.7030e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-1.4038e-02,  2.1872e-02,  1.4401e-02,  3.4174e-02,  1.7075e-02],\n",
       "                        [-6.6800e-03,  2.3080e-02,  2.2958e-02,  1.8068e-02,  3.1013e-02],\n",
       "                        [ 3.7927e-03,  3.1411e-03,  1.3602e-02,  2.2425e-02,  2.3308e-02],\n",
       "                        [-2.4274e-02,  1.0510e-02, -3.9224e-03,  2.2499e-02, -2.0603e-02],\n",
       "                        [ 1.0033e-02, -1.3719e-02,  4.7322e-03, -6.3079e-03,  1.7086e-02]],\n",
       "              \n",
       "                       [[ 1.9785e-02,  3.3728e-02,  1.4909e-03,  1.9525e-02,  4.8421e-03],\n",
       "                        [-1.2396e-02,  1.2213e-02,  1.5297e-02,  3.2702e-02,  2.0292e-02],\n",
       "                        [-6.4176e-04, -3.1666e-02,  6.1417e-03,  2.7249e-02, -1.9256e-02],\n",
       "                        [-1.0917e-02, -1.4140e-03,  1.8151e-02, -5.9190e-03,  8.6410e-03],\n",
       "                        [-1.0431e-02, -2.1148e-02,  1.7659e-02, -1.4099e-02,  1.5785e-02]],\n",
       "              \n",
       "                       [[ 1.7286e-02, -2.1858e-02,  1.3856e-03,  2.5440e-02,  3.0624e-02],\n",
       "                        [ 2.7891e-02, -9.2770e-03, -2.4855e-02,  3.3810e-02,  1.1739e-02],\n",
       "                        [-2.1978e-02,  1.8993e-03, -3.3958e-02,  1.3043e-03,  2.8383e-02],\n",
       "                        [-3.3752e-02, -1.6549e-02, -1.8692e-02,  5.8130e-03,  3.2414e-02],\n",
       "                        [-4.9668e-03, -3.2946e-02,  2.2190e-02, -1.8773e-02, -5.7675e-03]],\n",
       "              \n",
       "                       ...,\n",
       "              \n",
       "                       [[ 3.7133e-03, -2.5594e-02,  3.2101e-02,  3.0481e-02,  2.4964e-02],\n",
       "                        [-3.3496e-02, -1.6708e-02,  9.7357e-03,  2.6209e-02,  1.0731e-02],\n",
       "                        [ 2.8822e-02, -3.8109e-03,  1.0688e-02,  1.6347e-02, -1.2344e-02],\n",
       "                        [-5.7336e-03, -3.1138e-02, -3.4224e-02,  2.2714e-03,  4.3142e-03],\n",
       "                        [ 2.5517e-02,  5.8749e-03, -2.4409e-02,  2.1977e-02,  1.2157e-02]],\n",
       "              \n",
       "                       [[ 2.0194e-02,  3.3823e-02,  3.4043e-02, -5.2277e-03, -2.7872e-02],\n",
       "                        [ 3.4212e-02, -1.2996e-02,  2.3665e-02, -5.4606e-03, -5.6746e-03],\n",
       "                        [ 9.8889e-03, -1.7201e-05,  3.3181e-02, -5.6078e-03,  1.2751e-02],\n",
       "                        [ 1.2507e-02,  1.9545e-02, -1.8658e-02,  1.1137e-02,  2.0438e-02],\n",
       "                        [ 1.7684e-02, -2.4177e-02,  4.1943e-03,  3.2646e-02, -6.7360e-03]],\n",
       "              \n",
       "                       [[ 2.5854e-02, -1.9834e-03,  5.1506e-03,  1.0732e-03,  2.1432e-04],\n",
       "                        [-1.4423e-02, -1.9094e-02,  2.0224e-02,  3.2925e-03, -7.1479e-03],\n",
       "                        [-2.2512e-02, -1.0585e-03,  3.1557e-02,  3.2594e-02, -3.1875e-02],\n",
       "                        [-2.4404e-02, -3.0969e-02,  3.0404e-02, -5.5568e-03, -3.3785e-02],\n",
       "                        [ 1.5672e-02, -3.7798e-03, -3.1876e-02, -2.8159e-02,  2.7748e-02]]]])},\n",
       "             Parameter containing:\n",
       "             tensor([-0.0086, -0.0050,  0.0671,  0.0075, -0.0331, -0.0085, -0.0060,  0.0211,\n",
       "                      0.0524, -0.0165,  0.0098,  0.0172,  0.0577,  0.0368, -0.0037,  0.0301,\n",
       "                     -0.0512, -0.0067,  0.0054,  0.0366,  0.0627, -0.0030,  0.0747,  0.0203,\n",
       "                     -0.0367, -0.0036,  0.0029, -0.0175, -0.0325,  0.0059, -0.0207,  0.0118,\n",
       "                      0.0087, -0.0414,  0.0089, -0.0358, -0.0226, -0.0476, -0.0461,  0.0544,\n",
       "                      0.0262,  0.0791, -0.0087,  0.1035, -0.0034,  0.0345,  0.0177,  0.0893,\n",
       "                      0.0303, -0.0075, -0.0356, -0.0125, -0.0052, -0.0408,  0.0493,  0.0371,\n",
       "                      0.0066, -0.0313,  0.0316, -0.0299, -0.0064,  0.0071,  0.0341,  0.0145],\n",
       "                    requires_grad=True): {'old_init': tensor([-1.7455e-02,  1.7160e-02, -1.7602e-02, -1.8584e-02, -1.0697e-02,\n",
       "                       3.6759e-02,  1.6384e-02,  1.1206e-02,  2.0863e-02,  2.8052e-03,\n",
       "                       2.0454e-02,  1.9863e-02,  2.5893e-02,  2.7057e-02, -1.6745e-02,\n",
       "                       3.6302e-02, -2.3533e-02, -4.7361e-03, -2.8667e-02, -5.4040e-03,\n",
       "                       2.8941e-02, -1.2287e-03, -1.3834e-02,  2.1900e-02, -2.2312e-02,\n",
       "                      -1.2252e-02,  2.1842e-02, -3.1324e-02, -1.1247e-02,  1.7694e-02,\n",
       "                       3.6313e-03,  1.5252e-02,  8.0387e-03, -3.4393e-02,  3.5438e-02,\n",
       "                       4.7281e-03, -3.3815e-02, -2.4289e-02, -3.3264e-02,  2.3605e-02,\n",
       "                       3.2010e-02, -1.1581e-03, -1.7528e-03,  3.5894e-02,  5.4654e-03,\n",
       "                      -6.5955e-03,  3.6671e-02,  3.5254e-02,  3.1197e-05, -1.7742e-02,\n",
       "                      -1.5700e-02,  9.3243e-03, -2.9625e-02, -3.2409e-02,  2.9405e-02,\n",
       "                       1.4401e-02,  1.1839e-04, -1.7342e-02,  3.6220e-02,  1.7631e-03,\n",
       "                      -9.6436e-03,  1.7688e-02,  1.3177e-02, -2.6239e-02])},\n",
       "             Parameter containing:\n",
       "             tensor([[ 0.0142, -0.0080,  0.0280,  ...,  0.0124, -0.0010,  0.0051],\n",
       "                     [ 0.0230,  0.0118,  0.0018,  ..., -0.0264,  0.0098, -0.0267],\n",
       "                     [ 0.0042, -0.0228, -0.0268,  ...,  0.0050,  0.0061, -0.0181],\n",
       "                     ...,\n",
       "                     [ 0.0271,  0.0123,  0.0531,  ...,  0.0073,  0.0659,  0.0254],\n",
       "                     [ 0.0038,  0.0177, -0.0115,  ..., -0.0097, -0.0264, -0.0060],\n",
       "                     [ 0.0036, -0.0089,  0.0155,  ...,  0.0073,  0.0039,  0.0016]],\n",
       "                    requires_grad=True): {'old_init': tensor([[ 0.0142, -0.0080,  0.0280,  ...,  0.0124, -0.0010,  0.0051],\n",
       "                      [ 0.0230,  0.0118,  0.0018,  ..., -0.0264,  0.0098, -0.0267],\n",
       "                      [ 0.0042, -0.0228, -0.0268,  ...,  0.0050,  0.0061, -0.0181],\n",
       "                      ...,\n",
       "                      [ 0.0123, -0.0099,  0.0153,  ..., -0.0284,  0.0308,  0.0064],\n",
       "                      [ 0.0038,  0.0177, -0.0115,  ..., -0.0097, -0.0264, -0.0060],\n",
       "                      [-0.0095, -0.0286, -0.0180,  ..., -0.0245, -0.0273, -0.0153]])},\n",
       "             Parameter containing:\n",
       "             tensor([-1.5014e-02,  2.7167e-02, -1.8762e-02,  5.6500e-03,  3.5492e-02,\n",
       "                     -2.5427e-02, -1.9837e-02,  5.9704e-04,  1.5771e-02, -1.0484e-02,\n",
       "                     -2.7753e-02,  2.8411e-02, -1.8881e-02, -1.2449e-02, -2.8969e-02,\n",
       "                      2.2648e-02, -3.2879e-02, -3.8309e-02,  3.0655e-02,  3.2975e-02,\n",
       "                      2.0601e-02,  2.6068e-02, -1.4796e-02,  2.8815e-02, -1.4164e-02,\n",
       "                     -1.2534e-02,  2.5051e-02, -9.8884e-03, -5.5921e-02,  1.2249e-02,\n",
       "                     -4.5575e-03, -1.1136e-02, -2.0198e-03, -2.7706e-02,  2.2427e-03,\n",
       "                     -2.8352e-02, -2.2383e-02, -2.2084e-02,  2.0964e-02, -2.7535e-02,\n",
       "                      8.2531e-04,  2.9925e-02, -5.4893e-03, -7.8679e-03, -1.9892e-02,\n",
       "                      4.5041e-02, -5.9190e-02, -2.3236e-02,  1.2650e-02, -3.0488e-02,\n",
       "                      1.6560e-02,  7.3937e-03,  3.0317e-02,  5.1569e-02,  9.8904e-03,\n",
       "                      4.2360e-03, -4.3738e-03,  5.4619e-03, -1.2048e-02, -1.2202e-02,\n",
       "                      1.9142e-02,  3.1533e-02, -4.4089e-03, -1.2862e-03,  1.9433e-02,\n",
       "                     -3.2537e-02,  2.4227e-02, -4.2608e-02, -5.6595e-03, -1.0659e-02,\n",
       "                      3.6608e-02,  3.9588e-02,  1.1275e-02,  4.2755e-04, -3.1279e-02,\n",
       "                     -2.0162e-02,  9.6216e-03,  3.3339e-03,  2.8276e-02,  3.2140e-02,\n",
       "                     -2.7536e-02,  2.7046e-03,  8.9967e-03, -2.4494e-02,  5.1284e-02,\n",
       "                      4.3938e-02, -2.6627e-02,  2.4252e-03,  3.7267e-02,  3.5251e-03,\n",
       "                      3.4622e-03, -4.1719e-02,  9.6295e-03, -2.0162e-02,  1.6560e-02,\n",
       "                      3.4536e-03,  2.0336e-02,  8.0655e-03, -1.3707e-02,  1.9437e-02,\n",
       "                     -3.1173e-02, -2.9105e-03,  9.8901e-03, -1.4592e-03, -2.1270e-02,\n",
       "                     -2.5483e-02,  4.7283e-02,  7.9949e-03, -4.4258e-02,  2.2646e-02,\n",
       "                     -1.5364e-05, -2.9589e-02,  2.5826e-02,  4.6207e-03,  7.5396e-03,\n",
       "                     -1.9409e-02,  1.0388e-02,  2.3470e-02,  8.2717e-03, -2.4235e-02,\n",
       "                      4.1296e-04, -2.4931e-02, -2.4303e-02,  3.4489e-03,  9.2502e-03,\n",
       "                      2.9593e-02, -3.3982e-02,  5.4559e-02, -1.0745e-02, -6.2671e-03,\n",
       "                     -3.3826e-03,  7.0952e-04, -1.2624e-02, -2.1085e-02, -1.8246e-02,\n",
       "                     -2.1612e-03, -4.6226e-02,  3.8946e-03,  2.0734e-02, -2.6878e-02,\n",
       "                      5.8546e-02, -1.6473e-02,  2.7820e-02, -1.4705e-02, -2.3284e-02,\n",
       "                      3.2901e-02,  5.6613e-03, -1.7120e-03,  2.7828e-02,  5.6509e-03,\n",
       "                      1.0548e-03, -2.5234e-02,  3.0478e-03,  2.3420e-02,  4.9480e-03,\n",
       "                      9.3008e-03,  2.9692e-02, -7.8338e-03, -3.0175e-02,  2.4516e-02,\n",
       "                     -4.5736e-03,  1.0236e-03,  2.3455e-02,  2.3512e-02, -5.8713e-02,\n",
       "                     -5.8948e-05, -8.0037e-03, -2.9973e-02, -4.0582e-02, -2.1649e-02,\n",
       "                     -5.2034e-03,  1.6393e-02,  3.8271e-02, -1.0325e-02, -8.9941e-04,\n",
       "                      3.7573e-02, -1.4876e-03,  7.4907e-03, -5.6352e-02,  1.2375e-02,\n",
       "                     -2.5127e-02,  1.6934e-02,  1.2279e-02,  1.4983e-02, -1.0046e-02,\n",
       "                      2.6173e-02, -2.7371e-02, -1.0122e-02,  1.3628e-02, -2.3223e-02,\n",
       "                      1.5142e-02,  3.3925e-02,  2.6067e-02,  7.2719e-04,  3.5799e-02,\n",
       "                      1.8271e-02, -4.2072e-02,  2.4237e-02, -1.4649e-02,  3.0413e-02,\n",
       "                     -1.1212e-02, -1.3550e-02,  5.3686e-03, -3.8544e-03, -2.7703e-02,\n",
       "                      1.3730e-03,  2.7088e-02,  3.3572e-02, -4.7206e-03,  3.4348e-02,\n",
       "                     -1.6543e-02, -3.5779e-02,  4.2226e-03, -1.3541e-02,  1.2995e-02,\n",
       "                     -2.5492e-03, -1.1319e-02,  6.8318e-03,  3.0808e-02, -7.1886e-03,\n",
       "                     -8.1466e-03,  2.3612e-02,  8.5978e-03, -1.4416e-02,  3.0363e-02,\n",
       "                      3.6567e-02,  3.6466e-03, -4.1062e-02, -9.7187e-03,  7.9166e-03,\n",
       "                     -5.4961e-03,  2.5511e-02,  3.1521e-03,  1.0108e-02, -2.1276e-02,\n",
       "                      1.2740e-02, -3.4038e-02,  3.1485e-02, -7.0251e-03, -1.2462e-02,\n",
       "                      1.7823e-02,  3.1059e-03,  3.0977e-02, -1.1046e-02,  1.0528e-02,\n",
       "                      1.7728e-02, -2.0551e-02, -8.9893e-03,  2.5751e-02,  9.8601e-03,\n",
       "                      2.6759e-02, -3.3643e-02,  3.3517e-03, -1.8854e-03,  1.1989e-02,\n",
       "                      8.9923e-03,  2.9821e-02,  1.2768e-02,  6.6306e-03,  9.0862e-03,\n",
       "                      1.7279e-02, -1.4506e-03,  4.5298e-03, -3.6835e-02,  1.7464e-02,\n",
       "                     -3.5694e-02,  3.0902e-03,  1.6251e-02, -2.4803e-02, -1.3117e-02,\n",
       "                      5.0269e-02,  1.4800e-02, -1.3869e-02,  8.2642e-03, -1.1383e-02,\n",
       "                     -7.1375e-03,  3.0514e-02,  3.0014e-02, -1.2239e-02,  2.0372e-02,\n",
       "                     -2.6510e-03,  4.6620e-03,  1.9598e-02,  3.7020e-02,  4.1387e-03,\n",
       "                      2.7625e-02, -4.1916e-02, -3.2156e-04, -2.3301e-02, -2.8727e-02,\n",
       "                      5.3031e-02,  1.5077e-02,  3.0326e-02, -1.1303e-02,  9.6344e-03,\n",
       "                      9.4463e-03, -1.0202e-02, -2.1429e-02,  1.7104e-02, -1.1573e-02,\n",
       "                     -1.7551e-02, -4.5423e-03,  1.2794e-02,  1.3429e-02, -1.6031e-02,\n",
       "                      3.0786e-02,  1.0059e-02,  2.9071e-02, -5.2034e-03,  1.8610e-02,\n",
       "                      2.8382e-02, -2.2175e-02, -1.2631e-02,  2.2675e-03, -1.8024e-02,\n",
       "                      3.2747e-02, -7.7838e-03, -3.6409e-02, -1.3737e-02,  1.7313e-02,\n",
       "                     -2.2260e-02,  4.0410e-03, -2.9277e-02,  2.9143e-02,  9.6328e-03,\n",
       "                      3.0377e-02,  1.0184e-02,  3.1339e-02,  2.8429e-02, -1.9923e-02,\n",
       "                     -2.6908e-02,  3.2130e-02, -3.7146e-02, -5.5803e-03,  2.8247e-02,\n",
       "                      5.8501e-02, -9.4723e-03,  4.7224e-02,  6.2875e-02, -1.1709e-02,\n",
       "                      2.6759e-02, -1.6174e-02,  1.5638e-02,  3.0513e-02, -9.6240e-03,\n",
       "                     -1.2581e-02, -1.4626e-02, -2.0443e-02, -1.5347e-02, -2.5652e-02,\n",
       "                     -4.7095e-02,  4.0725e-02, -1.9763e-02,  4.2171e-02,  1.2461e-02,\n",
       "                     -2.1856e-02,  4.8239e-02, -1.5329e-02, -1.2033e-02, -4.4894e-03,\n",
       "                      9.8144e-03,  2.7972e-02,  4.3753e-03,  1.4404e-03, -2.8469e-02,\n",
       "                      2.6880e-03,  4.4949e-02,  2.6290e-02, -5.7661e-03,  1.5920e-02,\n",
       "                      3.7575e-02,  2.7437e-02, -1.1881e-02,  8.0718e-03,  2.3094e-02,\n",
       "                     -3.1469e-03,  5.0671e-03,  1.0800e-03,  8.4571e-03, -2.5469e-02,\n",
       "                      3.0509e-02,  1.4297e-02, -1.2508e-03, -1.2880e-02, -2.3735e-02,\n",
       "                     -2.7488e-02,  2.7652e-03, -3.1078e-02,  5.0980e-02,  3.1905e-03,\n",
       "                      2.6874e-02, -3.9199e-03, -7.2583e-04,  4.6227e-03, -1.5888e-02,\n",
       "                      5.7330e-03,  9.0155e-03,  1.1233e-02,  5.0582e-02, -2.2725e-02,\n",
       "                     -1.9572e-02, -1.8308e-02, -2.0142e-02, -4.5095e-02, -1.3661e-02,\n",
       "                     -3.8651e-02, -8.1463e-03, -1.7586e-02, -2.6175e-02, -7.6208e-03,\n",
       "                     -1.3342e-02, -1.8077e-03, -1.5290e-02, -3.2309e-04,  5.3769e-03,\n",
       "                     -2.7386e-02, -1.8501e-02,  2.9320e-02,  2.9647e-02,  1.6486e-02,\n",
       "                     -2.2416e-02, -1.0107e-02, -2.7156e-02, -8.3679e-03, -3.6506e-02,\n",
       "                     -9.1898e-03, -5.5672e-03,  2.7053e-02, -1.2564e-04,  3.0508e-02,\n",
       "                      2.9714e-02, -4.8171e-03,  1.3303e-02,  3.5581e-02,  1.5961e-02,\n",
       "                      6.3135e-02, -6.0026e-04,  4.2822e-03, -2.0111e-02,  1.1368e-02,\n",
       "                     -9.1601e-03, -2.1328e-02, -1.8180e-02, -3.9077e-02,  1.5738e-02,\n",
       "                      9.7244e-03, -1.9936e-02,  3.5810e-02,  1.0389e-02,  1.1538e-02,\n",
       "                      1.0562e-02, -3.8671e-03, -4.5352e-02, -2.3273e-02,  2.4189e-02,\n",
       "                      7.8327e-03, -4.1984e-02,  1.3262e-02,  1.5655e-02,  1.3456e-02,\n",
       "                      2.1307e-02, -2.1197e-02,  2.8025e-02,  8.9569e-03, -4.7258e-03,\n",
       "                     -1.8840e-02,  6.4759e-02, -1.1695e-02,  2.5326e-02, -4.1348e-02,\n",
       "                     -3.1514e-03,  4.5137e-03, -3.0435e-03,  5.3983e-03,  1.9520e-02,\n",
       "                      2.6670e-02,  7.4768e-03, -6.7055e-03, -2.7095e-02, -9.9242e-03,\n",
       "                     -1.0577e-03,  9.9071e-03, -3.0285e-02,  1.4458e-02, -2.7577e-02,\n",
       "                     -2.2822e-02,  3.1006e-02, -2.9068e-02, -3.1995e-02,  3.8711e-02,\n",
       "                     -5.8949e-05, -2.8406e-02, -5.9847e-02,  8.9855e-03, -7.2919e-03,\n",
       "                      5.3985e-04, -6.0643e-02, -1.6122e-02, -2.1111e-02,  1.1811e-02,\n",
       "                      3.1476e-04,  2.7534e-02,  2.0474e-02,  2.3548e-02, -4.3580e-02,\n",
       "                     -3.1332e-03, -2.4816e-02,  2.5337e-02, -7.9936e-03,  2.6133e-02,\n",
       "                     -1.1696e-02,  1.1851e-02], requires_grad=True): {'old_init': tensor([-1.5014e-02,  2.7167e-02, -1.8762e-02,  5.6500e-03,  1.4499e-02,\n",
       "                      -2.5427e-02, -1.9837e-02,  2.8037e-02,  1.5771e-02,  2.5556e-02,\n",
       "                      -2.7753e-02,  1.0693e-02, -1.4378e-02, -1.2449e-02, -2.8969e-02,\n",
       "                       2.2648e-02, -1.0110e-02,  1.5648e-03,  1.8669e-02,  1.5947e-02,\n",
       "                       2.0601e-02,  2.2351e-02, -2.5566e-02,  2.8815e-02,  2.8233e-03,\n",
       "                       9.4683e-03, -5.7406e-03, -1.7145e-02, -3.0800e-02,  7.5810e-03,\n",
       "                       1.8427e-02, -9.9543e-03, -1.9393e-02, -4.4331e-03, -5.2725e-04,\n",
       "                       6.0428e-03, -2.2383e-02, -2.2084e-02,  2.0964e-02, -2.7535e-02,\n",
       "                       2.1296e-03,  2.9925e-02, -4.4046e-03, -7.8679e-03,  4.0044e-03,\n",
       "                       2.0593e-02, -2.8865e-02, -1.8988e-03, -4.8475e-03, -5.1270e-03,\n",
       "                      -1.3273e-02, -2.1193e-02,  3.0317e-02,  1.8631e-02,  9.8904e-03,\n",
       "                       8.6854e-03,  2.7088e-03,  7.1278e-03,  6.6975e-04,  6.4567e-03,\n",
       "                       1.9010e-03,  3.1533e-02, -1.2234e-02, -7.4569e-03,  1.2005e-03,\n",
       "                      -2.9698e-02,  2.4227e-02, -1.6231e-02, -3.7629e-03,  2.8782e-02,\n",
       "                       2.0791e-02,  2.9137e-03, -2.4259e-04,  2.3043e-02, -2.4406e-03,\n",
       "                      -2.0162e-02, -1.3594e-02, -2.7089e-02, -1.4817e-02, -2.4663e-03,\n",
       "                      -2.7536e-02,  9.8017e-03,  3.0601e-02, -2.4494e-02,  2.8439e-02,\n",
       "                       1.3651e-02,  1.6093e-02,  2.4252e-03,  9.7166e-03,  1.7910e-02,\n",
       "                       2.7103e-02,  7.0640e-03,  9.6295e-03, -2.0162e-02,  1.6560e-02,\n",
       "                      -2.9480e-02,  2.0336e-02,  2.0934e-02,  9.6272e-03,  1.9437e-02,\n",
       "                      -3.1173e-02,  1.0894e-02,  1.2025e-02,  2.9650e-02, -8.9165e-03,\n",
       "                      -2.5483e-02,  1.9586e-02, -2.5953e-02, -1.3280e-02,  2.2646e-02,\n",
       "                      -8.6839e-03, -8.1217e-03, -7.7251e-03,  3.0489e-02,  1.1796e-02,\n",
       "                       2.7719e-03,  1.0388e-02,  5.0912e-04,  8.2717e-03, -2.4235e-02,\n",
       "                       4.1296e-04, -2.4931e-02, -2.4303e-02, -2.4467e-02,  9.2502e-03,\n",
       "                       2.9593e-02, -2.3354e-02,  2.7224e-02, -2.8442e-02,  2.8858e-03,\n",
       "                       1.9770e-02,  7.0952e-04, -1.2624e-02, -8.9014e-03,  1.7690e-02,\n",
       "                      -2.1612e-03, -9.0146e-03,  3.8946e-03,  2.0734e-02, -2.6878e-02,\n",
       "                       1.9740e-02, -1.7267e-02,  2.7820e-02, -3.0578e-02, -2.3284e-02,\n",
       "                       1.8327e-02,  5.8799e-03,  1.3788e-02,  2.7828e-02,  5.6509e-03,\n",
       "                       2.6653e-02, -2.5234e-02,  2.2746e-02,  2.1466e-02,  1.3518e-02,\n",
       "                      -1.1779e-02,  2.9692e-02,  1.7413e-02, -3.0175e-02,  2.4516e-02,\n",
       "                      -4.5736e-03, -2.3534e-02,  3.9101e-03, -8.1206e-03, -2.4113e-02,\n",
       "                      -5.8948e-05,  8.1280e-03, -2.9973e-02,  2.6981e-03, -2.1649e-02,\n",
       "                      -5.2034e-03,  1.6393e-02,  1.7285e-02, -1.0325e-02, -8.9941e-04,\n",
       "                       1.6351e-02, -1.4876e-03,  7.4907e-03, -1.6232e-02,  1.2375e-02,\n",
       "                       1.3516e-02, -8.4792e-03,  9.9183e-04,  1.3733e-02, -1.0046e-02,\n",
       "                       2.6173e-02, -1.7727e-02, -2.5072e-02,  1.3628e-02, -2.3223e-02,\n",
       "                       1.5142e-02,  4.6196e-03,  2.6067e-02, -2.8333e-02,  2.6980e-03,\n",
       "                       1.8271e-02, -2.9107e-02,  1.7515e-02,  2.6501e-03,  2.4082e-03,\n",
       "                       1.3195e-02, -1.3550e-02,  5.3686e-03, -3.8544e-03, -2.7703e-02,\n",
       "                      -9.9638e-03,  2.9883e-02,  1.1202e-02, -4.7206e-03,  2.6638e-02,\n",
       "                       3.1548e-02, -2.7510e-02,  2.3287e-02, -1.3541e-02,  2.5258e-02,\n",
       "                       2.2571e-02, -1.1319e-02,  6.8318e-03, -1.0684e-03, -2.9729e-02,\n",
       "                      -8.1466e-03,  2.3612e-02,  5.3561e-03, -1.4416e-02,  1.4951e-02,\n",
       "                       4.8485e-03, -1.0835e-02, -3.0201e-02, -1.2508e-02,  7.9166e-03,\n",
       "                      -5.4961e-03,  2.5511e-02, -9.8319e-03,  7.3466e-03, -2.1276e-02,\n",
       "                      -2.4707e-03, -2.8731e-02,  8.9284e-03, -2.9435e-02, -6.9808e-03,\n",
       "                       3.1149e-02,  1.9256e-02,  3.0977e-02, -1.1046e-02, -2.0336e-03,\n",
       "                       1.7728e-02, -2.0551e-02, -8.9893e-03, -2.0188e-02,  9.8601e-03,\n",
       "                       1.4382e-02, -9.7616e-03,  3.3517e-03,  3.0172e-02,  1.2788e-03,\n",
       "                       8.9923e-03,  3.0905e-02, -1.1204e-02,  1.7559e-02,  9.2670e-03,\n",
       "                       1.8647e-02, -7.5381e-03,  4.5298e-03, -4.7707e-03,  1.7464e-02,\n",
       "                      -5.4314e-03,  3.0902e-03, -1.0862e-02, -6.3151e-03,  3.0907e-02,\n",
       "                       2.6289e-02, -7.4894e-03, -1.3869e-02,  8.2642e-03,  9.5242e-03,\n",
       "                      -7.1375e-03,  3.0514e-02,  3.0014e-02, -1.2239e-02,  2.0372e-02,\n",
       "                      -2.6510e-03,  2.6170e-02,  1.9598e-02,  1.5768e-02,  1.3751e-02,\n",
       "                       7.0122e-03, -1.2491e-03, -3.2156e-04,  1.7394e-02, -2.8727e-02,\n",
       "                       2.2173e-02,  1.5077e-02,  3.0326e-02, -1.1303e-02,  9.6344e-03,\n",
       "                       2.7738e-02, -1.0202e-02, -2.1429e-02, -2.4327e-02, -1.1573e-02,\n",
       "                      -1.0157e-02, -3.0016e-02,  9.6249e-03, -1.4120e-02, -1.6249e-02,\n",
       "                       2.5027e-03,  9.2210e-03,  2.9071e-02, -5.2034e-03,  1.8610e-02,\n",
       "                       4.0471e-03, -2.2175e-02, -1.2631e-02,  2.2675e-03, -1.8024e-02,\n",
       "                       1.8861e-02, -7.7838e-03, -2.0410e-02, -1.3737e-02,  1.7313e-02,\n",
       "                      -2.2260e-02,  4.0410e-03,  1.2430e-03,  2.9143e-02,  9.6328e-03,\n",
       "                       3.0377e-02,  1.0184e-02, -1.2263e-03,  2.8429e-02, -1.5753e-02,\n",
       "                      -1.8337e-02,  1.8916e-02, -1.8174e-02, -5.5803e-03, -6.2378e-03,\n",
       "                       2.3051e-02, -9.4723e-03,  1.0870e-02,  2.6438e-02, -1.1709e-02,\n",
       "                       2.6759e-02, -3.2321e-03, -2.1983e-02, -9.8302e-03, -9.6240e-03,\n",
       "                      -1.2581e-02, -1.4626e-02, -2.0443e-02,  2.7917e-02, -2.0975e-02,\n",
       "                      -1.8253e-02,  2.5667e-02, -7.4054e-03,  5.8630e-03,  1.2461e-02,\n",
       "                      -2.4177e-02,  6.8143e-04, -3.0441e-02, -1.2033e-02, -2.6718e-04,\n",
       "                      -5.7481e-03,  4.8646e-03,  4.3753e-03,  1.4404e-03, -1.3721e-02,\n",
       "                       6.3026e-03,  2.6411e-02,  3.3430e-03,  2.3780e-02,  1.5920e-02,\n",
       "                      -1.8508e-03,  2.7437e-02,  2.3359e-02, -7.8485e-03, -2.0523e-02,\n",
       "                       8.3155e-03,  5.0671e-03, -1.7455e-02,  8.4571e-03, -2.5225e-02,\n",
       "                       3.0509e-02,  1.8750e-02, -9.1895e-03, -1.2880e-02, -1.9107e-02,\n",
       "                      -2.7488e-02,  2.7652e-03, -1.2752e-02,  1.5209e-02,  2.5557e-02,\n",
       "                       1.4776e-02,  2.6717e-04, -7.2583e-04,  4.6227e-03, -1.5888e-02,\n",
       "                      -1.8479e-02,  9.0155e-03, -1.9291e-02,  1.9815e-02,  3.2629e-03,\n",
       "                      -1.9572e-02,  1.9327e-02, -1.7160e-02, -9.5998e-03,  1.3684e-02,\n",
       "                      -1.5304e-02, -2.7585e-02, -8.9294e-03, -2.6175e-02, -7.6208e-03,\n",
       "                       1.1418e-03, -1.8077e-03, -3.0466e-04, -1.5505e-02,  5.3769e-03,\n",
       "                      -2.7386e-02, -2.4769e-02,  2.9320e-02,  2.9647e-02,  1.6486e-02,\n",
       "                      -2.8118e-02, -1.0107e-02, -2.7156e-02, -8.3679e-03, -3.0004e-02,\n",
       "                       6.5497e-03, -5.5672e-03, -3.1488e-04, -1.2564e-04, -1.4423e-02,\n",
       "                      -4.0789e-03, -2.0847e-02,  1.3303e-02,  9.6718e-03,  1.1784e-02,\n",
       "                       1.7530e-02,  2.1290e-02,  4.2822e-03, -2.0111e-02,  1.1368e-02,\n",
       "                      -9.1601e-03, -2.1328e-02, -2.7466e-02, -1.6885e-02, -1.6091e-02,\n",
       "                       9.7244e-03, -1.9936e-02,  1.2929e-02,  1.0389e-02,  1.1538e-02,\n",
       "                      -2.1175e-02, -6.5031e-03, -3.0190e-02, -2.3273e-02,  2.0548e-02,\n",
       "                       7.8327e-03, -1.8466e-02,  1.3262e-02,  1.5655e-02,  1.3456e-02,\n",
       "                       2.1307e-02,  9.7154e-03,  2.8025e-02, -3.0610e-03,  2.1344e-02,\n",
       "                      -1.8840e-02,  3.0141e-02, -1.1695e-02, -5.4149e-03, -7.0828e-03,\n",
       "                       8.4438e-04,  4.5137e-03,  2.0981e-02,  5.3983e-03, -9.9910e-03,\n",
       "                       9.3201e-03,  7.4768e-03,  2.6612e-02, -2.7095e-02,  2.9950e-02,\n",
       "                       1.3759e-02,  9.9071e-03, -3.0285e-02,  1.4458e-02, -2.7577e-02,\n",
       "                      -2.2822e-02,  1.6508e-02, -2.9068e-02, -1.6515e-03,  2.4786e-02,\n",
       "                       2.7606e-02, -2.8406e-02, -1.3199e-02,  8.9855e-03,  2.2281e-03,\n",
       "                      -2.1551e-02, -2.5784e-02, -1.6122e-02, -2.1111e-02,  1.1811e-02,\n",
       "                       3.2591e-04,  1.9587e-02, -8.2446e-03,  2.3548e-02, -3.0504e-02,\n",
       "                      -3.1332e-03, -5.5919e-04,  2.5337e-02,  3.0130e-02, -6.7615e-03,\n",
       "                      -1.1696e-02, -1.7320e-02])},\n",
       "             Parameter containing:\n",
       "             tensor([[-0.0277, -0.0181, -0.0108,  ..., -0.0538,  0.0203,  0.0191],\n",
       "                     [ 0.0089, -0.0076, -0.0139,  ..., -0.0477,  0.0035, -0.0482],\n",
       "                     [-0.0173,  0.0101, -0.0397,  ..., -0.1093, -0.0136, -0.0085],\n",
       "                     ...,\n",
       "                     [ 0.0339,  0.0080,  0.0308,  ..., -0.0766,  0.0043, -0.0093],\n",
       "                     [-0.0417,  0.0071,  0.0091,  ..., -0.0197,  0.0193, -0.0354],\n",
       "                     [-0.0422,  0.0207,  0.0240,  ..., -0.0659, -0.0279,  0.0017]],\n",
       "                    requires_grad=True): {'old_init': tensor([[-0.0277, -0.0181, -0.0108,  ..., -0.0011,  0.0203,  0.0395],\n",
       "                      [ 0.0089, -0.0076, -0.0139,  ..., -0.0091,  0.0035, -0.0333],\n",
       "                      [-0.0173,  0.0101, -0.0397,  ..., -0.0374, -0.0136,  0.0193],\n",
       "                      ...,\n",
       "                      [ 0.0339,  0.0080,  0.0308,  ..., -0.0311,  0.0043,  0.0083],\n",
       "                      [-0.0417,  0.0071,  0.0091,  ...,  0.0259,  0.0193, -0.0177],\n",
       "                      [-0.0422,  0.0207,  0.0240,  ..., -0.0110, -0.0279,  0.0229]])},\n",
       "             Parameter containing:\n",
       "             tensor([-0.1434, -0.1015, -0.1162, -0.0842, -0.0748,  0.8578, -0.0959, -0.0589,\n",
       "                     -0.1204, -0.0854], requires_grad=True): {'old_init': tensor([-0.0406, -0.0262,  0.0240,  0.0101, -0.0002, -0.0005, -0.0099,  0.0299,\n",
       "                      -0.0314,  0.0218])},\n",
       "             'tmp': {}})"
      ]
     },
     "execution_count": 1250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1244,
   "id": "2ad5b9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'dict'>, {})\n",
      "param_state\n",
      "here\n",
      "param_state\n",
      "here\n",
      "param_state\n",
      "here\n",
      "param_state\n",
      "here\n",
      "param_state\n",
      "here\n",
      "param_state\n",
      "here\n",
      "param_state\n",
      "here\n",
      "param_state\n",
      "here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1233-bfb02e1ec90b>:115: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370172916/work/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  d_p.add_(self.mu, p.data - param_state['old_init'])\n"
     ]
    }
   ],
   "source": [
    "tmp.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1245,
   "id": "e505d646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 1245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.state[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1243,
   "id": "9dad4f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1243-0b5045d583a1>:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  tmp.param_groups[0]['params'][0][0].grad\n"
     ]
    }
   ],
   "source": [
    "tmp.param_groups[0]['params'][0][0].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1227,
   "id": "41d3ef0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1227-822eb47558e5>:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  pp[0][0].grad\n"
     ]
    }
   ],
   "source": [
    "pp[0][0].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc237af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    r\"\"\"Implements stochastic gradient descent (optionally with momentum).\n",
    "    Nesterov momentum is based on the formula from\n",
    "    `On the importance of initialization and momentum in deep learning`__.\n",
    "    Args:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        lr (float): learning rate\n",
    "        momentum (float, optional): momentum factor (default: 0)\n",
    "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
    "        dampening (float, optional): dampening for momentum (default: 0)\n",
    "        nesterov (bool, optional): enables Nesterov momentum (default: False)\n",
    "    Example:\n",
    "        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "        >>> optimizer.zero_grad()\n",
    "        >>> loss_fn(model(input), target).backward()\n",
    "        >>> optimizer.step()\n",
    "    __ http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf\n",
    "    .. note::\n",
    "        The implementation of SGD with Momentum/Nesterov subtly differs from\n",
    "        Sutskever et. al. and implementations in some other frameworks.\n",
    "        Considering the specific case of Momentum, the update can be written as\n",
    "        .. math::\n",
    "            \\begin{aligned}\n",
    "                v_{t+1} & = \\mu * v_{t} + g_{t+1}, \\\\\n",
    "                p_{t+1} & = p_{t} - \\text{lr} * v_{t+1},\n",
    "            \\end{aligned}\n",
    "        where :math:`p`, :math:`g`, :math:`v` and :math:`\\mu` denote the\n",
    "        parameters, gradient, velocity, and momentum respectively.\n",
    "        This is in contrast to Sutskever et. al. and\n",
    "        other frameworks which employ an update of the form\n",
    "        .. math::\n",
    "            \\begin{aligned}\n",
    "                v_{t+1} & = \\mu * v_{t} + \\text{lr} * g_{t+1}, \\\\\n",
    "                p_{t+1} & = p_{t} - v_{t+1}.\n",
    "            \\end{aligned}\n",
    "        The Nesterov version is analogously modified.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=required, momentum=0, dampening=0,\n",
    "                 weight_decay=0, nesterov=False):\n",
    "        if lr is not required and lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if momentum < 0.0:\n",
    "            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n",
    "        if weight_decay < 0.0:\n",
    "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
    "\n",
    "        defaults = dict(lr=lr, momentum=momentum, dampening=dampening,\n",
    "                        weight_decay=weight_decay, nesterov=nesterov)\n",
    "        if nesterov and (momentum <= 0 or dampening != 0):\n",
    "            raise ValueError(\"Nesterov momentum requires a momentum and zero dampening\")\n",
    "        super(SGD, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(SGD, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('nesterov', False)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Args:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            params_with_grad = []\n",
    "            d_p_list = []\n",
    "            momentum_buffer_list = []\n",
    "            weight_decay = group['weight_decay']\n",
    "            momentum = group['momentum']\n",
    "            dampening = group['dampening']\n",
    "            nesterov = group['nesterov']\n",
    "            lr = group['lr']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is not None:\n",
    "                    params_with_grad.append(p)\n",
    "                    d_p_list.append(p.grad)\n",
    "\n",
    "                    state = self.state[p]\n",
    "                    if 'momentum_buffer' not in state:\n",
    "                        momentum_buffer_list.append(None)\n",
    "                    else:\n",
    "                        momentum_buffer_list.append(state['momentum_buffer'])\n",
    "\n",
    "            F.sgd(params_with_grad,\n",
    "                  d_p_list,\n",
    "                  momentum_buffer_list,\n",
    "                  weight_decay=weight_decay,\n",
    "                  momentum=momentum,\n",
    "                  lr=lr,\n",
    "                  dampening=dampening,\n",
    "                  nesterov=nesterov)\n",
    "\n",
    "            # update momentum_buffers in state\n",
    "            for p, momentum_buffer in zip(params_with_grad, momentum_buffer_list):\n",
    "                state = self.state[p]\n",
    "                state['momentum_buffer'] = momentum_buffer\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1230,
   "id": "6317a171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 1230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.state['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314dcc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, param in enumerate(params):\n",
    "\n",
    "    d_p = d_p_list[i]\n",
    "    if weight_decay != 0:\n",
    "        d_p = d_p.add(param, alpha=weight_decay)\n",
    "\n",
    "    if momentum != 0:\n",
    "        buf = momentum_buffer_list[i]\n",
    "\n",
    "        if buf is None:\n",
    "            buf = torch.clone(d_p).detach()\n",
    "            momentum_buffer_list[i] = buf\n",
    "        else:\n",
    "            buf.mul_(momentum).add_(d_p, alpha=1 - dampening)\n",
    "\n",
    "        if nesterov:\n",
    "            d_p = d_p.add(buf, alpha=momentum)\n",
    "        else:\n",
    "            d_p = buf\n",
    "\n",
    "    param.add_(d_p, alpha=-lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1207,
   "id": "695849bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1212,
   "id": "19803bc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.parameter.Parameter"
      ]
     },
     "execution_count": 1212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1221,
   "id": "cbe12bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    }
   ],
   "source": [
    "param_group = {'params': pp[0]}\n",
    "params = param_group['params']\n",
    "if isinstance(params, torch.Tensor):\n",
    "    print('here')\n",
    "    param_group['params'] = [params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0a8104",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_group['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6999ff6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1217,
   "id": "eccab450",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1217-c748ff28a5ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 4"
     ]
    }
   ],
   "source": [
    "param_group['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d71885d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53251320",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not isinstance(param_groups[0], dict):\n",
    "    param_groups = [{'params': param_groups}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0137449d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1195,
   "id": "6c8c2dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "clients = get_clients(\n",
    "    train_ds, \n",
    "    method='fedprox',\n",
    "    num_clients=100,\n",
    "    is_iid=False,\n",
    "    shard_size=300,\n",
    "    batch_size=50,\n",
    "    num_workers=0,\n",
    "    device=device\n",
    ")\n",
    "fed_cls = FedProx\n",
    "federater = fed_cls(model, \n",
    "                   clients=clients,\n",
    "                   optim_cls=optim_cls,\n",
    "                   optim_params=optim_params,\n",
    "                   criterion=criterion, \n",
    "                   C=0.1,\n",
    "                    mu=1000,\n",
    "                   output_dir='mu1'\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1196,
   "id": "822d0ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1 - train_loss : 78.3555 - train_accuracy : 0.8820 - train_xent_loss : 0.9399 - train_weights_delta : 0.0808 - val_loss : 2.3053 - val_accuracy : 0.1034\n",
      "round 2 - train_loss : 81.7870 - train_accuracy : 0.8213 - train_xent_loss : 0.9839 - train_weights_delta : 0.0840 - val_loss : 2.3001 - val_accuracy : 0.1042\n",
      "round 3 - train_loss : 93.3500 - train_accuracy : 0.8510 - train_xent_loss : 0.8035 - train_weights_delta : 0.0922 - val_loss : 2.2993 - val_accuracy : 0.1529\n",
      "round 4 - train_loss : 101.7852 - train_accuracy : 0.8333 - train_xent_loss : 0.7750 - train_weights_delta : 0.0989 - val_loss : 2.3005 - val_accuracy : 0.1253\n",
      "round 5 - train_loss : 106.8303 - train_accuracy : 0.8857 - train_xent_loss : 0.6514 - train_weights_delta : 0.1007 - val_loss : 2.3055 - val_accuracy : 0.1460\n",
      "round 6 - train_loss : 114.3917 - train_accuracy : 0.8423 - train_xent_loss : 0.6932 - train_weights_delta : 0.1076 - val_loss : 2.2933 - val_accuracy : 0.1507\n",
      "round 7 - train_loss : 126.1735 - train_accuracy : 0.8630 - train_xent_loss : 0.6315 - train_weights_delta : 0.1159 - val_loss : 2.2786 - val_accuracy : 0.1913\n",
      "round 8 - train_loss : 121.4042 - train_accuracy : 0.8953 - train_xent_loss : 0.5595 - train_weights_delta : 0.1102 - val_loss : 2.2831 - val_accuracy : 0.0894\n",
      "round 9 - train_loss : 125.0169 - train_accuracy : 0.8500 - train_xent_loss : 0.5672 - train_weights_delta : 0.1135 - val_loss : 2.2835 - val_accuracy : 0.1707\n",
      "round 10 - train_loss : 140.9137 - train_accuracy : 0.8627 - train_xent_loss : 0.5537 - train_weights_delta : 0.1260 - val_loss : 2.2606 - val_accuracy : 0.1720\n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "federater.fit(num_rounds=10, num_epochs=1, val_dl=test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1163,
   "id": "62c4a74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1 - train_loss : 1.0243 - train_accuracy : 0.8240 - train_xent_loss : 0.9379 - train_weights_delta : 0.0888 - val_loss : 2.3261 - val_accuracy : 0.1138\n",
      "round 2 - train_loss : 0.9458 - train_accuracy : 0.8500 - train_xent_loss : 0.8507 - train_weights_delta : 0.0953 - val_loss : 2.3223 - val_accuracy : 0.1138\n",
      "round 3 - train_loss : 0.8591 - train_accuracy : 0.8500 - train_xent_loss : 0.7551 - train_weights_delta : 0.1012 - val_loss : 2.3194 - val_accuracy : 0.1162\n",
      "round 4 - train_loss : 0.7957 - train_accuracy : 0.8503 - train_xent_loss : 0.6841 - train_weights_delta : 0.1058 - val_loss : 2.3144 - val_accuracy : 0.1673\n",
      "round 5 - train_loss : 0.7910 - train_accuracy : 0.8477 - train_xent_loss : 0.6673 - train_weights_delta : 0.1152 - val_loss : 2.3000 - val_accuracy : 0.1278\n",
      "round 6 - train_loss : 0.7412 - train_accuracy : 0.8677 - train_xent_loss : 0.6166 - train_weights_delta : 0.1148 - val_loss : 2.2938 - val_accuracy : 0.1147\n",
      "round 7 - train_loss : 0.6914 - train_accuracy : 0.8500 - train_xent_loss : 0.5604 - train_weights_delta : 0.1186 - val_loss : 2.2827 - val_accuracy : 0.2067\n",
      "round 8 - train_loss : 0.6707 - train_accuracy : 0.8497 - train_xent_loss : 0.5352 - train_weights_delta : 0.1215 - val_loss : 2.2649 - val_accuracy : 0.2273\n",
      "round 9 - train_loss : 0.6542 - train_accuracy : 0.8997 - train_xent_loss : 0.5266 - train_weights_delta : 0.1147 - val_loss : 2.2764 - val_accuracy : 0.1514\n",
      "round 10 - train_loss : 0.6620 - train_accuracy : 0.8503 - train_xent_loss : 0.5138 - train_weights_delta : 0.1308 - val_loss : 2.2499 - val_accuracy : 0.1162\n",
      "round 11 - train_loss : 0.6142 - train_accuracy : 0.8500 - train_xent_loss : 0.4740 - train_weights_delta : 0.1234 - val_loss : 2.2502 - val_accuracy : 0.2160\n",
      "round 12 - train_loss : 0.5985 - train_accuracy : 0.8920 - train_xent_loss : 0.4593 - train_weights_delta : 0.1219 - val_loss : 2.2617 - val_accuracy : 0.1812\n",
      "round 13 - train_loss : 0.5732 - train_accuracy : 0.8780 - train_xent_loss : 0.4320 - train_weights_delta : 0.1225 - val_loss : 2.2644 - val_accuracy : 0.1280\n",
      "round 14 - train_loss : 0.5880 - train_accuracy : 0.8500 - train_xent_loss : 0.4357 - train_weights_delta : 0.1309 - val_loss : 2.2412 - val_accuracy : 0.1173\n",
      "round 15 - train_loss : 0.5768 - train_accuracy : 0.8690 - train_xent_loss : 0.4347 - train_weights_delta : 0.1231 - val_loss : 2.2472 - val_accuracy : 0.1603\n",
      "round 16 - train_loss : 0.5506 - train_accuracy : 0.8837 - train_xent_loss : 0.4103 - train_weights_delta : 0.1207 - val_loss : 2.2537 - val_accuracy : 0.1804\n",
      "round 17 - train_loss : 0.5753 - train_accuracy : 0.8513 - train_xent_loss : 0.4168 - train_weights_delta : 0.1344 - val_loss : 2.2062 - val_accuracy : 0.1903\n",
      "round 18 - train_loss : 0.5325 - train_accuracy : 0.8813 - train_xent_loss : 0.3788 - train_weights_delta : 0.1299 - val_loss : 2.2142 - val_accuracy : 0.1847\n",
      "round 19 - train_loss : 0.5525 - train_accuracy : 0.8830 - train_xent_loss : 0.3882 - train_weights_delta : 0.1383 - val_loss : 2.2245 - val_accuracy : 0.1000\n",
      "round 20 - train_loss : 0.5190 - train_accuracy : 0.8670 - train_xent_loss : 0.3703 - train_weights_delta : 0.1258 - val_loss : 2.2257 - val_accuracy : 0.1220\n",
      "round 21 - train_loss : 0.6222 - train_accuracy : 0.8373 - train_xent_loss : 0.4386 - train_weights_delta : 0.1544 - val_loss : 2.1860 - val_accuracy : 0.1260\n",
      "round 22 - train_loss : 0.5470 - train_accuracy : 0.8667 - train_xent_loss : 0.3846 - train_weights_delta : 0.1367 - val_loss : 2.1661 - val_accuracy : 0.1050\n",
      "round 23 - train_loss : 0.5459 - train_accuracy : 0.8333 - train_xent_loss : 0.3802 - train_weights_delta : 0.1391 - val_loss : 2.1699 - val_accuracy : 0.2685\n",
      "round 24 - train_loss : 0.5685 - train_accuracy : 0.8740 - train_xent_loss : 0.4007 - train_weights_delta : 0.1414 - val_loss : 2.1473 - val_accuracy : 0.1764\n",
      "round 25 - train_loss : 0.6302 - train_accuracy : 0.8483 - train_xent_loss : 0.4504 - train_weights_delta : 0.1526 - val_loss : 2.1267 - val_accuracy : 0.2851\n",
      "round 26 - train_loss : 0.5381 - train_accuracy : 0.8847 - train_xent_loss : 0.3764 - train_weights_delta : 0.1362 - val_loss : 2.1234 - val_accuracy : 0.3020\n",
      "round 27 - train_loss : 0.5254 - train_accuracy : 0.8880 - train_xent_loss : 0.3690 - train_weights_delta : 0.1317 - val_loss : 2.1742 - val_accuracy : 0.1286\n",
      "round 28 - train_loss : 0.5270 - train_accuracy : 0.8640 - train_xent_loss : 0.3546 - train_weights_delta : 0.1439 - val_loss : 2.1724 - val_accuracy : 0.1697\n",
      "round 29 - train_loss : 0.6333 - train_accuracy : 0.8333 - train_xent_loss : 0.4234 - train_weights_delta : 0.1748 - val_loss : 2.1011 - val_accuracy : 0.2327\n",
      "round 30 - train_loss : 0.5334 - train_accuracy : 0.8780 - train_xent_loss : 0.3668 - train_weights_delta : 0.1400 - val_loss : 2.1021 - val_accuracy : 0.3895\n",
      "round 31 - train_loss : 0.5446 - train_accuracy : 0.8907 - train_xent_loss : 0.3760 - train_weights_delta : 0.1414 - val_loss : 2.1022 - val_accuracy : 0.2050\n",
      "round 32 - train_loss : 0.5162 - train_accuracy : 0.8973 - train_xent_loss : 0.3476 - train_weights_delta : 0.1412 - val_loss : 2.1057 - val_accuracy : 0.2247\n",
      "round 33 - train_loss : 0.5363 - train_accuracy : 0.8683 - train_xent_loss : 0.3499 - train_weights_delta : 0.1550 - val_loss : 2.0942 - val_accuracy : 0.2086\n",
      "round 34 - train_loss : 0.5534 - train_accuracy : 0.8600 - train_xent_loss : 0.3648 - train_weights_delta : 0.1570 - val_loss : 2.0619 - val_accuracy : 0.3084\n",
      "round 35 - train_loss : 0.5457 - train_accuracy : 0.8840 - train_xent_loss : 0.3707 - train_weights_delta : 0.1466 - val_loss : 2.0349 - val_accuracy : 0.4455\n",
      "round 36 - train_loss : 0.5495 - train_accuracy : 0.8930 - train_xent_loss : 0.3722 - train_weights_delta : 0.1484 - val_loss : 2.0483 - val_accuracy : 0.3613\n",
      "round 37 - train_loss : 0.5640 - train_accuracy : 0.8607 - train_xent_loss : 0.3704 - train_weights_delta : 0.1610 - val_loss : 2.0240 - val_accuracy : 0.3031\n",
      "round 38 - train_loss : 0.5290 - train_accuracy : 0.8830 - train_xent_loss : 0.3503 - train_weights_delta : 0.1493 - val_loss : 2.0244 - val_accuracy : 0.4280\n",
      "round 39 - train_loss : 0.5136 - train_accuracy : 0.9090 - train_xent_loss : 0.3431 - train_weights_delta : 0.1424 - val_loss : 2.0472 - val_accuracy : 0.3148\n",
      "round 40 - train_loss : 0.5493 - train_accuracy : 0.8687 - train_xent_loss : 0.3569 - train_weights_delta : 0.1599 - val_loss : 2.0098 - val_accuracy : 0.4679\n",
      "round 41 - train_loss : 0.5392 - train_accuracy : 0.9113 - train_xent_loss : 0.3517 - train_weights_delta : 0.1562 - val_loss : 1.9872 - val_accuracy : 0.4925\n",
      "round 42 - train_loss : 0.5348 - train_accuracy : 0.9213 - train_xent_loss : 0.3357 - train_weights_delta : 0.1652 - val_loss : 2.0595 - val_accuracy : 0.1548\n",
      "round 43 - train_loss : 0.5648 - train_accuracy : 0.8567 - train_xent_loss : 0.3590 - train_weights_delta : 0.1711 - val_loss : 1.9865 - val_accuracy : 0.3844\n",
      "round 44 - train_loss : 0.5192 - train_accuracy : 0.9017 - train_xent_loss : 0.3202 - train_weights_delta : 0.1650 - val_loss : 1.9914 - val_accuracy : 0.2823\n",
      "round 45 - train_loss : 0.4907 - train_accuracy : 0.9200 - train_xent_loss : 0.3065 - train_weights_delta : 0.1532 - val_loss : 2.1003 - val_accuracy : 0.1213\n",
      "round 46 - train_loss : 0.6008 - train_accuracy : 0.8370 - train_xent_loss : 0.3679 - train_weights_delta : 0.1931 - val_loss : 1.9857 - val_accuracy : 0.4145\n",
      "round 47 - train_loss : 0.5019 - train_accuracy : 0.9230 - train_xent_loss : 0.3118 - train_weights_delta : 0.1579 - val_loss : 1.9960 - val_accuracy : 0.3418\n",
      "round 48 - train_loss : 0.6401 - train_accuracy : 0.8427 - train_xent_loss : 0.4145 - train_weights_delta : 0.1872 - val_loss : 1.9200 - val_accuracy : 0.5127\n",
      "round 49 - train_loss : 0.5481 - train_accuracy : 0.8947 - train_xent_loss : 0.3440 - train_weights_delta : 0.1695 - val_loss : 1.9600 - val_accuracy : 0.2407\n",
      "round 50 - train_loss : 0.5244 - train_accuracy : 0.8867 - train_xent_loss : 0.3292 - train_weights_delta : 0.1621 - val_loss : 1.9417 - val_accuracy : 0.2336\n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "federater.fit(num_rounds=50, num_epochs=1, val_dl=test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1153,
   "id": "5c0fb77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1 - train_loss : 1.1173 - train_accuracy : 0.7953 - train_xent_loss : 1.1106 - train_weights_delta : 0.0723 - val_loss : 2.3110 - val_accuracy : 0.0953\n",
      "round 2 - train_loss : 1.0041 - train_accuracy : 0.8437 - train_xent_loss : 0.9962 - train_weights_delta : 0.0832 - val_loss : 2.3044 - val_accuracy : 0.1025\n",
      "round 3 - train_loss : 0.8556 - train_accuracy : 0.8340 - train_xent_loss : 0.8465 - train_weights_delta : 0.0915 - val_loss : 2.2992 - val_accuracy : 0.0984\n",
      "round 4 - train_loss : 0.8237 - train_accuracy : 0.8620 - train_xent_loss : 0.8142 - train_weights_delta : 0.0939 - val_loss : 2.2952 - val_accuracy : 0.1003\n",
      "round 5 - train_loss : 0.7290 - train_accuracy : 0.8500 - train_xent_loss : 0.7186 - train_weights_delta : 0.0998 - val_loss : 2.2968 - val_accuracy : 0.1227\n",
      "round 6 - train_loss : 0.6908 - train_accuracy : 0.8523 - train_xent_loss : 0.6795 - train_weights_delta : 0.1068 - val_loss : 2.2913 - val_accuracy : 0.1278\n",
      "round 7 - train_loss : 0.6580 - train_accuracy : 0.8650 - train_xent_loss : 0.6471 - train_weights_delta : 0.1030 - val_loss : 2.2978 - val_accuracy : 0.1126\n",
      "round 8 - train_loss : 0.6362 - train_accuracy : 0.8347 - train_xent_loss : 0.6242 - train_weights_delta : 0.1116 - val_loss : 2.2925 - val_accuracy : 0.2505\n",
      "round 9 - train_loss : 0.5479 - train_accuracy : 0.8897 - train_xent_loss : 0.5357 - train_weights_delta : 0.1107 - val_loss : 2.2966 - val_accuracy : 0.2790\n",
      "round 10 - train_loss : 0.5663 - train_accuracy : 0.8580 - train_xent_loss : 0.5523 - train_weights_delta : 0.1257 - val_loss : 2.2829 - val_accuracy : 0.2704\n",
      "round 11 - train_loss : 0.4878 - train_accuracy : 0.9097 - train_xent_loss : 0.4752 - train_weights_delta : 0.1118 - val_loss : 2.3016 - val_accuracy : 0.1571\n",
      "round 12 - train_loss : 0.4949 - train_accuracy : 0.9033 - train_xent_loss : 0.4820 - train_weights_delta : 0.1143 - val_loss : 2.3068 - val_accuracy : 0.1624\n",
      "round 13 - train_loss : 0.5086 - train_accuracy : 0.8667 - train_xent_loss : 0.4941 - train_weights_delta : 0.1274 - val_loss : 2.2851 - val_accuracy : 0.1304\n",
      "round 14 - train_loss : 0.4796 - train_accuracy : 0.8553 - train_xent_loss : 0.4660 - train_weights_delta : 0.1197 - val_loss : 2.2716 - val_accuracy : 0.1955\n",
      "round 15 - train_loss : 0.4623 - train_accuracy : 0.8807 - train_xent_loss : 0.4480 - train_weights_delta : 0.1244 - val_loss : 2.2546 - val_accuracy : 0.1954\n",
      "round 16 - train_loss : 0.4759 - train_accuracy : 0.8633 - train_xent_loss : 0.4616 - train_weights_delta : 0.1250 - val_loss : 2.2472 - val_accuracy : 0.2805\n",
      "round 17 - train_loss : 0.4377 - train_accuracy : 0.8890 - train_xent_loss : 0.4237 - train_weights_delta : 0.1216 - val_loss : 2.2485 - val_accuracy : 0.3078\n",
      "round 18 - train_loss : 0.4372 - train_accuracy : 0.9010 - train_xent_loss : 0.4232 - train_weights_delta : 0.1209 - val_loss : 2.2519 - val_accuracy : 0.2367\n",
      "round 19 - train_loss : 0.4453 - train_accuracy : 0.8763 - train_xent_loss : 0.4300 - train_weights_delta : 0.1312 - val_loss : 2.2318 - val_accuracy : 0.2547\n",
      "round 20 - train_loss : 0.4191 - train_accuracy : 0.8633 - train_xent_loss : 0.4040 - train_weights_delta : 0.1285 - val_loss : 2.2137 - val_accuracy : 0.3359\n",
      "round 21 - train_loss : 0.4059 - train_accuracy : 0.8680 - train_xent_loss : 0.3908 - train_weights_delta : 0.1281 - val_loss : 2.2130 - val_accuracy : 0.2761\n",
      "round 22 - train_loss : 0.3797 - train_accuracy : 0.9283 - train_xent_loss : 0.3660 - train_weights_delta : 0.1166 - val_loss : 2.2314 - val_accuracy : 0.2882\n",
      "round 23 - train_loss : 0.4106 - train_accuracy : 0.8627 - train_xent_loss : 0.3946 - train_weights_delta : 0.1346 - val_loss : 2.2136 - val_accuracy : 0.2838\n",
      "round 24 - train_loss : 0.3821 - train_accuracy : 0.9090 - train_xent_loss : 0.3676 - train_weights_delta : 0.1228 - val_loss : 2.2200 - val_accuracy : 0.2998\n",
      "round 25 - train_loss : 0.3967 - train_accuracy : 0.8973 - train_xent_loss : 0.3815 - train_weights_delta : 0.1291 - val_loss : 2.2232 - val_accuracy : 0.2350\n",
      "round 26 - train_loss : 0.4093 - train_accuracy : 0.8797 - train_xent_loss : 0.3924 - train_weights_delta : 0.1423 - val_loss : 2.2082 - val_accuracy : 0.2394\n",
      "round 27 - train_loss : 0.3517 - train_accuracy : 0.9000 - train_xent_loss : 0.3371 - train_weights_delta : 0.1233 - val_loss : 2.2443 - val_accuracy : 0.2094\n",
      "round 28 - train_loss : 0.3726 - train_accuracy : 0.9023 - train_xent_loss : 0.3582 - train_weights_delta : 0.1209 - val_loss : 2.2437 - val_accuracy : 0.2389\n",
      "round 29 - train_loss : 0.3645 - train_accuracy : 0.9040 - train_xent_loss : 0.3495 - train_weights_delta : 0.1260 - val_loss : 2.2348 - val_accuracy : 0.2230\n",
      "round 30 - train_loss : 0.3279 - train_accuracy : 0.9183 - train_xent_loss : 0.3141 - train_weights_delta : 0.1154 - val_loss : 2.2725 - val_accuracy : 0.2104\n",
      "round 31 - train_loss : 0.3914 - train_accuracy : 0.8663 - train_xent_loss : 0.3742 - train_weights_delta : 0.1436 - val_loss : 2.2389 - val_accuracy : 0.2969\n",
      "round 32 - train_loss : 0.3255 - train_accuracy : 0.9467 - train_xent_loss : 0.3122 - train_weights_delta : 0.1123 - val_loss : 2.2742 - val_accuracy : 0.2945\n",
      "round 33 - train_loss : 0.4205 - train_accuracy : 0.8497 - train_xent_loss : 0.4020 - train_weights_delta : 0.1548 - val_loss : 2.1954 - val_accuracy : 0.3143\n",
      "round 34 - train_loss : 0.4048 - train_accuracy : 0.8973 - train_xent_loss : 0.3884 - train_weights_delta : 0.1378 - val_loss : 2.1740 - val_accuracy : 0.2499\n",
      "round 35 - train_loss : 0.3682 - train_accuracy : 0.8690 - train_xent_loss : 0.3521 - train_weights_delta : 0.1344 - val_loss : 2.1832 - val_accuracy : 0.2199\n",
      "round 36 - train_loss : 0.3626 - train_accuracy : 0.8710 - train_xent_loss : 0.3452 - train_weights_delta : 0.1448 - val_loss : 2.1476 - val_accuracy : 0.3017\n",
      "round 37 - train_loss : 0.3416 - train_accuracy : 0.9103 - train_xent_loss : 0.3258 - train_weights_delta : 0.1321 - val_loss : 2.1556 - val_accuracy : 0.2995\n",
      "round 38 - train_loss : 0.3869 - train_accuracy : 0.8807 - train_xent_loss : 0.3696 - train_weights_delta : 0.1446 - val_loss : 2.1400 - val_accuracy : 0.3770\n",
      "round 39 - train_loss : 0.3287 - train_accuracy : 0.9237 - train_xent_loss : 0.3131 - train_weights_delta : 0.1307 - val_loss : 2.1459 - val_accuracy : 0.3102\n",
      "round 40 - train_loss : 0.3103 - train_accuracy : 0.8983 - train_xent_loss : 0.2954 - train_weights_delta : 0.1244 - val_loss : 2.2932 - val_accuracy : 0.2096\n",
      "round 41 - train_loss : 0.3469 - train_accuracy : 0.8660 - train_xent_loss : 0.3282 - train_weights_delta : 0.1549 - val_loss : 2.2145 - val_accuracy : 0.4271\n",
      "round 42 - train_loss : 0.3800 - train_accuracy : 0.9010 - train_xent_loss : 0.3612 - train_weights_delta : 0.1567 - val_loss : 2.1875 - val_accuracy : 0.2220\n",
      "round 43 - train_loss : 0.3938 - train_accuracy : 0.8793 - train_xent_loss : 0.3768 - train_weights_delta : 0.1424 - val_loss : 2.1504 - val_accuracy : 0.2399\n",
      "round 44 - train_loss : 0.4116 - train_accuracy : 0.8663 - train_xent_loss : 0.3922 - train_weights_delta : 0.1618 - val_loss : 2.0994 - val_accuracy : 0.2239\n",
      "round 45 - train_loss : 0.3912 - train_accuracy : 0.8333 - train_xent_loss : 0.3696 - train_weights_delta : 0.1796 - val_loss : 2.0694 - val_accuracy : 0.3408\n",
      "round 46 - train_loss : 0.3903 - train_accuracy : 0.8987 - train_xent_loss : 0.3734 - train_weights_delta : 0.1409 - val_loss : 2.0431 - val_accuracy : 0.3986\n",
      "round 47 - train_loss : 0.3441 - train_accuracy : 0.9043 - train_xent_loss : 0.3260 - train_weights_delta : 0.1504 - val_loss : 2.0598 - val_accuracy : 0.4070\n",
      "round 48 - train_loss : 0.3815 - train_accuracy : 0.8990 - train_xent_loss : 0.3633 - train_weights_delta : 0.1522 - val_loss : 2.0050 - val_accuracy : 0.4447\n",
      "round 49 - train_loss : 0.3263 - train_accuracy : 0.9303 - train_xent_loss : 0.3106 - train_weights_delta : 0.1317 - val_loss : 2.0851 - val_accuracy : 0.2612\n",
      "round 50 - train_loss : 0.3120 - train_accuracy : 0.8937 - train_xent_loss : 0.2943 - train_weights_delta : 0.1466 - val_loss : 2.0694 - val_accuracy : 0.3668\n"
     ]
    }
   ],
   "source": [
    "# 0.1\n",
    "federater.fit(num_rounds=50, num_epochs=1, val_dl=test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1134,
   "id": "12213471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1 - train_loss : 0.8697 - train_accuracy : 0.8507 - val_loss : 2.3002 - val_accuracy : 0.1090\n",
      "round 2 - train_loss : 0.8123 - train_accuracy : 0.8583 - val_loss : 2.2975 - val_accuracy : 0.1299\n",
      "round 3 - train_loss : 0.7712 - train_accuracy : 0.8477 - val_loss : 2.2965 - val_accuracy : 0.1081\n",
      "round 4 - train_loss : 0.6790 - train_accuracy : 0.8703 - val_loss : 2.2955 - val_accuracy : 0.1208\n",
      "round 5 - train_loss : 0.6348 - train_accuracy : 0.8413 - val_loss : 2.2920 - val_accuracy : 0.1671\n",
      "round 6 - train_loss : 0.6197 - train_accuracy : 0.8447 - val_loss : 2.2789 - val_accuracy : 0.2053\n",
      "round 7 - train_loss : 0.5574 - train_accuracy : 0.8797 - val_loss : 2.2858 - val_accuracy : 0.1840\n",
      "round 8 - train_loss : 0.5512 - train_accuracy : 0.8767 - val_loss : 2.2874 - val_accuracy : 0.1009\n",
      "round 9 - train_loss : 0.4433 - train_accuracy : 0.9333 - val_loss : 2.3899 - val_accuracy : 0.1009\n",
      "round 10 - train_loss : 0.5090 - train_accuracy : 0.8667 - val_loss : 2.3651 - val_accuracy : 0.1009\n"
     ]
    }
   ],
   "source": [
    "federater.fit(num_rounds=10, num_epochs=1, val_dl=test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1560fc00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1096,
   "id": "01527b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1 - train_loss : 0.8465 - train_accuracy : 0.8743 - val_loss : 2.2897 - val_accuracy : 0.2361\n",
      "round 2 - train_loss : 0.7888 - train_accuracy : 0.8497 - val_loss : 2.2847 - val_accuracy : 0.1830\n",
      "round 3 - train_loss : 0.7610 - train_accuracy : 0.8487 - val_loss : 2.2787 - val_accuracy : 0.1567\n",
      "round 4 - train_loss : 0.6402 - train_accuracy : 0.9013 - val_loss : 2.2836 - val_accuracy : 0.1010\n",
      "round 5 - train_loss : 0.6472 - train_accuracy : 0.8333 - val_loss : 2.2681 - val_accuracy : 0.1503\n",
      "round 6 - train_loss : 0.5997 - train_accuracy : 0.8433 - val_loss : 2.2516 - val_accuracy : 0.2674\n",
      "round 7 - train_loss : 0.5411 - train_accuracy : 0.8953 - val_loss : 2.2446 - val_accuracy : 0.1829\n",
      "round 8 - train_loss : 0.5276 - train_accuracy : 0.8637 - val_loss : 2.2356 - val_accuracy : 0.1947\n",
      "round 9 - train_loss : 0.4706 - train_accuracy : 0.9150 - val_loss : 2.2518 - val_accuracy : 0.0958\n",
      "round 10 - train_loss : 0.4918 - train_accuracy : 0.8500 - val_loss : 2.2306 - val_accuracy : 0.1027\n"
     ]
    }
   ],
   "source": [
    "federater.fit(num_rounds=10, num_epochs=1, val_dl=test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1093,
   "id": "2905944f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1 - train_loss : 0.5057 - train_accuracy : 0.8840 - val_loss : 2.3208 - val_accuracy : 0.0936\n",
      "round 2 - train_loss : 0.4780 - train_accuracy : 0.8343 - val_loss : 2.3035 - val_accuracy : 0.2034\n",
      "round 3 - train_loss : 0.4342 - train_accuracy : 0.8767 - val_loss : 2.3096 - val_accuracy : 0.2068\n",
      "round 4 - train_loss : 0.4736 - train_accuracy : 0.8670 - val_loss : 2.3189 - val_accuracy : 0.3255\n",
      "round 5 - train_loss : 0.4287 - train_accuracy : 0.9023 - val_loss : 2.3270 - val_accuracy : 0.2267\n",
      "round 6 - train_loss : 0.4493 - train_accuracy : 0.8537 - val_loss : 2.2895 - val_accuracy : 0.3200\n",
      "round 7 - train_loss : 0.4289 - train_accuracy : 0.8857 - val_loss : 2.2831 - val_accuracy : 0.2452\n",
      "round 8 - train_loss : 0.4404 - train_accuracy : 0.8850 - val_loss : 2.2737 - val_accuracy : 0.2514\n",
      "round 9 - train_loss : 0.4510 - train_accuracy : 0.8660 - val_loss : 2.2353 - val_accuracy : 0.3152\n",
      "round 10 - train_loss : 0.4347 - train_accuracy : 0.8877 - val_loss : 2.2077 - val_accuracy : 0.2248\n"
     ]
    }
   ],
   "source": [
    "federater.fit(num_rounds=10, num_epochs=1, val_dl=test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1082,
   "id": "1b0138ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1 - train_loss : 1.0563 - train_accuracy : 0.8353 - val_loss : 2.2984 - val_accuracy : 0.1088\n",
      "round 2 - train_loss : 0.9254 - train_accuracy : 0.8500 - val_loss : 2.2924 - val_accuracy : 0.1308\n",
      "round 3 - train_loss : 0.8273 - train_accuracy : 0.8697 - val_loss : 2.2908 - val_accuracy : 0.1035\n",
      "round 4 - train_loss : 0.7899 - train_accuracy : 0.8347 - val_loss : 2.2940 - val_accuracy : 0.1139\n",
      "round 5 - train_loss : 0.6584 - train_accuracy : 0.8687 - val_loss : 2.2940 - val_accuracy : 0.1279\n",
      "round 6 - train_loss : 0.6520 - train_accuracy : 0.8427 - val_loss : 2.3053 - val_accuracy : 0.1472\n",
      "round 7 - train_loss : 0.5746 - train_accuracy : 0.8800 - val_loss : 2.3093 - val_accuracy : 0.1566\n",
      "round 8 - train_loss : 0.5822 - train_accuracy : 0.8597 - val_loss : 2.3179 - val_accuracy : 0.0980\n",
      "round 9 - train_loss : 0.5821 - train_accuracy : 0.8333 - val_loss : 2.3079 - val_accuracy : 0.1019\n",
      "round 10 - train_loss : 0.5713 - train_accuracy : 0.8333 - val_loss : 2.2820 - val_accuracy : 0.1566\n"
     ]
    }
   ],
   "source": [
    "federater.fit(num_rounds=10, num_epochs=1, val_dl=test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1000,
   "id": "d34c2197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1 - train_loss : 0.1825 - train_accuracy : 0.9713 - val_loss : 2.2786 - val_accuracy : 0.1963\n",
      "round 2 - train_loss : 0.1276 - train_accuracy : 0.9763 - val_loss : 2.2947 - val_accuracy : 0.1642\n",
      "round 3 - train_loss : 0.1219 - train_accuracy : 0.9753 - val_loss : 2.2815 - val_accuracy : 0.0967\n",
      "round 4 - train_loss : 0.1064 - train_accuracy : 0.9667 - val_loss : 2.3481 - val_accuracy : 0.1166\n",
      "round 5 - train_loss : 0.0966 - train_accuracy : 0.9700 - val_loss : 2.2797 - val_accuracy : 0.2585\n",
      "round 6 - train_loss : 0.1048 - train_accuracy : 0.9667 - val_loss : 2.2043 - val_accuracy : 0.1887\n",
      "round 7 - train_loss : 0.0805 - train_accuracy : 0.9743 - val_loss : 2.3510 - val_accuracy : 0.1010\n",
      "round 8 - train_loss : 0.0762 - train_accuracy : 0.9733 - val_loss : 2.2810 - val_accuracy : 0.2122\n",
      "round 9 - train_loss : 0.0737 - train_accuracy : 0.9820 - val_loss : 2.2808 - val_accuracy : 0.1925\n",
      "round 10 - train_loss : 0.0878 - train_accuracy : 0.9667 - val_loss : 2.2817 - val_accuracy : 0.2073\n",
      "round 11 - train_loss : 0.0672 - train_accuracy : 0.9790 - val_loss : 2.2504 - val_accuracy : 0.2669\n",
      "round 12 - train_loss : 0.0869 - train_accuracy : 0.9723 - val_loss : 2.2102 - val_accuracy : 0.1733\n",
      "round 13 - train_loss : 0.0648 - train_accuracy : 0.9740 - val_loss : 2.2208 - val_accuracy : 0.2274\n",
      "round 14 - train_loss : 0.1004 - train_accuracy : 0.9667 - val_loss : 2.1707 - val_accuracy : 0.0983\n",
      "round 15 - train_loss : 0.0752 - train_accuracy : 0.9700 - val_loss : 2.1440 - val_accuracy : 0.2293\n",
      "round 16 - train_loss : 0.0799 - train_accuracy : 0.9747 - val_loss : 2.0146 - val_accuracy : 0.3075\n",
      "round 17 - train_loss : 0.0687 - train_accuracy : 0.9780 - val_loss : 2.1213 - val_accuracy : 0.1957\n",
      "round 18 - train_loss : 0.0765 - train_accuracy : 0.9730 - val_loss : 2.0062 - val_accuracy : 0.3954\n",
      "round 19 - train_loss : 0.0790 - train_accuracy : 0.9740 - val_loss : 1.9276 - val_accuracy : 0.5451\n",
      "round 20 - train_loss : 0.0744 - train_accuracy : 0.9773 - val_loss : 2.0373 - val_accuracy : 0.3736\n",
      "round 21 - train_loss : 0.0772 - train_accuracy : 0.9723 - val_loss : 1.9701 - val_accuracy : 0.2672\n",
      "round 22 - train_loss : 0.0644 - train_accuracy : 0.9783 - val_loss : 1.9409 - val_accuracy : 0.2882\n",
      "round 23 - train_loss : 0.0731 - train_accuracy : 0.9717 - val_loss : 1.8585 - val_accuracy : 0.4710\n",
      "round 24 - train_loss : 0.0669 - train_accuracy : 0.9767 - val_loss : 2.1738 - val_accuracy : 0.0975\n",
      "round 25 - train_loss : 0.0741 - train_accuracy : 0.9700 - val_loss : 1.9599 - val_accuracy : 0.2589\n",
      "round 26 - train_loss : 0.0646 - train_accuracy : 0.9770 - val_loss : 2.1072 - val_accuracy : 0.2173\n",
      "round 27 - train_loss : 0.0787 - train_accuracy : 0.9720 - val_loss : 1.8159 - val_accuracy : 0.4611\n",
      "round 28 - train_loss : 0.0563 - train_accuracy : 0.9880 - val_loss : 2.0321 - val_accuracy : 0.2506\n",
      "round 29 - train_loss : 0.0749 - train_accuracy : 0.9730 - val_loss : 1.8164 - val_accuracy : 0.3869\n",
      "round 30 - train_loss : 0.0536 - train_accuracy : 0.9853 - val_loss : 1.8798 - val_accuracy : 0.3180\n",
      "round 31 - train_loss : 0.0636 - train_accuracy : 0.9773 - val_loss : 1.8751 - val_accuracy : 0.2806\n",
      "round 32 - train_loss : 0.0702 - train_accuracy : 0.9700 - val_loss : 1.8382 - val_accuracy : 0.3350\n",
      "round 33 - train_loss : 0.0571 - train_accuracy : 0.9843 - val_loss : 1.8015 - val_accuracy : 0.4185\n",
      "round 34 - train_loss : 0.0568 - train_accuracy : 0.9830 - val_loss : 1.7786 - val_accuracy : 0.5089\n",
      "round 35 - train_loss : 0.0676 - train_accuracy : 0.9810 - val_loss : 1.8016 - val_accuracy : 0.2928\n",
      "round 36 - train_loss : 0.0555 - train_accuracy : 0.9800 - val_loss : 1.7348 - val_accuracy : 0.3984\n",
      "round 37 - train_loss : 0.0491 - train_accuracy : 0.9873 - val_loss : 1.7470 - val_accuracy : 0.3973\n",
      "round 38 - train_loss : 0.0544 - train_accuracy : 0.9867 - val_loss : 1.7698 - val_accuracy : 0.3117\n",
      "round 39 - train_loss : 0.0563 - train_accuracy : 0.9793 - val_loss : 1.7979 - val_accuracy : 0.3207\n",
      "round 40 - train_loss : 0.0732 - train_accuracy : 0.9727 - val_loss : 1.6200 - val_accuracy : 0.4735\n",
      "round 41 - train_loss : 0.0577 - train_accuracy : 0.9820 - val_loss : 1.5675 - val_accuracy : 0.5903\n",
      "round 42 - train_loss : 0.0631 - train_accuracy : 0.9790 - val_loss : 1.9416 - val_accuracy : 0.2371\n",
      "round 43 - train_loss : 0.0621 - train_accuracy : 0.9757 - val_loss : 1.6902 - val_accuracy : 0.3640\n",
      "round 44 - train_loss : 0.0517 - train_accuracy : 0.9820 - val_loss : 1.5689 - val_accuracy : 0.5252\n",
      "round 45 - train_loss : 0.0530 - train_accuracy : 0.9807 - val_loss : 1.5505 - val_accuracy : 0.5639\n",
      "round 46 - train_loss : 0.0617 - train_accuracy : 0.9807 - val_loss : 1.5961 - val_accuracy : 0.4741\n",
      "round 47 - train_loss : 0.0426 - train_accuracy : 0.9880 - val_loss : 1.7038 - val_accuracy : 0.3887\n",
      "round 48 - train_loss : 0.0484 - train_accuracy : 0.9840 - val_loss : 1.5826 - val_accuracy : 0.4842\n",
      "round 49 - train_loss : 0.0630 - train_accuracy : 0.9770 - val_loss : 1.6309 - val_accuracy : 0.4130\n",
      "round 50 - train_loss : 0.0623 - train_accuracy : 0.9780 - val_loss : 1.4847 - val_accuracy : 0.5061\n"
     ]
    }
   ],
   "source": [
    "federater.fit(num_rounds=50, num_epochs=1, val_dl=test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 902,
   "id": "7ea22056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 2 - train_loss : 0.0716 - train_accuracy : 0.0871 - val_loss : 2.3012 - val_accuracy : 0.1034\n",
      "round 3 - train_loss : 0.0684 - train_accuracy : 0.0834 - val_loss : 2.3043 - val_accuracy : 0.1396\n",
      "round 4 - train_loss : 0.0649 - train_accuracy : 0.0868 - val_loss : 2.2986 - val_accuracy : 0.1518\n",
      "round 5 - train_loss : 0.0638 - train_accuracy : 0.0863 - val_loss : 2.2956 - val_accuracy : 0.2013\n",
      "round 6 - train_loss : 0.0578 - train_accuracy : 0.0880 - val_loss : 2.2905 - val_accuracy : 0.2041\n",
      "round 7 - train_loss : 0.0524 - train_accuracy : 0.0866 - val_loss : 2.2830 - val_accuracy : 0.2260\n",
      "round 8 - train_loss : 0.0504 - train_accuracy : 0.0857 - val_loss : 2.2845 - val_accuracy : 0.3227\n",
      "round 9 - train_loss : 0.0526 - train_accuracy : 0.0879 - val_loss : 2.2593 - val_accuracy : 0.3334\n",
      "round 10 - train_loss : 0.0469 - train_accuracy : 0.0882 - val_loss : 2.2494 - val_accuracy : 0.3007\n",
      "round 11 - train_loss : 0.0439 - train_accuracy : 0.0894 - val_loss : 2.2341 - val_accuracy : 0.3179\n",
      "round 12 - train_loss : 0.0411 - train_accuracy : 0.0902 - val_loss : 2.2473 - val_accuracy : 0.2760\n",
      "round 13 - train_loss : 0.0434 - train_accuracy : 0.0873 - val_loss : 2.2469 - val_accuracy : 0.2762\n",
      "round 14 - train_loss : 0.0373 - train_accuracy : 0.0913 - val_loss : 2.3006 - val_accuracy : 0.1010\n",
      "round 15 - train_loss : 0.0396 - train_accuracy : 0.0867 - val_loss : 2.2965 - val_accuracy : 0.1860\n",
      "round 16 - train_loss : 0.0431 - train_accuracy : 0.0866 - val_loss : 2.2431 - val_accuracy : 0.1132\n",
      "round 17 - train_loss : 0.0385 - train_accuracy : 0.0853 - val_loss : 2.2305 - val_accuracy : 0.2022\n",
      "round 18 - train_loss : 0.0380 - train_accuracy : 0.0865 - val_loss : 2.2345 - val_accuracy : 0.2866\n",
      "round 19 - train_loss : 0.0344 - train_accuracy : 0.0917 - val_loss : 2.2693 - val_accuracy : 0.2022\n",
      "round 20 - train_loss : 0.0354 - train_accuracy : 0.0880 - val_loss : 2.2626 - val_accuracy : 0.2693\n",
      "round 21 - train_loss : 0.0466 - train_accuracy : 0.0848 - val_loss : 2.1718 - val_accuracy : 0.3606\n"
     ]
    }
   ],
   "source": [
    "federater.fit(num_rounds=20, num_epochs=1, val_dl=test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 903,
   "id": "f7c69587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 22 - train_loss : 0.0373 - train_accuracy : 0.0887 - val_loss : 2.1633 - val_accuracy : 0.4132\n",
      "round 23 - train_loss : 0.0390 - train_accuracy : 0.0909 - val_loss : 2.1414 - val_accuracy : 0.3046\n",
      "round 24 - train_loss : 0.0336 - train_accuracy : 0.0916 - val_loss : 2.1610 - val_accuracy : 0.2358\n",
      "round 25 - train_loss : 0.0373 - train_accuracy : 0.0882 - val_loss : 2.1217 - val_accuracy : 0.3317\n",
      "round 26 - train_loss : 0.0323 - train_accuracy : 0.0925 - val_loss : 2.1613 - val_accuracy : 0.2572\n",
      "round 27 - train_loss : 0.0400 - train_accuracy : 0.0866 - val_loss : 2.1418 - val_accuracy : 0.3099\n",
      "round 28 - train_loss : 0.0343 - train_accuracy : 0.0908 - val_loss : 2.1510 - val_accuracy : 0.2642\n",
      "round 29 - train_loss : 0.0366 - train_accuracy : 0.0862 - val_loss : 2.1381 - val_accuracy : 0.2546\n",
      "round 30 - train_loss : 0.0349 - train_accuracy : 0.0878 - val_loss : 2.1111 - val_accuracy : 0.2814\n",
      "round 31 - train_loss : 0.0339 - train_accuracy : 0.0880 - val_loss : 2.1247 - val_accuracy : 0.3089\n",
      "round 32 - train_loss : 0.0403 - train_accuracy : 0.0880 - val_loss : 2.0719 - val_accuracy : 0.3501\n",
      "round 33 - train_loss : 0.0376 - train_accuracy : 0.0902 - val_loss : 2.0318 - val_accuracy : 0.2922\n",
      "round 34 - train_loss : 0.0352 - train_accuracy : 0.0878 - val_loss : 2.0170 - val_accuracy : 0.3560\n",
      "round 35 - train_loss : 0.0374 - train_accuracy : 0.0856 - val_loss : 2.0161 - val_accuracy : 0.3284\n",
      "round 36 - train_loss : 0.0315 - train_accuracy : 0.0933 - val_loss : 2.0423 - val_accuracy : 0.3197\n",
      "round 37 - train_loss : 0.0379 - train_accuracy : 0.0856 - val_loss : 2.0128 - val_accuracy : 0.2954\n",
      "round 38 - train_loss : 0.0368 - train_accuracy : 0.0854 - val_loss : 1.9718 - val_accuracy : 0.3395\n",
      "round 39 - train_loss : 0.0358 - train_accuracy : 0.0859 - val_loss : 1.9647 - val_accuracy : 0.4198\n",
      "round 40 - train_loss : 0.0345 - train_accuracy : 0.0898 - val_loss : 1.9452 - val_accuracy : 0.5155\n",
      "round 41 - train_loss : 0.0327 - train_accuracy : 0.0908 - val_loss : 2.0327 - val_accuracy : 0.2043\n"
     ]
    }
   ],
   "source": [
    "federater.fit(num_rounds=20, num_epochs=1, val_dl=test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 890,
   "id": "4977d916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 890,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0 % 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 866,
   "id": "b0ee4216",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "id": "082df0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = defaultdict(lambda: 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "id": "20280b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp['a'] += 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "id": "10780579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>, {'a': 6})"
      ]
     },
     "execution_count": 881,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "id": "fe4f5c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 6\n"
     ]
    }
   ],
   "source": [
    "for k, v in tmp.items():\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e584a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db25e887",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "id": "6dc41e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.tensorboard.writer.SummaryWriter at 0x7f0610288580>"
      ]
     },
     "execution_count": 838,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "federater.writer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "id": "3fc41c7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.ClientDataset"
      ]
     },
     "execution_count": 749,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(federater.clients[0].dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "id": "b7574cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = federater.validate(test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "id": "ff3bdc31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0078, -0.1566,  0.0084,  0.0535, -0.1603, -0.1524, -0.1051, -0.0275,\n",
       "         0.0413, -0.1712])"
      ]
     },
     "execution_count": 685,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "id": "3332eca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2917)"
      ]
     },
     "execution_count": 682,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(torch.stack(a), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9439bbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "id": "f8dc4c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in test_dl:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "849573e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 575,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dl.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "7feea323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.2842,\n",
       "            0.2249,  1.5105,  2.5415,  2.8088,  2.7960,  2.8088,  2.2869,\n",
       "           -0.1569, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242,  0.6195,  0.8741,  1.3832,  2.2869,\n",
       "            2.7833,  2.7960,  2.7833,  2.7960,  2.7833,  2.7960,  2.7833,\n",
       "            1.9051, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.0296,  0.2249,\n",
       "            1.0141,  1.5105,  2.8215,  2.7960,  2.8088,  2.7960,  2.6815,\n",
       "            2.1596,  0.8741,  0.3522, -0.4242, -0.4242,  0.2249,  2.7960,\n",
       "            2.1596, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  2.0323,  2.7833,\n",
       "            2.7960,  2.7833,  2.7960,  2.7833,  1.4978,  0.4668,  0.0849,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  1.2559,  2.7833,\n",
       "            1.6378, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  1.5105,  2.7960,\n",
       "            2.4269,  0.8741, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242,  0.3649,  2.8088,  2.0196,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  1.0013,  2.7833,\n",
       "            2.1596, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242,  1.9051,  2.7960,  0.9886,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  1.1414,\n",
       "            2.8088,  1.2432, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242,  1.0141,  2.7960,  2.1596, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "            2.2869,  2.7833,  0.0976, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242,  2.5415,  2.7833,  1.1286, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "            0.7468,  2.7960,  1.6505, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242,  0.8741,  2.8088,  2.5415, -0.2842,  0.2249,\n",
       "            0.4922,  1.0013,  1.7778,  0.7341, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "            0.7468,  2.7833,  1.6378, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242,  0.8741,  2.1596,  2.7960,  2.5287,  2.2869,  2.7833,\n",
       "            2.7960,  2.7833,  1.4978, -0.0424, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "            0.3649,  0.8741, -0.4242,  0.0976,  0.4922,  1.5105,  1.7778,\n",
       "            2.7960,  2.8088,  2.7960,  2.8088,  2.7960,  2.4269,  2.1596,\n",
       "            0.8868, -0.1696, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242,  0.0976,  2.1596,  2.6687,  2.7960,  2.7833,  2.7960,\n",
       "            2.7833,  1.7650,  2.7833,  2.7960,  0.7213, -0.1696, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "            0.2377,  2.7960,  2.4269,  1.6378,  0.8741,  0.3522, -0.4242,\n",
       "           -0.4242, -0.4242,  2.1596,  1.9051, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.2969,  0.2122, -0.1696, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242,  2.1596,  2.1596, -0.1696, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242,  2.1596,  2.8088,  0.2122, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242,  2.1596,  2.7960,  0.2122, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242,  2.1723,  2.8215,  0.2122, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242,  2.1596,  2.7960,  0.2122, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.2842,  2.2869,  2.6815,  0.0849, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.2969,  1.7650,  1.6378, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242]]]),\n",
       " 7)"
      ]
     },
     "execution_count": 573,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl.dataset[299]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "6b9b5fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = federater.clients[0].dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "ecd9d185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200"
      ]
     },
     "execution_count": 562,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "09146a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-563-bb0a70b10197>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mi\u001b[0m\u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-553-35e43ce5ca70>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for x,y in dl:\n",
    "    print(i)\n",
    "    i+= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "e1d90e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for x, y in federater.clients[0].dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "1dc2fe02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ClientDataset at 0x7f06671b4df0>"
      ]
     },
     "execution_count": 518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "federater.clients[0].dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "6ced22cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "420dbfce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=10, bias=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b054bad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c7ec44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "41d55a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros((1, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "80085391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0948, 0.0948, 0.0948,  ..., 0.0884, 0.0884, 0.0884])"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.flatten(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "7ad07890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0948, 0.0948, 0.0948,  ..., 0.0884, 0.0884, 0.0884])"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "eb19925e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "20bfbfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    p = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4eddd94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b3241b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb290665",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "167cfa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "dataset_to_transform = {\n",
    "    'mnist': {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307), (0.3081))\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307), (0.3081))\n",
    "        ]),\n",
    "    },\n",
    "    'cifar10': {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    },\n",
    "    'cifar100': {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.507, 0.487, 0.441],\n",
    "                                 std=[0.267, 0.256, 0.276])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.507, 0.487, 0.441],\n",
    "                                 std=[0.267, 0.256, 0.276])\n",
    "        ]),\n",
    "    }\n",
    "}\n",
    "\n",
    "train_ds = datasets.MNIST('data/', train=True, download=True, transform=dataset_to_transform['mnist']['train'])\n",
    "test_ds = datasets.MNIST('data/', train=False, download=True, transform=dataset_to_transform['mnist']['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e87ccd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "88cddaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_train_ds = ClientDataset(train_ds, indices=[0,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ead1079",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcad12d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c95497a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7215634",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300e02c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
