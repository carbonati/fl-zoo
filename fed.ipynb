{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dc275f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.py\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_state(seed=42069):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf3a2050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.py\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "dataset_to_transform = {\n",
    "    'mnist': {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307), (0.3081))\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307), (0.3081))\n",
    "        ]),\n",
    "    },\n",
    "    'cifar10': {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    },\n",
    "    'cifar100': {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.507, 0.487, 0.441],\n",
    "                                 std=[0.267, 0.256, 0.276])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.507, 0.487, 0.441],\n",
    "                                 std=[0.267, 0.256, 0.276])\n",
    "        ]),\n",
    "    }\n",
    "}\n",
    "\n",
    "train_ds = datasets.MNIST('data/', train=True, download=True, transform=dataset_to_transform['mnist']['train'])\n",
    "test_ds = datasets.MNIST('data/', train=False, download=True, transform=dataset_to_transform['mnist']['val'])\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "test_dl = DataLoader(test_ds, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68e5c919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_emnist = datasets.EMNIST('data/', split='balanced', train=True, download=True, transform=dataset_to_transform['mnist']['train'])\n",
    "test_emnist = datasets.EMNIST('data/', split='balanced', train=False, download=True, transform=dataset_to_transform['mnist']['val'])\n",
    "train_cifar = datasets.CIFAR10('data/', train=True, download=True, transform=dataset_to_transform['cifar10']['train'])\n",
    "val_cifar = datasets.CIFAR10('data/', train=False, download=True, transform=dataset_to_transform['cifar10']['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "663d30d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.py\n",
    "\n",
    "\n",
    "class Client:\n",
    "    \"\"\"Base client.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    client_id : str\n",
    "        Id of the client.\n",
    "    dataloader : DataLoader\n",
    "        Local dataset used for training on the client.\n",
    "    device : str, torch.device (default='cpu')\n",
    "        Device type.\n",
    "    \"\"\"\n",
    "    def __init__(self, client_id, dataloader, device='cpu'):\n",
    "        self.client_id = client_id\n",
    "        self.dataloader = dataloader\n",
    "        self.device = device\n",
    "        self._model = None\n",
    "        self._device = None\n",
    "        self._optimizer = None\n",
    "        \n",
    "        self._local_steps = 0\n",
    "        \n",
    "    @property\n",
    "    def model(self):\n",
    "        return self._model\n",
    "    \n",
    "    @model.setter\n",
    "    def model(self, model):\n",
    "        self._model = model\n",
    "        self.device = next(self._model.parameters()).device\n",
    "\n",
    "    @property\n",
    "    def optimizer(self):\n",
    "        return self._optimizer\n",
    "    \n",
    "    @optimizer.setter\n",
    "    def optimizer(self, optimizer):\n",
    "        self._optimizer = optimizer\n",
    "    \n",
    "    @property\n",
    "    def device(self):\n",
    "        return self._device\n",
    "\n",
    "    @device.setter\n",
    "    def device(self, device):\n",
    "        self._device = device\n",
    "    \n",
    "    @property\n",
    "    def local_steps(self):\n",
    "        return self._local_steps\n",
    "    \n",
    "    @local_steps.setter\n",
    "    def local_steps(self, v):\n",
    "        self._local_steps = v\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataloader.dataset)\n",
    "    \n",
    "    def update(self, criterion, num_epochs=1):\n",
    "        \"\"\"Algorithm 1 (ClientUpdate).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        optim_cls : \n",
    "        optim_params :\n",
    "        num_epochs (E) : int\n",
    "            Number of epochs.\n",
    "        criterion : \n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        self.model.to(self.device)\n",
    "        self.local_steps = 0\n",
    "        \n",
    "        total_loss = np.zeros(num_epochs, dtype=np.float32)\n",
    "        total_correct = np.zeros(num_epochs, dtype=np.float32)\n",
    "        for i in range(num_epochs):\n",
    "            for x, y in self.dataloader:\n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                logits = self.model(x)\n",
    "                loss = criterion(logits, y)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                total_loss[i] += loss.item()\n",
    "                total_correct[i] += (logits.argmax(-1) == y).sum().item()\n",
    "                self.local_steps += 1\n",
    "                \n",
    "            if self.scheduler is not None:\n",
    "                self.scheduler.step()\n",
    "            \n",
    "        self.model.to('cpu')\n",
    "        results = {\n",
    "            'loss': total_loss / len(self.dataloader),\n",
    "            'accuracy': total_correct / len(self)\n",
    "        }\n",
    "        return results\n",
    "\n",
    "    def get_gradients(self, criterion):\n",
    "        self.model.eval()\n",
    "        self.model.to(self.device)\n",
    "        # clear gradients\n",
    "        for p in self.model.parameters():\n",
    "            if p.grad is not None and p.requires_grad:\n",
    "                p.grad.zero_()\n",
    "        for x, y in self.dataloader:\n",
    "            x = x.to(self.device)\n",
    "            y = y.to(self.device)\n",
    "\n",
    "            logits = self.model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            \n",
    "            # accumulate the average gradient of each batch\n",
    "            loss.backward()\n",
    "        \n",
    "        # normalize the accumulated gradient across batches\n",
    "        grads = []\n",
    "        for p in self.model.parameters():\n",
    "            # what to do when the model has layers that don't require gradients?\n",
    "            grads.append(p.grad / len(self.dataloader))\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f2a5b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.py\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class SplitDataset(Dataset):\n",
    "    \"\"\"Dataset for a client partitioned by a list of indices.\"\"\"\n",
    "    def __init__(self, dataset, indices):\n",
    "        self.dataset = dataset\n",
    "        self.indices = list(indices)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.dataset[self.indices[index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6085f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling.py\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def get_iid_shards(labels,\n",
    "                   num_clients=100,\n",
    "                   client_ids=None,\n",
    "                   seed=None):\n",
    "    \"\"\"Returns a homogeneous (IID) mapping of client ID's to sample indices of a dataset.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    labels : list, np.ndarray, torch.Tensor\n",
    "        List of class labels for a dataset.\n",
    "    num_clients : int (default=100)\n",
    "        Number of clients used for training.\n",
    "    client_ids : list, np.ndarray (default=None)\n",
    "        List of client ID's. If `None`, defaults to a `range(num_clients)`.\n",
    "    seed : int (default=None)\n",
    "        Random state.\n",
    "       \n",
    "    Notes\n",
    "    -----\n",
    "    ```\n",
    "    num_samples_per_client = len(labels) // num_clients # 60000 / 100 = 600 for MNIST\n",
    "    ```\n",
    "    \"\"\"\n",
    "    random_state = np.random.RandomState(seed)\n",
    "    client_ids = client_ids or list(range(num_clients))\n",
    "    \n",
    "    # randomly shuffle sample indices to generate IID (homogeneous) clients\n",
    "    indices_shuffled = random_state.choice(range(len(labels)), len(labels), replace=False)\n",
    "    num_samples_per_client = len(indices_shuffled) // num_clients\n",
    "\n",
    "    # assign `num_samples_per_client` random samples to each client\n",
    "    for i, client_id in enumerate(client_ids):\n",
    "        client_indices = indices_shuffled[i * num_samples_per_client : (i + 1) * num_samples_per_client]\n",
    "        client_to_shard[client_id] = client_indices\n",
    "    \n",
    "    return client_to_shard\n",
    "    \n",
    "    \n",
    "# def get_non_iid_shards(labels, \n",
    "#                        num_clients=100,\n",
    "#                        client_ids=None,\n",
    "#                        shard_size=300,\n",
    "#                        drop_last=False,\n",
    "#                        seed=None):\n",
    "#     \"\"\"Returns a heterogeneous (non-IID) mapping of client ID's to sample indices of a dataset.\n",
    "    \n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     labels : list, np.ndarray, torch.Tensor\n",
    "#         List of class labels for a dataset.\n",
    "#     num_clients : int (default=100)\n",
    "#         Number of clients used for training.\n",
    "#     client_ids : list, np.ndarray (default=None)\n",
    "#         List of client ID's. If `None`, defaults to a `range(num_clients)`.\n",
    "#     shard_size : int (default=300)\n",
    "#         Size of each shard to split labels by.\n",
    "#     seed : int (default=None)\n",
    "#         Random state.\n",
    "       \n",
    "#     Notes\n",
    "#     -----\n",
    "#     ```\n",
    "#     num_shards = len(labels) // shard_size # 60000 / 300 = 200 for MNIST\n",
    "#     num_shards_per_client = num_shards // num_clients # 200 / 100 = 2 for MNIST\n",
    "#     ```\n",
    "#     \"\"\"\n",
    "#     random_state = np.random.RandomState(seed)\n",
    "#     client_ids = client_ids or list(range(num_clients))\n",
    "#     classes = np.unique(labels)\n",
    "#     num_classes = len(classes)\n",
    "\n",
    "#     label_to_indices = {}\n",
    "#     client_to_shard = {}\n",
    "#     shards = []\n",
    "#     shard_labels = []\n",
    "#     # map each class to the sample indices in the dataset\n",
    "#     for label in classes:\n",
    "#         label_to_indices[label] = np.where(labels == label)[0]\n",
    "\n",
    "#     # split each class labels indices to shards of size `shard_size`\n",
    "#     for label in classes:\n",
    "#         indices = label_to_indices[label]\n",
    "#         num_extra = len(indices) % shard_size\n",
    "#         # (num_shards_per_label, shard_size)\n",
    "#         label_shards = indices[:-num_extra].reshape(-1, shard_size).tolist()\n",
    "\n",
    "#         if num_extra > 0 and not drop_last:\n",
    "#             label_shards.append(indices[-num_extra:])\n",
    "\n",
    "#         # store the shards in a list to resample and assign to a client\n",
    "#         shard_labels.extend([label] * len(label_shards))\n",
    "#         shards.extend(label_shards)\n",
    "\n",
    "#     num_shards = len(shard_labels) # number of shards of size `shard_size`\n",
    "#     num_shards_per_client = num_shards // num_clients # how many shards fit in each client\n",
    "\n",
    "#     # shuffle the shards and assign `num_shards_per_client` to each client\n",
    "#     # each client should ideally have `num_shards_per_client` from a different class lablel\n",
    "#     shard_indices_shuffled = random_state.choice(range(num_shards), \n",
    "#                                                  num_shards,\n",
    "#                                                  replace=False)\n",
    "#     for i, client_id in enumerate(client_ids):\n",
    "#         client_to_shard[client_id] = []\n",
    "#         shard_indices = shard_indices_shuffled[i * num_shards_per_client : (i + 1) * num_shards_per_client]\n",
    "#         for idx in shard_indices:\n",
    "#             client_to_shard[client_id].extend(shards[idx])\n",
    "            \n",
    "#     return client_to_shard\n",
    "\n",
    "\n",
    "def get_client_shards(labels, \n",
    "                      is_iid=True,\n",
    "                      num_clients=100,\n",
    "                      client_ids=None,\n",
    "                      shard_size=300,\n",
    "                      seed=None):\n",
    "    \"\"\"Returns a heterogeneous (non-IID) mapping of client ID's to sample indices of a dataset.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    labels : list, np.ndarray, torch.Tensor\n",
    "        List of class labels for a dataset.\n",
    "    num_clients : int (default=100)\n",
    "        Number of clients used for training.\n",
    "    client_ids : list, np.ndarray (default=None)\n",
    "        List of client ID's. If `None`, defaults to a `range(num_clients)`.\n",
    "    shard_size : int (default=300)\n",
    "        Size of each shard to split labels by.\n",
    "    seed : int (default=None)\n",
    "        Random state.\n",
    "       \n",
    "    Notes\n",
    "    -----\n",
    "    ```\n",
    "    num_shards = len(labels) // shard_size # 60000 / 300 = 200 for MNIST\n",
    "    num_shards_per_client = num_shards // num_clients # 200 / 100 = 2 for MNIST\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    random_state = np.random.RandomState(seed)\n",
    "    client_ids = client_ids or list(range(num_clients))\n",
    "    client_to_shard = {c_id: [] for c_id in client_ids}\n",
    "    if is_iid:\n",
    "        # randomly shuffle sample indices if IID (homogoneous)\n",
    "        sample_indices = random_state.choice(range(len(labels)), len(labels), replace=False)\n",
    "    else:\n",
    "        # sort sample indices by by label if non-IID (heterogeneous)\n",
    "        sample_indices = np.argsort(labels).tolist()\n",
    "\n",
    "    num_shards = len(labels) // shard_size\n",
    "    num_shards_per_client = num_shards // num_clients # how many shards fit in each client\n",
    "    shard_indices = set(range(num_shards))\n",
    "\n",
    "    for i, client_id in enumerate(client_ids):\n",
    "        client_shard_indices = random_state.choice(list(shard_indices), \n",
    "                                                   num_shards_per_client, \n",
    "                                                   replace=False)\n",
    "        for shard_idx in client_shard_indices:\n",
    "            client_to_shard[client_id].extend(\n",
    "                sample_indices[shard_idx*shard_size : (shard_idx+1)*shard_size]\n",
    "            )\n",
    "            shard_indices.remove(shard_idx)\n",
    "            \n",
    "    return client_to_shard\n",
    "\n",
    "\n",
    "def get_client_data(dataset, \n",
    "                    num_clients,\n",
    "                    client_ids=None,\n",
    "                    is_iid=True,\n",
    "                    shard_size=300,\n",
    "                    batch_size=32,\n",
    "                    shuffle=True,\n",
    "                    seed=None,\n",
    "                    **kwargs):\n",
    "    \"\"\"Returns a mapping of client ID's to their corresponding train & validation dataloaders.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    is_iid : bool (default=True)\n",
    "        Boolean whether data is IID (Homogeneous) or non-IID (Heterogeneous)\n",
    "    **kwargs\n",
    "        Additional parameters used when instantiating each clients dataloader \n",
    "        \n",
    "    \"\"\"\n",
    "    client_to_data = {}\n",
    "    labels = dataset.targets\n",
    "    client_to_shard = get_client_shards(\n",
    "        labels,\n",
    "        is_iid=is_iid,\n",
    "        num_clients=num_clients,\n",
    "        client_ids=client_ids,\n",
    "        shard_size=shard_size,\n",
    "        seed=seed\n",
    "    )\n",
    "    # iterate through each client and create train/val dataloaders using the shard indices\n",
    "    for k in client_to_shard.keys():\n",
    "        client_indices = client_to_shard[k]\n",
    "        client_dataset = SplitDataset(dataset, client_indices)\n",
    "        # reseed workers for reproducibility\n",
    "#         g = torch.Generator()\n",
    "#         g.manual_seed(0)\n",
    "        client_to_data[k] = DataLoader(\n",
    "            client_dataset, \n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle, \n",
    "#             worker_init_fn=seed_worker,\n",
    "#             generator=g,\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "    return client_to_data\n",
    "\n",
    "\n",
    "def get_clients(dataset,\n",
    "                num_clients=100,\n",
    "                client_ids=None,\n",
    "                is_iid=True,\n",
    "                shard_size=300,\n",
    "                drop_last=True,\n",
    "                batch_size=32,\n",
    "                num_workers=0,\n",
    "                device='cpu',\n",
    "                **kwargs):\n",
    "    client_to_data = get_client_data(\n",
    "        dataset,\n",
    "        num_clients=num_clients,\n",
    "        client_ids=client_ids,\n",
    "        is_iid=is_iid,\n",
    "        shard_size=shard_size,\n",
    "        drop_last=drop_last,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        **kwargs\n",
    "    )\n",
    "    clients = {\n",
    "        k: Client(k, dl, device=device)\n",
    "        for k, dl\n",
    "        in client_to_data.items()\n",
    "    }\n",
    "    return clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c7ce54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.py\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \"\"\"CNN described in \"Communication-Efficient Learning of Deep Networks\n",
    "    from Decentralized Data\" (https://arxiv.org/pdf/1602.05629.pdf).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    in_features : int (default=1)\n",
    "        Number of channels in the input image.\n",
    "    num_classes : int (default=10)\n",
    "        Number of class labels.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features=1, num_classes=10):\n",
    "        super(CNN, self).__init__()\n",
    "        self._in_features = in_features\n",
    "        self._num_classes = num_classes\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(self._in_features,\n",
    "                               32,\n",
    "                               kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32,\n",
    "                               64,\n",
    "                               kernel_size=5)\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, self._num_classes)\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(2,2))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = x.reshape(len(x), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class MLP(nn.Module):\n",
    "    \"\"\"CNN described in \"Communication-Efficient Learning of Deep Networks\n",
    "    from Decentralized Data\" (https://arxiv.org/pdf/1602.05629.pdf).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    in_features : int (default=784)\n",
    "        Number input features.\n",
    "    hidden_dim : int (cdefault=200)\n",
    "        Number of hidden units.\n",
    "    num_classes : int (default=10)\n",
    "        Number of class labels.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features=784, hidden_dim=200, num_classes=10):\n",
    "        super(MLP, self).__init__()\n",
    "        self._in_features = in_features\n",
    "        self._hidden_dim = hidden_dim\n",
    "        self._num_classes = num_classes\n",
    "        \n",
    "        self.fc1 = nn.Linear(self._in_features, self._hidden_dim)\n",
    "        self.fc2 = nn.Linear(self._hidden_dim, self._num_classes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "073f00e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# servers (aggregates parameters from local solvers)\n",
    "    # base.py\n",
    "    # fedavg.py\n",
    "    # fedprox.py\n",
    "    # feddane.py\n",
    "    # fednova.py\n",
    "    # fedopt.py\n",
    "    # scaffold.py\n",
    "# optimizers (local solvers)\n",
    "    # fedprox.py\n",
    "    # feddane.py\n",
    "    # fedopt.py\n",
    "    # scaffold.py\n",
    "    # fednova.py\n",
    "# utils\n",
    "    # client.py\n",
    "    # sampling.py\n",
    "# models\n",
    "    # mnist\n",
    "        # cnn.py\n",
    "        # mlp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f8d700a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a server for each algo\n",
    "# create a client for each algo\n",
    "# create an optimizer for each algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "bf21843f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizers/fedprox.py\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "\n",
    "class FedProxSolver(Optimizer):\n",
    "    \"\"\"Implements FedProx local solver.\n",
    "    \n",
    "    This adds a proximal term to any clients optimizer.\n",
    "    \n",
    "    This wrapper allows us to pass in any torch.optim.Optimizer for\n",
    "    a given client, not limited to SGD as originally proposed.\n",
    "    \n",
    "    Args:\n",
    "        optimizer (torch.optim.Optimizer): local optimizer.\n",
    "        mu (float): proximal term weight (default: 0)\n",
    "\n",
    "    __ https://arxiv.org/pdf/1812.06127.pdf\n",
    "        \n",
    "    Example:\n",
    "        >>> # train a model locally for a client\n",
    "        >>> client_optimizer = torch.optim.SGD(model.parametesr(), lr=0.1)\n",
    "        >>> client_optimizer = FedProxLocal(client_optimizer, mu=0.1)\n",
    "        >>> client_optimizer.zero_grad()\n",
    "        >>> loss_fn(model(input), target).backward()\n",
    "        >>> client_optimizer.step()\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 optimizer,\n",
    "                 mu=0):\n",
    "        if mu < 0.0:\n",
    "            raise ValueError(f'Invalid mu value: {mu}')\n",
    "        self.optimizer = optimizer\n",
    "        self.mu = mu\n",
    "        self.param_groups = self.optimizer.param_groups\n",
    "        self.state = self.optimizer.state\n",
    "#         self.state = defaultdict(dict)\n",
    "    \n",
    "    def _update(self, group):\n",
    "        \"\"\"Applies a proximal update to a parameter group.\"\"\"\n",
    "        for p in group['params']:\n",
    "            if p.grad is None:\n",
    "                continue\n",
    "            state = self.state[p]\n",
    "            p.data.add_(state['proximal'], alpha=-group['lr'])\n",
    "        \n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        closure : bool\n",
    "            A closure that reevaluates the model and returns the loss.\n",
    "        \"\"\"\n",
    "        # set the initial (global) weights and proximal term before we update the client optimizer\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                state = self.state[p]\n",
    "                if 'initial_weights' not in state:\n",
    "                    state['initial_weights'] = torch.clone(p.data).detach()\n",
    "                state['proximal'] = self.mu * (p.data - state['initial_weights'])\n",
    "\n",
    "        loss = self.optimizer.step(closure=closure)\n",
    "        for group in self.param_groups:\n",
    "            self._update(group)\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "class FedDaneSolver(Optimizer):\n",
    "    \"\"\"Implements FedDane local solver.\n",
    "    \n",
    "    Args:\n",
    "        optimizer (torch.optim.Optimizer): local optimizer.\n",
    "        mu (float): proximal term weight (default: 0)\n",
    "    \n",
    "        \n",
    "    Example:\n",
    "        >>> # train a model locally for a client\n",
    "        >>> client_optimizer = torch.optim.SGD(model.parametesr(), lr=0.1)\n",
    "        >>> client_optimizer = FedDaneLocal(client_optimizer, average_gradients, mu=0.1)\n",
    "        >>> client_optimizer.zero_grad()\n",
    "        >>> loss_fn(model(input), target).backward()\n",
    "        >>> client_optimizer.step()\n",
    "    \n",
    "    __ https://arxiv.org/pdf/2001.01920.pdf\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 optimizer,\n",
    "                 average_gradients,\n",
    "                 mu=0):\n",
    "        if mu < 0.0:\n",
    "            raise ValueError(f'Invalid mu value: {mu}')\n",
    "        self.optimizer = optimizer\n",
    "        self.average_gradients = average_gradients\n",
    "        self.mu = mu\n",
    "        self.param_groups = self.optimizer.param_groups\n",
    "        self.state = self.optimizer.state\n",
    "    \n",
    "    def _update(self, group):\n",
    "        \"\"\"Applies a proximal update to a parameter group.\"\"\"\n",
    "        for p in group['params']:\n",
    "            if p.grad is None:\n",
    "                continue\n",
    "            state = self.state[p]\n",
    "            p.data.add_(state['proximal'] + state['grad_delta'], alpha=-group['lr'])\n",
    "        \n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        closure : bool\n",
    "            A closure that reevaluates the model and returns the loss.\n",
    "        \"\"\"\n",
    "        # set the initial (global) weights and proximal term before we update the client optimizer\n",
    "        for group in self.param_groups:\n",
    "            for i, p in enumerate(group['params']):\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                state = self.state[p]\n",
    "                if 'initial_weights' not in state:\n",
    "                    state['initial_weights'] = torch.clone(p.data).detach()\n",
    "                state['proximal'] = self.mu * (p.data - state['initial_weights'])\n",
    "                if 'average_gradient' not in state:\n",
    "                    state['average_gradient'] = torch.clone(self.average_gradients[i]).detach()\n",
    "                state['grad_delta'] = state['average_gradient'] - p.grad.data\n",
    "\n",
    "        loss = self.optimizer.step(closure=closure)\n",
    "        for group in self.param_groups:\n",
    "            self._update(group)\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "class FedNovaSolver(Optimizer):\n",
    "    \"\"\"Implements FedNova local solver.\n",
    "    \n",
    "    Args:\n",
    "        optimizer (torch.optim.Optimizer): local optimizer.\n",
    "        mu (float): proximal term weight (default: 0)\n",
    "\n",
    "    __ https://arxiv.org/pdf/2007.07481.pdf\n",
    "        \n",
    "    Example:\n",
    "        >>> # train a model locally for a client\n",
    "        >>> client_optimizer = torch.optim.SGD(model.parametesr(), lr=0.1)\n",
    "        >>> client_optimizer = FedNovaLocal(client_optimizer, mu=0.1)\n",
    "        >>> client_optimizer.zero_grad()\n",
    "        >>> loss_fn(model(input), target).backward()\n",
    "        >>> client_optimizer.step()\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 optimizer,\n",
    "                 mu=0):\n",
    "        if mu < 0.0:\n",
    "            raise ValueError(f'Invalid mu value: {mu}')\n",
    "        self.optimizer = optimizer\n",
    "        self.mu = mu\n",
    "        self.param_groups = self.optimizer.param_groups\n",
    "        self.state = self.optimizer.state\n",
    "#         self.state = defaultdict(dict)\n",
    "    \n",
    "    def _update(self, group):\n",
    "        \"\"\"Applies a proximal update to a parameter group.\"\"\"\n",
    "        for p in group['params']:\n",
    "            if p.grad is None:\n",
    "                continue\n",
    "            state = self.state[p]\n",
    "            p.data.add_(state['proximal'], alpha=-group['lr'])\n",
    "        \n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        closure : bool\n",
    "            A closure that reevaluates the model and returns the loss.\n",
    "        \"\"\"\n",
    "        # set the initial (global) weights and proximal term before we update the client optimizer\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                state = self.state[p]\n",
    "                if 'initial_weights' not in state:\n",
    "                    state['initial_weights'] = torch.clone(p.data).detach()\n",
    "                state['proximal'] = self.mu * (p.data - state['initial_weights'])\n",
    "\n",
    "        # update the weights and gradient with the client optimizer\n",
    "        loss = self.optimizer.step(closure=closure)\n",
    "        \n",
    "        # update the weights by adding the (negative) proximal term\n",
    "        for group in self.param_groups:\n",
    "            self._update(group)\n",
    "        \n",
    "        # accumualte gradients after calculating loss\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                \n",
    "                momentum = group.get('momentum', 0)\n",
    "                \n",
    "                state = self.state[p]\n",
    "                if 'local_step' not in state:\n",
    "                    state['local_step'] = 0\n",
    "                state['local_step'] += 1\n",
    "                \n",
    "                # momentum (1 - p^t) / (1 - p)\n",
    "                a = (1 - momentum ** state['local_step']) / (1 - momentum)\n",
    "                # proximal (1 - lr * mu)^t\n",
    "                a *= (1 - group['lr'] * self.mu) ** (state['local_step']-1)\n",
    "                # record the norm factor (a) to divide the l1-norm during aggregation\n",
    "                if 'norm_factor' not in state:\n",
    "                    state['norm_factor'] = []\n",
    "                state['norm_factor'].append(a)\n",
    "                \n",
    "                if 'cgrad' not in state:\n",
    "                    state['cgrad'] = torch.clone(p.grad.data).detach()\n",
    "                    state['cgrad'].mul_(group['lr']) # do we need the lr ?\n",
    "                    state['cgrad'].mul_(a) # G * a\n",
    "                else:\n",
    "                    state['cgrad'].add_(p.grad.data, alpha=group['lr'])\n",
    "                    state['cgrad'].mul_(a) # G * a\n",
    "                    \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "044a2135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# servers/base.py\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "\n",
    "\n",
    "class BaseFederater:\n",
    "    \"\"\"Base Federater.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "    dataset : torch.utils.data.Dataset\n",
    "    num_clients (K) : int (default=100)\n",
    "        Number of clients to partition `dataset`.\n",
    "    batch_size (B) : int, dict[str, int] (defualt=32)\n",
    "        Number of samples per batch to load on each client. \n",
    "        Can be a dictionary mapping each client ID to it's corresponding batch size\n",
    "        to allow for various batch sizes across clients.\n",
    "    shard_size : int (default=300)\n",
    "    is_iid : bool (default=False)\n",
    "    drop_last : bool (default=True)\n",
    "    num_workers : int (default=0)\n",
    "    device : str (default='cpu')\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 model,\n",
    "                 clients,\n",
    "                 client_optimizer_cls,\n",
    "                 client_optimizer_params,\n",
    "                 server_optimizer=None,\n",
    "                 client_scheduler_cls=None,\n",
    "                 client_scheduler_params=None,\n",
    "                 server_scheduler=None,\n",
    "                 C=0.1,\n",
    "                 seed=None,\n",
    "                 writer=None):\n",
    "        self.model = model\n",
    "        self.clients = clients\n",
    "        self.client_optimizer_cls = client_optimizer_cls\n",
    "        self.client_optimizer_params = client_optimizer_params\n",
    "        self.server_optimizer = server_optimizer\n",
    "        self.client_scheduler_cls = client_scheduler_cls\n",
    "        self.client_scheduler_params = client_scheduler_params\n",
    "        self.server_scheduler = server_scheduler\n",
    "        self.C = C\n",
    "        self.writer = writer or SummaryWriter()\n",
    "        \n",
    "        self.client_ids = list(self.clients.keys())\n",
    "        self.num_clients = len(self.clients)\n",
    "        self.num_samples = sum([len(c) for c in self.clients.values()]) # n\n",
    "        self.client_weights = [len(c) / self.num_samples for c in self.clients.values()]\n",
    "        \n",
    "        self.device = next(self.model.parameters()).device\n",
    "        self._global_round = 0\n",
    "        self._random_state = np.random.RandomState(seed)\n",
    "        \n",
    "    @property\n",
    "    def global_round(self):\n",
    "        return self._global_round\n",
    "\n",
    "    @global_round.setter\n",
    "    def global_round(self, global_round):\n",
    "        self._global_round = global_round\n",
    "    \n",
    "    def aggregate(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def update(self, \n",
    "               client_ids, \n",
    "               criterion,\n",
    "               num_epochs, \n",
    "               straggler_rate=0):\n",
    "        \"\"\"Performs a full communication round.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        client_ids (S_t): list, np.ndarray\n",
    "            List of client ID's to train.\n",
    "        criterion : nn.Module\n",
    "            Loss function to optimize on each client.\n",
    "        num_epochs (E): int\n",
    "            Number of epochs to train on each client.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        metrics_dict : dict\n",
    "            Dictionary mapping each metric to the average score across `client_ids`\n",
    "        \"\"\"\n",
    "        # send the global model parameters to each client\n",
    "        self.send_model()\n",
    "        \n",
    "        metrics_dict = defaultdict(lambda: 0)\n",
    "        for k in client_ids:\n",
    "            # instantiate client optimizer and scheduler\n",
    "            client = self.clients[k]\n",
    "            client.optimizer = self.get_client_optimizer(client)\n",
    "            client.scheduler = self.get_client_scheduler(client.optimizer)\n",
    "            \n",
    "            # for heterogeneity experiments we can train clients for varying epochs (stragglers)\n",
    "            if self._random_state.random() < straggler_rate:\n",
    "                client_epochs = self._random_state.choice(range(1, num_epochs+1))\n",
    "            else:\n",
    "                client_epochs = num_epochs\n",
    "                \n",
    "            # update the client weights and record the local training metrics\n",
    "            client_metrics_dict = client.update(\n",
    "                criterion,\n",
    "                num_epochs=client_epochs,\n",
    "            )\n",
    "            \n",
    "            # update the summary writer and record loss/acc from the client\n",
    "            for metric, values in client_metrics_dict.items():\n",
    "                self.writer.add_scalar(f'client/{k}/{metric}', values[-1], self.global_round)\n",
    "                metrics_dict[metric] += values[-1] / len(client_ids)\n",
    "        \n",
    "        # aggregate the parameters of the local solvers\n",
    "        self.aggregate()\n",
    "        if self.server_scheduler is not None:\n",
    "            self.server_scheduler.step()\n",
    "        \n",
    "        return metrics_dict\n",
    "        \n",
    "    def fit(self, \n",
    "            num_rounds,\n",
    "            criterion, \n",
    "            num_epochs,\n",
    "            val_dl=None,\n",
    "            straggler_rate=0,\n",
    "            eval_every_n=1):\n",
    "        # subset a sample of `m` clients each round\n",
    "        m = max(int(np.ceil(self.num_clients * self.C)), 1)\n",
    "        for t in range(num_rounds):\n",
    "            self.global_round += 1\n",
    "            \n",
    "            # update a subset of clients with the local solver\n",
    "            S = self._random_state.choice(self.client_ids, m, replace=False)\n",
    "            train_metrics = self.update(client_ids=S, \n",
    "                                        criterion=criterion,\n",
    "                                        num_epochs=num_epochs, \n",
    "                                        straggler_rate=straggler_rate)\n",
    "            \n",
    "            if eval_every_n is not None and t % eval_every_n == 0 and val_dl is not None:\n",
    "                template_str = f'round {self.global_round}'\n",
    "                val_metrics = self.validate(val_dl, criterion)\n",
    "                for metric, value in train_metrics.items():\n",
    "                    self.writer.add_scalar(f'train/{metric}', value, self.global_round)\n",
    "                    template_str += f' - train_{metric} : {value:0.4f}'\n",
    "                for metric, value in val_metrics.items():\n",
    "                    self.writer.add_scalar(f'val/{metric}', value, self.global_round)\n",
    "                    template_str += f' - val_{metric} : {value:0.4f}'\n",
    "                \n",
    "                print(template_str)\n",
    "    \n",
    "    def get_client_optimizer(self, client):\n",
    "        \"\"\"Returns a client optimizer (local solver).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        params : iterable\n",
    "            Client parameters to optimize\n",
    "        optimizer_params : dict\n",
    "            Client optimizer hyperparameters\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        torch.optim.Optimizer\n",
    "        \"\"\"\n",
    "        optimizer_params = self.client_optimizer_params or {}\n",
    "        return self.client_optimizer_cls(client.model.parameters(), **optimizer_params)\n",
    "    \n",
    "    def get_client_scheduler(self, optimizer):\n",
    "        \"\"\"Returns a LR scheduler for a client optimizer.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        optimizer : torch.optim.Optimizer\n",
    "            Client optimizer\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        torch.optim.lr_scheduler._LRScheduler or None\n",
    "            Client LR scheduler, or None if not specified\n",
    "        \"\"\"\n",
    "        if self.client_scheduler_cls is not None:\n",
    "            scheduler_params = self.client_scheduler_params or {}\n",
    "            return self.client_scheduler_cls(optimizer, **scheduler_params)\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def validate(self, val_dl, criterion):\n",
    "#         self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        loss = 0 \n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_dl:\n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "                logits = self.model(x)\n",
    "                correct += (logits.argmax(-1) == y).sum().item()\n",
    "                loss += criterion(logits, y).item()\n",
    "                \n",
    "        results = {\n",
    "            'loss': loss / len(val_dl),\n",
    "            'accuracy': correct / len(val_dl.dataset)\n",
    "        }\n",
    "        return results\n",
    "    \n",
    "    def send_model(self, client_ids=None):\n",
    "        \"\"\"Send the current state of the global model to each client.\"\"\"\n",
    "        if client_ids is None:\n",
    "            client_ids = self.client_ids\n",
    "        for client_id in client_ids:\n",
    "            self.clients[client_id].model = deepcopy(self.model)\n",
    "            \n",
    "    def get_gradients(self, client_ids, criterion):\n",
    "        self.send_model(client_ids)\n",
    "        grads = []\n",
    "        for k, client_id in enumerate(client_ids):\n",
    "            client = self.clients[client_id]\n",
    "            client_grads = client.get_gradients(criterion)\n",
    "            grads.append(client_grads)\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d847fa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FedAvg(BaseFederater):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model,\n",
    "                 clients,\n",
    "                 client_optimizer_cls,\n",
    "                 client_optimizer_params,\n",
    "                 client_scheduler_cls=None,\n",
    "                 client_scheduler_params=None,\n",
    "                 C=0.1,\n",
    "                 seed=None,\n",
    "                 writer=None):\n",
    "        super().__init__(model,\n",
    "                         clients,\n",
    "                         client_optimizer_cls=client_optimizer_cls,\n",
    "                         client_optimizer_params=client_optimizer_params,\n",
    "                         client_scheduler_cls=client_scheduler_cls,\n",
    "                         client_scheduler_params=client_scheduler_params,\n",
    "                         C=C,\n",
    "                         seed=seed,\n",
    "                         writer=writer)\n",
    "    \n",
    "    def aggregate(self):\n",
    "        global_state = {} # self.model.state_dict()\n",
    "        for k, (client_id, client) in enumerate(self.clients.items()):\n",
    "            local_state = client.model.state_dict()\n",
    "            for layer_name, param in local_state.items():\n",
    "                if k == 0:\n",
    "                    global_state[layer_name] = self.client_weights[k] * param\n",
    "                else:\n",
    "                    global_state[layer_name] += self.client_weights[k] * param\n",
    "\n",
    "        self.model.load_state_dict(global_state)\n",
    "                    \n",
    "\n",
    "class FedProx(BaseFederater):\n",
    "    def __init__(self, \n",
    "                 model,\n",
    "                 clients,\n",
    "                 client_optimizer_cls,\n",
    "                 client_optimizer_params,\n",
    "                 server_optimizer,\n",
    "                 mu=0,\n",
    "                 client_scheduler_cls=None,\n",
    "                 client_scheduler_params=None,\n",
    "                 server_scheduler=None,\n",
    "                 C=0.1,\n",
    "                 seed=None,\n",
    "                 writer=None):\n",
    "        super().__init__(model,\n",
    "                         clients,\n",
    "                         client_optimizer_cls,\n",
    "                         client_optimizer_params,\n",
    "                         server_optimizer=server_optimizer,\n",
    "                         server_scheduler=server_scheduler,\n",
    "                         client_scheduler_cls=client_scheduler_cls,\n",
    "                         client_scheduler_params=client_scheduler_params,\n",
    "                         C=C,\n",
    "                         seed=seed,\n",
    "                         writer=writer)\n",
    "        self.mu = mu\n",
    "            \n",
    "    def get_client_optimizer(self, client):\n",
    "        optimizer_params = self.client_optimizer_params or {}\n",
    "        client_optimizer = self.client_optimizer_cls(client.model.parameters(), **optimizer_params)\n",
    "        return FedProxSolver(client_optimizer, mu=self.mu)\n",
    "    \n",
    "    def aggregate(self):\n",
    "        self.server_optimizer.zero_grad()\n",
    "        for k, client in enumerate(self.clients.values()):\n",
    "            for p_server, p_client in zip(self.model.parameters(), client.model.parameters()):\n",
    "                if p_server.requires_grad:\n",
    "                    if k == 0:\n",
    "                        p_server.grad = self.client_weights[k] * (p_server.data - p_client.data)\n",
    "                    else:\n",
    "                        p_server.grad.data.add_(p_server.data - p_client.data, alpha=self.client_weights[k])\n",
    "        \n",
    "        self.server_optimizer.step()\n",
    "\n",
    "    \n",
    "class FedOpt(BaseFederater):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 clients,\n",
    "                 client_optimizer_cls,\n",
    "                 client_optimizer_params,\n",
    "                 server_optimizer,\n",
    "                 server_scheduler=None,\n",
    "                 client_scheduler_cls=None,\n",
    "                 client_scheduler_params=None,\n",
    "                 C=0.1,\n",
    "                 seed=None,\n",
    "                 writer=None):\n",
    "        super().__init__(model,\n",
    "                         clients,\n",
    "                         client_optimizer_cls,\n",
    "                         client_optimizer_params,\n",
    "                         server_optimizer=server_optimizer,\n",
    "                         server_scheduler=server_scheduler,\n",
    "                         client_scheduler_cls=client_scheduler_cls,\n",
    "                         client_scheduler_params=client_scheduler_params,\n",
    "                         C=C,\n",
    "                         seed=seed,\n",
    "                         writer=writer)\n",
    "    \n",
    "    def aggregate(self):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        self.server_optimizer.zero_grad()\n",
    "        # iterate through each client\n",
    "        for k, client in enumerate(self.clients.values()):\n",
    "            for p_server, p_client in zip(self.model.parameters(), client.model.parameters()):\n",
    "                if p_server.requires_grad:\n",
    "                    if k == 0:\n",
    "                        p_server.grad = client_weights[k] * (p_server.data - p_client.data)\n",
    "                    else:\n",
    "                        p_server.grad.add_(p_server.data - p_client.data, alpha=self.client_weights[k])\n",
    "        \n",
    "        self.server_optimizer.step()\n",
    "        \n",
    "        \n",
    "class FedNova(BaseFederater):\n",
    "    \"\"\"FedNova\n",
    "    \n",
    "    https://arxiv.org/pdf/2007.07481.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 clients,\n",
    "                 server_optimizer,\n",
    "                 client_optimizer_cls,\n",
    "                 client_optimizer_params,\n",
    "                 mu=0,\n",
    "                 server_scheduler=None,\n",
    "                 client_scheduler_cls=None,\n",
    "                 client_scheduler_params=None,\n",
    "                 C=0.1,\n",
    "                 seed=None,\n",
    "                 writer=None):\n",
    "        super().__init__(model,\n",
    "                         clients,\n",
    "                         client_optimizer_cls=client_optimizer_cls,\n",
    "                         client_optimizer_params=client_optimizer_params,\n",
    "                         server_optimizer=server_optimizer,\n",
    "                         server_scheduler=server_scheduler,\n",
    "                         client_scheduler_cls=client_scheduler_cls,\n",
    "                         client_scheduler_params=client_scheduler_params,\n",
    "                         C=C,\n",
    "                         seed=seed,\n",
    "                         writer=writer)\n",
    "        self.mu = mu\n",
    "        \n",
    "    def get_client_optimizer(self, client):\n",
    "        optimizer_params = self.client_optimizer_params or {}\n",
    "        client_optimizer = self.client_optimizer_cls(client.model.parameters(), **optimizer_params)\n",
    "        client_optimizer = FedNovaSolver(client_optimizer, mu=self.mu)\n",
    "        return client_optimizer\n",
    "    \n",
    "    def aggregate(self):\n",
    "        \"\"\" \"\"\"\n",
    "        \n",
    "        self.server_optimizer.zero_grad()\n",
    "        # iterate through each client and set gradients\n",
    "        for k, client in enumerate(self.clients.values()):\n",
    "            # skip clients with no optimizer\n",
    "            # we may want to use the weights of the local model instead\n",
    "            if client.optimizer is None:\n",
    "                continue\n",
    "            for group_server, group_client in zip(self.server_optimizer.param_groups, \n",
    "                                                  client.optimizer.param_groups):\n",
    "                for p_server, p_client in zip(group_server['params'], group_client['params']):\n",
    "                    if p_server.requires_grad:\n",
    "                        state = client.optimizer.state[p_client]\n",
    "                        w = self.client_weights[k]\n",
    "                        G_a = state['cgrad']\n",
    "                        a = torch.tensor(state['norm_factor'])\n",
    "                        d = G_a / a.abs().sum()\n",
    "                        tau_eff = client.local_steps\n",
    "                        if p_server.grad is None:\n",
    "                            p_server.grad = tau_eff * w * d  # need to take lr off of G ? jk lr is necessary for client (local)\n",
    "                        else:\n",
    "                            p_server.grad.add_(d, alpha=tau_eff * w)\n",
    "\n",
    "        self.server_optimizer.step()\n",
    "        \n",
    "        \n",
    "class FedDane(BaseFederater):\n",
    "    def __init__(self, \n",
    "                 model,\n",
    "                 clients,\n",
    "                 client_optimizer_cls,\n",
    "                 client_optimizer_params,\n",
    "                 server_optimizer,\n",
    "                 mu=0,\n",
    "                 client_scheduler_cls=None,\n",
    "                 client_scheduler_params=None,\n",
    "                 server_scheduler=None,\n",
    "                 C=0.1,\n",
    "                 seed=None,\n",
    "                 writer=None):\n",
    "        super().__init__(model,\n",
    "                         clients,\n",
    "                         client_optimizer_cls,\n",
    "                         client_optimizer_params,\n",
    "                         server_optimizer=server_optimizer,\n",
    "                         server_scheduler=server_scheduler,\n",
    "                         client_scheduler_cls=client_scheduler_cls,\n",
    "                         client_scheduler_params=client_scheduler_params,\n",
    "                         C=C,\n",
    "                         seed=seed,\n",
    "                         writer=writer)\n",
    "        self.mu = mu\n",
    "        self.average_gradients = None\n",
    "            \n",
    "    def get_client_optimizer(self, client):\n",
    "        optimizer_params = self.client_optimizer_params or {}\n",
    "        client_optimizer = self.client_optimizer_cls(client.model.parameters(), **optimizer_params)\n",
    "        client_optimizer = FedDaneSolver(client_optimizer, \n",
    "                                         average_gradients=self.average_gradients,\n",
    "                                         mu=self.mu)\n",
    "        return client_optimizer\n",
    "    \n",
    "    def fit(self, \n",
    "            num_rounds,\n",
    "            criterion, \n",
    "            num_epochs,\n",
    "            val_dl=None,\n",
    "            straggler_rate=0,\n",
    "            eval_every_n=1):\n",
    "        # subset a sample of `m` clients each round\n",
    "        m = max(int(np.ceil(self.num_clients * self.C)), 1)\n",
    "        \n",
    "        for t in range(num_rounds):\n",
    "            self.global_round += 1\n",
    "            \n",
    "            # calculate the average gradient on a subset of clients\n",
    "            S_grad = self._random_state.choice(self.client_ids, m, replace=False)\n",
    "            self.set_average_gradients(S_grad, criterion)\n",
    "            \n",
    "            # update a subset of clients with the local solver\n",
    "            S = self._random_state.choice(self.client_ids, m, replace=False)\n",
    "            train_metrics = self.update(client_ids=S, \n",
    "                                        criterion=criterion,\n",
    "                                        num_epochs=num_epochs, \n",
    "                                        straggler_rate=straggler_rate)\n",
    "            \n",
    "            if eval_every_n is not None and t % eval_every_n == 0 and val_dl is not None:\n",
    "                template_str = f'round {self.global_round}'\n",
    "                val_metrics = self.validate(val_dl, criterion)\n",
    "                for metric, value in train_metrics.items():\n",
    "                    self.writer.add_scalar(f'train/{metric}', value, self.global_round)\n",
    "                    template_str += f' - train_{metric} : {value:0.4f}'\n",
    "                for metric, value in val_metrics.items():\n",
    "                    self.writer.add_scalar(f'val/{metric}', value, self.global_round)\n",
    "                    template_str += f' - val_{metric} : {value:0.4f}'\n",
    "                \n",
    "                print(template_str)\n",
    "    \n",
    "    def aggregate(self):\n",
    "        self.server_optimizer.zero_grad()\n",
    "        for k, client in enumerate(self.clients.values()):\n",
    "            for p_server, p_client in zip(self.model.parameters(), client.model.parameters()):\n",
    "                if p_server.requires_grad:\n",
    "                    if k == 0:\n",
    "                        p_server.grad = self.client_weights[k] * (p_server.data - p_client.data)\n",
    "                    else:\n",
    "                        p_server.grad.add_(p_server.data - p_client.data, alpha=self.client_weights[k])\n",
    "        \n",
    "        self.server_optimizer.step()\n",
    "        \n",
    "    def set_average_gradients(self, client_ids, criterion):\n",
    "        grads = self.get_gradients(client_ids, criterion)\n",
    "        average_gradients = [0] * len(grads[0])\n",
    "        for client_grads in grads:\n",
    "            for i, g in enumerate(client_grads):\n",
    "                average_gradients[i] += g\n",
    "        self.average_gradients = [g / len(grads) for g in average_gradients]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "00056244",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \n",
    "    fed_avg = {\n",
    "        'mnist': {\n",
    "            'clients': {\n",
    "                'num_clients': 100,\n",
    "                'shard_size': 300,\n",
    "                'batch_size': 50,\n",
    "                'is_iid': False,\n",
    "            },\n",
    "            'client_optimizer': 'SGD',\n",
    "            'client_optimizer_params': {\n",
    "                'lr': 0.1,\n",
    "            },\n",
    "            'federater': {\n",
    "                'C': 0.1\n",
    "            },\n",
    "            'fit': {\n",
    "                'num_rounds': 3,\n",
    "                'num_epochs': 5\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    fed_prox = {\n",
    "        'optimizer': 'SGD',\n",
    "        'params': {\n",
    "            'lr': 1,\n",
    "            'momentum': 0.1\n",
    "        }\n",
    "    }\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e9af4176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1 - train_loss : 0.0522 - train_accuracy : 0.9828 - val_loss : 2.2997 - val_accuracy : 0.1437\n",
      "round 2 - train_loss : 0.0411 - train_accuracy : 0.9868 - val_loss : 2.3005 - val_accuracy : 0.1852\n",
      "round 3 - train_loss : 0.0404 - train_accuracy : 0.9873 - val_loss : 2.2985 - val_accuracy : 0.1398\n"
     ]
    }
   ],
   "source": [
    "# Fed avg baselien\n",
    "seed = 42069\n",
    "device = 'cpu'\n",
    "num_workers = 0\n",
    "experiment_name = 'fedavg_baseline'\n",
    "set_state(seed)\n",
    "writer = SummaryWriter(os.path.join('logs', experiment_name))\n",
    "\n",
    "client_params = config.fed_avg['mnist']['clients']\n",
    "clients = get_clients(\n",
    "    train_ds, \n",
    "    num_workers=num_workers,\n",
    "    seed=seed,\n",
    "    device=device,\n",
    "    **client_params,\n",
    ")\n",
    "\n",
    "model = CNN()\n",
    "# client optimizer\n",
    "# client_optimizer_cls = torch.optim.SGD\n",
    "# client_optimizer_params = {\n",
    "#     'lr': 0.1,\n",
    "# }\n",
    "client_optimizer_cls = getattr(torch.optim, config.fed_avg['mnist']['client_optimizer'])\n",
    "client_optimizer_params = config.fed_avg['mnist']['client_optimizer_params']\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "fed_params = config.fed_avg['mnist']['federater']\n",
    "fed_params['seed'] = seed\n",
    "# fed_params = {\n",
    "#     'seed': seed,\n",
    "#     'C': 0.1,\n",
    "# }\n",
    "num_rounds = config.fed_avg['mnist']['fit']['num_rounds']\n",
    "num_epochs = config.fed_avg['mnist']['fit']['num_epochs']\n",
    "\n",
    "federater_prox = FedAvg(model,\n",
    "                    clients=clients,\n",
    "                    client_optimizer_cls=client_optimizer_cls,\n",
    "                    client_optimizer_params=client_optimizer_params,\n",
    "                    **fed_params)\n",
    "federater_prox.fit(num_rounds=num_rounds, criterion=criterion, num_epochs=num_epochs, val_dl=test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "fa6be157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ERROR: Failed to launch TensorBoard (exited with 1).\n",
       "Contents of stderr:\n",
       "TensorFlow installation not found - running with reduced feature set.\n",
       "Error: Must not specify both --host and --bind_all."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "%tensorboard --bind_all --logdir logs/fedavg_avg --host localhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b4c85f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['events.out.tfevents.1624399407.64acb651e23a.401.20',\n",
       " 'events.out.tfevents.1624399370.64acb651e23a.401.19',\n",
       " 'events.out.tfevents.1624399485.64acb651e23a.401.27',\n",
       " 'events.out.tfevents.1624399462.64acb651e23a.401.25',\n",
       " 'events.out.tfevents.1624399450.64acb651e23a.401.23',\n",
       " 'events.out.tfevents.1624399444.64acb651e23a.401.22',\n",
       " 'events.out.tfevents.1624399432.64acb651e23a.401.21']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('logs/fedavg_baseline/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485b6c2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "30e0d152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12 56 43 34 32 55 21  2 41 77]\n",
      "round 1 - train_loss : 0.6275 - train_accuracy : 0.8185 - val_loss : 2.2982 - val_accuracy : 0.1241\n",
      "[ 1 10 86 59 64 47  3 27 80 72]\n",
      "round 2 - train_loss : 0.7072 - train_accuracy : 0.7943 - val_loss : 2.2948 - val_accuracy : 0.1811\n",
      "[91 80  2 34 63 97 12 14 49 89]\n",
      "round 3 - train_loss : 0.6144 - train_accuracy : 0.8287 - val_loss : 2.2911 - val_accuracy : 0.2160\n",
      "[ 5 14 29 47 48  3 74 62 64 66]\n",
      "round 4 - train_loss : 0.6083 - train_accuracy : 0.8285 - val_loss : 2.2900 - val_accuracy : 0.2767\n",
      "[73 94 87 63  3 44 23 88 59 84]\n",
      "round 5 - train_loss : 0.5569 - train_accuracy : 0.8543 - val_loss : 2.2860 - val_accuracy : 0.3574\n"
     ]
    }
   ],
   "source": [
    "# Fed avg baselien\n",
    "seed = 42069\n",
    "device = 'cpu'\n",
    "set_state(seed)\n",
    "model_avg = CNN()\n",
    "clients_avg = get_clients(\n",
    "    train_ds, \n",
    "    num_clients=100,\n",
    "    is_iid=False,\n",
    "    shard_size=300,\n",
    "    batch_size=10,\n",
    "    num_workers=0,\n",
    "    seed=seed,\n",
    "    device=device\n",
    ")\n",
    "# server optimizer\n",
    "server_optimizer_params = {\n",
    "    'lr': 1,\n",
    "    'momentum': 0\n",
    "}\n",
    "server_optimizer = torch.optim.SGD(model_avg.parameters(), **server_optimizer_params)\n",
    "\n",
    "# client optimizer\n",
    "client_optimizer_cls = torch.optim.SGD\n",
    "client_optimizer_params = {\n",
    "    'lr': 0.1,\n",
    "}\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "fed_params = {\n",
    "    'seed': seed,\n",
    "    'C': 0.1,\n",
    "}\n",
    "\n",
    "federater_avg = FedAvg(model_avg,\n",
    "                    clients=clients_avg,\n",
    "                    client_optimizer_cls=client_optimizer_cls,\n",
    "                    client_optimizer_params=client_optimizer_params,\n",
    "                    **fed_params)\n",
    "federater_avg.fit(num_rounds=5, criterion=criterion, num_epochs=1, val_dl=test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2ab2e069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12 56 43 34 32 55 21  2 41 77]\n",
      "round 1 - train_loss : 0.6275 - train_accuracy : 0.8185 - val_loss : 2.2982 - val_accuracy : 0.1241\n",
      "[ 1 10 86 59 64 47  3 27 80 72]\n",
      "round 2 - train_loss : 0.7070 - train_accuracy : 0.7947 - val_loss : 2.2948 - val_accuracy : 0.1812\n",
      "[91 80  2 34 63 97 12 14 49 89]\n",
      "round 3 - train_loss : 0.6384 - train_accuracy : 0.8200 - val_loss : 2.2918 - val_accuracy : 0.2129\n",
      "[ 5 14 29 47 48  3 74 62 64 66]\n",
      "round 4 - train_loss : 0.6047 - train_accuracy : 0.8390 - val_loss : 2.2907 - val_accuracy : 0.2724\n",
      "[73 94 87 63  3 44 23 88 59 84]\n",
      "round 5 - train_loss : 0.5962 - train_accuracy : 0.8512 - val_loss : 2.2884 - val_accuracy : 0.3317\n"
     ]
    }
   ],
   "source": [
    "# Fed avg baselien\n",
    "seed = 42069\n",
    "device = 'cpu'\n",
    "num_workers = 0\n",
    "experiment_name = 'fedavg_baseline'\n",
    "set_state(seed)\n",
    "writer = SummaryWriter(os.path.join('logs', experiment_name))\n",
    "\n",
    "data_params = config.fed_avg['mnist']['data']\n",
    "model_prox = CNN()\n",
    "clients_prox = get_clients(\n",
    "    train_ds, \n",
    "    num_workers=num_workers,\n",
    "    seed=seed,\n",
    "    device=device\n",
    "    **data_params,\n",
    ")\n",
    "# server optimizer\n",
    "server_optimizer_params = {\n",
    "    'lr': 1,\n",
    "    'momentum': 0\n",
    "}\n",
    "server_optimizer = torch.optim.SGD(model_prox.parameters(), **server_optimizer_params)\n",
    "\n",
    "# client optimizer\n",
    "client_optimizer_cls = torch.optim.SGD\n",
    "client_optimizer_params = {\n",
    "    'lr': 0.1,\n",
    "}\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "fed_params = {\n",
    "    'seed': seed,\n",
    "    'mu': 0,\n",
    "    'C': 0.1,\n",
    "}\n",
    "\n",
    "federater_prox = FedProx(model_prox,\n",
    "                    clients=clients_prox,\n",
    "                    client_optimizer_cls=client_optimizer_cls,\n",
    "                    client_optimizer_params=client_optimizer_params,\n",
    "                    server_optimizer=server_optimizer,\n",
    "                    **fed_params)\n",
    "federater_prox.fit(num_rounds=100, criterion=criterion, num_epochs=1, val_dl=test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "076baa3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1237,  0.0496, -0.0533, -0.0223,  0.0750],\n",
       "         [ 0.1293,  0.0621, -0.1308,  0.1752, -0.0304],\n",
       "         [ 0.1696,  0.1714, -0.1323, -0.0063,  0.0882],\n",
       "         [-0.0299,  0.1077,  0.0123,  0.1446,  0.1252],\n",
       "         [ 0.1886,  0.1777, -0.1097, -0.0612, -0.1472]]],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model_avg.parameters())[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "011049ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1241,  0.0502, -0.0526, -0.0217,  0.0757],\n",
       "         [ 0.1296,  0.0627, -0.1302,  0.1758, -0.0299],\n",
       "         [ 0.1698,  0.1716, -0.1321, -0.0061,  0.0883],\n",
       "         [-0.0298,  0.1078,  0.0124,  0.1448,  0.1253],\n",
       "         [ 0.1890,  0.1780, -0.1095, -0.0609, -0.1471]]],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model_avg.parameters())[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4accb726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1237,  0.0496, -0.0533, -0.0223,  0.0750],\n",
       "         [ 0.1293,  0.0621, -0.1308,  0.1752, -0.0304],\n",
       "         [ 0.1696,  0.1714, -0.1323, -0.0063,  0.0882],\n",
       "         [-0.0299,  0.1077,  0.0123,  0.1446,  0.1252],\n",
       "         [ 0.1886,  0.1777, -0.1097, -0.0612, -0.1472]]],\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_avg = torch.stack([federater_avg.client_weights[i] * list(clients_avg[i].model.parameters())[0][0] for i in range(100)]).sum(0)\n",
    "new_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7e56b6e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0003355368971824646"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(list(model_avg.parameters())[0][0] - new_avg)[0][0][0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4f3699ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = []\n",
    "for i in range(100):\n",
    "    g.append(list(model_avg.parameters())[0][0] - list(clients_avg[i].model.parameters())[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cbddf2e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00033553242683410646"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g[12][0][0][0].item() * federater_avg.client_weights[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2e9f91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab8e068",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "id": "fefc13d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 764,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model_avg.parameters())[0][0] - list(clients_avg[0].model.parameters())[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "22ac6340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-6.2098e-04, -8.3167e-04, -8.5653e-04, -1.0109e-04, -1.4994e-05],\n",
       "         [-1.3470e-03, -1.2671e-03, -7.1046e-04,  1.7881e-05, -3.6251e-04],\n",
       "         [-1.1452e-03, -1.8276e-03, -1.0516e-03, -6.7298e-04, -6.9550e-04],\n",
       "         [-1.1875e-03, -1.6784e-03, -7.8158e-04, -3.3040e-04, -7.4774e-05],\n",
       "         [-1.0530e-03, -9.4239e-04, -5.7211e-04, -4.1787e-04,  1.0461e-04]]],\n",
       "       grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model_avg.parameters())[0][0] - list(model_prox.parameters())[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fe824912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12 56 43 34 32 55 21  2 41 77]\n",
      "round 1 - train_loss : 0.7237 - train_accuracy : 0.7873 - val_loss : 2.3106 - val_accuracy : 0.1532\n",
      "[ 1 10 86 59 64 47  3 27 80 72]\n",
      "round 2 - train_loss : 0.8594 - train_accuracy : 0.7323 - val_loss : 2.3106 - val_accuracy : 0.1536\n",
      "[91 80  2 34 63 97 12 14 49 89]\n",
      "round 3 - train_loss : 0.7307 - train_accuracy : 0.7828 - val_loss : 2.3104 - val_accuracy : 0.1576\n",
      "[ 5 14 29 47 48  3 74 62 64 66]\n",
      "round 4 - train_loss : 0.7079 - train_accuracy : 0.8025 - val_loss : 2.3098 - val_accuracy : 0.1640\n",
      "[73 94 87 63  3 44 23 88 59 84]\n",
      "round 5 - train_loss : 0.8606 - train_accuracy : 0.7702 - val_loss : 2.3088 - val_accuracy : 0.1728\n"
     ]
    }
   ],
   "source": [
    "# Fed avg baselien\n",
    "seed = 42069\n",
    "device = 'cpu'\n",
    "set_state(seed)\n",
    "model_nova = CNN()\n",
    "clients_nova = get_clients(\n",
    "    train_ds, \n",
    "    num_clients=100,\n",
    "    is_iid=False,\n",
    "    shard_size=300,\n",
    "    batch_size=10,\n",
    "    num_workers=0,\n",
    "    seed=seed,\n",
    "    device=device\n",
    ")\n",
    "# server optimizer\n",
    "server_optimizer_params = {\n",
    "    'lr': 1,\n",
    "    'momentum': 0.9\n",
    "}\n",
    "server_optimizer = torch.optim.SGD(model_nova.parameters(), **server_optimizer_params)\n",
    "\n",
    "# client optimizer\n",
    "client_optimizer_cls = torch.optim.SGD\n",
    "client_optimizer_params = {\n",
    "    'lr': 0.1,\n",
    "}\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "fed_params = {\n",
    "    'seed': seed,\n",
    "    'C': 0.1,\n",
    "    'mu': 0.1,\n",
    "}\n",
    "\n",
    "federater_nova = FedNova(model_nova,\n",
    "                    clients=clients_nova,\n",
    "                    client_optimizer_cls=client_optimizer_cls,\n",
    "                    client_optimizer_params=client_optimizer_params,\n",
    "                         server_optimizer=server_optimizer,\n",
    "                    **fed_params)\n",
    "federater_nova.fit(num_rounds=5, criterion=criterion, num_epochs=1, val_dl=test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "b9387308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1 - train_loss : 0.6275 - train_accuracy : 0.8185 - val_loss : 2.2982 - val_accuracy : 0.1241\n",
      "round 2 - train_loss : 0.7086 - train_accuracy : 0.7940 - val_loss : 2.2951 - val_accuracy : 0.2126\n",
      "round 3 - train_loss : 0.6189 - train_accuracy : 0.8248 - val_loss : 2.2946 - val_accuracy : 0.2199\n",
      "round 4 - train_loss : 0.4919 - train_accuracy : 0.8758 - val_loss : 2.2911 - val_accuracy : 0.2282\n",
      "round 5 - train_loss : 0.5117 - train_accuracy : 0.8835 - val_loss : 2.2805 - val_accuracy : 0.2499\n"
     ]
    }
   ],
   "source": [
    "seed = 42069\n",
    "device = 'cpu'\n",
    "set_state(random_state)\n",
    "model = CNN()\n",
    "clients = get_clients(\n",
    "    train_ds, \n",
    "    num_clients=100,\n",
    "    is_iid=False,\n",
    "    shard_size=300,\n",
    "    batch_size=10,\n",
    "    num_workers=0,\n",
    "    seed=seed,\n",
    "    device=device\n",
    ")\n",
    "# server optimizer\n",
    "server_optimizer_params = {\n",
    "    'lr': 1,\n",
    "    'momentum': 0\n",
    "}\n",
    "server_optimizer = torch.optim.SGD(model.parameters(), **server_optimizer_params)\n",
    "\n",
    "# client optimizer\n",
    "client_optimizer_cls = torch.optim.SGD\n",
    "client_optimizer_params = {\n",
    "    'lr': 0.1,\n",
    "}\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "fed_params = {\n",
    "    'mu': 0,\n",
    "    'seed': seed,\n",
    "}\n",
    "\n",
    "federater = FedNova(model,\n",
    "                    clients=clients,\n",
    "                    client_optimizer_cls=client_optimizer_cls,\n",
    "                    client_optimizer_params=client_optimizer_params,\n",
    "                    server_optimizer=server_optimizer,\n",
    "                    **fed_params)\n",
    "\n",
    "# federater.fit(num_rounds=1, criterion=criterion, num_epochs=2, val_dl=test_dl)\n",
    "federater.fit(num_rounds=5, criterion=criterion, num_epochs=1, val_dl=test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f839a66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34af7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "federater.fit(num_rounds=5, criterion=criterion, num_epochs=1, val_dl=test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04c4d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "round 1 - train_loss : 0.0692 - train_accuracy : 0.9733 - val_loss : 2.2971 - val_accuracy : 0.1301"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e8c695e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1 - train_loss : 0.0896 - train_accuracy : 0.9767 - val_loss : 2.3033 - val_accuracy : 0.1749\n",
      "round 2 - train_loss : 0.0930 - train_accuracy : 0.9748 - val_loss : 2.2953 - val_accuracy : 0.1983\n",
      "round 3 - train_loss : 0.1644 - train_accuracy : 0.9507 - val_loss : 2.2892 - val_accuracy : 0.1776\n",
      "round 4 - train_loss : 0.0562 - train_accuracy : 0.9815 - val_loss : 2.2814 - val_accuracy : 0.1884\n",
      "round 5 - train_loss : 0.1558 - train_accuracy : 0.9643 - val_loss : 2.2728 - val_accuracy : 0.1976\n",
      "round 6 - train_loss : 0.1310 - train_accuracy : 0.9698 - val_loss : 2.2597 - val_accuracy : 0.2462\n",
      "round 7 - train_loss : 0.1211 - train_accuracy : 0.9558 - val_loss : 2.2503 - val_accuracy : 0.3114\n",
      "round 8 - train_loss : 0.1018 - train_accuracy : 0.9707 - val_loss : 2.2407 - val_accuracy : 0.3875\n",
      "round 9 - train_loss : 0.1268 - train_accuracy : 0.9698 - val_loss : 2.2280 - val_accuracy : 0.4173\n",
      "round 10 - train_loss : 0.0696 - train_accuracy : 0.9813 - val_loss : 2.2142 - val_accuracy : 0.4048\n",
      "round 11 - train_loss : 0.0906 - train_accuracy : 0.9768 - val_loss : 2.1966 - val_accuracy : 0.4343\n",
      "round 12 - train_loss : 0.0539 - train_accuracy : 0.9848 - val_loss : 2.1820 - val_accuracy : 0.5140\n",
      "round 13 - train_loss : 0.1284 - train_accuracy : 0.9687 - val_loss : 2.1688 - val_accuracy : 0.5509\n",
      "round 14 - train_loss : 0.0710 - train_accuracy : 0.9788 - val_loss : 2.1408 - val_accuracy : 0.5398\n",
      "round 15 - train_loss : 0.1180 - train_accuracy : 0.9703 - val_loss : 2.1267 - val_accuracy : 0.5815\n",
      "round 16 - train_loss : 0.0812 - train_accuracy : 0.9775 - val_loss : 2.1140 - val_accuracy : 0.6182\n",
      "round 17 - train_loss : 0.0371 - train_accuracy : 0.9895 - val_loss : 2.0866 - val_accuracy : 0.6332\n",
      "round 18 - train_loss : 0.0387 - train_accuracy : 0.9882 - val_loss : 2.0516 - val_accuracy : 0.6558\n",
      "round 19 - train_loss : 0.0488 - train_accuracy : 0.9860 - val_loss : 2.0209 - val_accuracy : 0.6747\n",
      "round 20 - train_loss : 0.1107 - train_accuracy : 0.9668 - val_loss : 1.9982 - val_accuracy : 0.6801\n",
      "round 21 - train_loss : 0.0906 - train_accuracy : 0.9762 - val_loss : 1.9676 - val_accuracy : 0.7257\n",
      "round 22 - train_loss : 0.0542 - train_accuracy : 0.9828 - val_loss : 1.9325 - val_accuracy : 0.7371\n",
      "round 23 - train_loss : 0.0565 - train_accuracy : 0.9833 - val_loss : 1.8955 - val_accuracy : 0.7533\n",
      "round 24 - train_loss : 0.0437 - train_accuracy : 0.9890 - val_loss : 1.8586 - val_accuracy : 0.7483\n",
      "round 25 - train_loss : 0.0571 - train_accuracy : 0.9843 - val_loss : 1.8202 - val_accuracy : 0.7300\n",
      "round 26 - train_loss : 0.0891 - train_accuracy : 0.9750 - val_loss : 1.7815 - val_accuracy : 0.7641\n",
      "round 27 - train_loss : 0.0684 - train_accuracy : 0.9795 - val_loss : 1.7390 - val_accuracy : 0.7771\n",
      "round 28 - train_loss : 0.0483 - train_accuracy : 0.9880 - val_loss : 1.6936 - val_accuracy : 0.7707\n",
      "round 29 - train_loss : 0.0466 - train_accuracy : 0.9865 - val_loss : 1.6553 - val_accuracy : 0.7895\n",
      "round 30 - train_loss : 0.1327 - train_accuracy : 0.9703 - val_loss : 1.6506 - val_accuracy : 0.8401\n",
      "round 31 - train_loss : 0.0437 - train_accuracy : 0.9867 - val_loss : 1.6068 - val_accuracy : 0.8400\n",
      "round 32 - train_loss : 0.0320 - train_accuracy : 0.9910 - val_loss : 1.5750 - val_accuracy : 0.8310\n",
      "round 33 - train_loss : 0.0361 - train_accuracy : 0.9905 - val_loss : 1.5433 - val_accuracy : 0.8134\n",
      "round 34 - train_loss : 0.0224 - train_accuracy : 0.9948 - val_loss : 1.4882 - val_accuracy : 0.7841\n",
      "round 35 - train_loss : 0.0320 - train_accuracy : 0.9918 - val_loss : 1.4289 - val_accuracy : 0.7929\n",
      "round 36 - train_loss : 0.0466 - train_accuracy : 0.9882 - val_loss : 1.4034 - val_accuracy : 0.8131\n",
      "round 37 - train_loss : 0.0968 - train_accuracy : 0.9757 - val_loss : 1.3948 - val_accuracy : 0.8584\n",
      "round 38 - train_loss : 0.0763 - train_accuracy : 0.9797 - val_loss : 1.3620 - val_accuracy : 0.8639\n",
      "round 39 - train_loss : 0.0331 - train_accuracy : 0.9903 - val_loss : 1.3076 - val_accuracy : 0.8788\n",
      "round 40 - train_loss : 0.0370 - train_accuracy : 0.9893 - val_loss : 1.2643 - val_accuracy : 0.8769\n",
      "round 41 - train_loss : 0.0602 - train_accuracy : 0.9823 - val_loss : 1.2527 - val_accuracy : 0.8579\n",
      "round 42 - train_loss : 0.0465 - train_accuracy : 0.9873 - val_loss : 1.2253 - val_accuracy : 0.8484\n",
      "round 43 - train_loss : 0.0324 - train_accuracy : 0.9897 - val_loss : 1.1812 - val_accuracy : 0.8886\n",
      "round 44 - train_loss : 0.0194 - train_accuracy : 0.9943 - val_loss : 1.1238 - val_accuracy : 0.8977\n",
      "round 45 - train_loss : 0.0399 - train_accuracy : 0.9892 - val_loss : 1.0846 - val_accuracy : 0.9016\n",
      "round 46 - train_loss : 0.0706 - train_accuracy : 0.9842 - val_loss : 1.0694 - val_accuracy : 0.9098\n",
      "round 47 - train_loss : 0.0721 - train_accuracy : 0.9782 - val_loss : 1.0585 - val_accuracy : 0.9109\n",
      "round 48 - train_loss : 0.0180 - train_accuracy : 0.9958 - val_loss : 1.0190 - val_accuracy : 0.9146\n",
      "round 49 - train_loss : 0.0229 - train_accuracy : 0.9937 - val_loss : 0.9746 - val_accuracy : 0.9204\n",
      "round 50 - train_loss : 0.0313 - train_accuracy : 0.9915 - val_loss : 0.9566 - val_accuracy : 0.9210\n",
      "round 51 - train_loss : 0.0739 - train_accuracy : 0.9833 - val_loss : 0.9446 - val_accuracy : 0.9212\n",
      "round 52 - train_loss : 0.0633 - train_accuracy : 0.9845 - val_loss : 0.9189 - val_accuracy : 0.9234\n",
      "round 53 - train_loss : 0.0492 - train_accuracy : 0.9863 - val_loss : 0.8966 - val_accuracy : 0.9215\n",
      "round 54 - train_loss : 0.0822 - train_accuracy : 0.9808 - val_loss : 0.8932 - val_accuracy : 0.9156\n",
      "round 55 - train_loss : 0.0312 - train_accuracy : 0.9925 - val_loss : 0.8566 - val_accuracy : 0.9170\n",
      "round 56 - train_loss : 0.0777 - train_accuracy : 0.9818 - val_loss : 0.8499 - val_accuracy : 0.9230\n",
      "round 57 - train_loss : 0.0240 - train_accuracy : 0.9932 - val_loss : 0.8062 - val_accuracy : 0.9171\n",
      "round 58 - train_loss : 0.0404 - train_accuracy : 0.9897 - val_loss : 0.7853 - val_accuracy : 0.9228\n",
      "round 59 - train_loss : 0.0273 - train_accuracy : 0.9918 - val_loss : 0.7503 - val_accuracy : 0.9290\n",
      "round 60 - train_loss : 0.0612 - train_accuracy : 0.9878 - val_loss : 0.7544 - val_accuracy : 0.9262\n",
      "round 61 - train_loss : 0.0408 - train_accuracy : 0.9888 - val_loss : 0.7404 - val_accuracy : 0.9259\n",
      "round 62 - train_loss : 0.0193 - train_accuracy : 0.9955 - val_loss : 0.7126 - val_accuracy : 0.9274\n",
      "round 63 - train_loss : 0.0276 - train_accuracy : 0.9920 - val_loss : 0.6802 - val_accuracy : 0.9249\n",
      "round 64 - train_loss : 0.0320 - train_accuracy : 0.9905 - val_loss : 0.6659 - val_accuracy : 0.9200\n",
      "round 65 - train_loss : 0.0163 - train_accuracy : 0.9957 - val_loss : 0.6325 - val_accuracy : 0.9283\n",
      "round 66 - train_loss : 0.0481 - train_accuracy : 0.9908 - val_loss : 0.6325 - val_accuracy : 0.9281\n",
      "round 67 - train_loss : 0.0452 - train_accuracy : 0.9870 - val_loss : 0.6322 - val_accuracy : 0.9294\n",
      "round 68 - train_loss : 0.0125 - train_accuracy : 0.9972 - val_loss : 0.6062 - val_accuracy : 0.9270\n",
      "round 69 - train_loss : 0.0640 - train_accuracy : 0.9857 - val_loss : 0.6124 - val_accuracy : 0.9294\n",
      "round 70 - train_loss : 0.0665 - train_accuracy : 0.9843 - val_loss : 0.6030 - val_accuracy : 0.9333\n",
      "round 71 - train_loss : 0.0296 - train_accuracy : 0.9907 - val_loss : 0.5788 - val_accuracy : 0.9330\n",
      "round 72 - train_loss : 0.0565 - train_accuracy : 0.9855 - val_loss : 0.5685 - val_accuracy : 0.9296\n",
      "round 73 - train_loss : 0.0179 - train_accuracy : 0.9953 - val_loss : 0.5457 - val_accuracy : 0.9324\n",
      "round 74 - train_loss : 0.0213 - train_accuracy : 0.9945 - val_loss : 0.5303 - val_accuracy : 0.9313\n",
      "round 75 - train_loss : 0.0202 - train_accuracy : 0.9947 - val_loss : 0.5130 - val_accuracy : 0.9315\n",
      "round 76 - train_loss : 0.0414 - train_accuracy : 0.9908 - val_loss : 0.5111 - val_accuracy : 0.9330\n",
      "round 77 - train_loss : 0.0358 - train_accuracy : 0.9917 - val_loss : 0.5003 - val_accuracy : 0.9296\n",
      "round 78 - train_loss : 0.0467 - train_accuracy : 0.9907 - val_loss : 0.4926 - val_accuracy : 0.9324\n",
      "round 79 - train_loss : 0.0288 - train_accuracy : 0.9935 - val_loss : 0.4814 - val_accuracy : 0.9319\n",
      "round 80 - train_loss : 0.0346 - train_accuracy : 0.9887 - val_loss : 0.4775 - val_accuracy : 0.9287\n",
      "round 81 - train_loss : 0.0295 - train_accuracy : 0.9925 - val_loss : 0.4675 - val_accuracy : 0.9261\n",
      "round 82 - train_loss : 0.0192 - train_accuracy : 0.9943 - val_loss : 0.4512 - val_accuracy : 0.9335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 83 - train_loss : 0.0351 - train_accuracy : 0.9910 - val_loss : 0.4455 - val_accuracy : 0.9396\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-fa73057580ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfederater\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-58-a87d903ae813>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, num_rounds, criterion, num_epochs, val_dl, straggler_rate, eval_every_n)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;31m# update a subset of clients with the local solver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0mS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_random_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             train_metrics = self.update(client_ids=S, \n\u001b[0m\u001b[1;32m    142\u001b[0m                                         \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                                         \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-58-a87d903ae813>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, client_ids, criterion, num_epochs, straggler_rate)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;31m# update the client weights and record the local training metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             client_metrics_dict = client.update(\n\u001b[0m\u001b[1;32m    109\u001b[0m                 \u001b[0mclient_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-178521c6a7a4>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, optimizer, criterion, num_epochs, scheduler)\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "federater.fit(num_rounds=100, criterion=criterion, num_epochs=20, val_dl=test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e167fd5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2072,
   "id": "89adc5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 11 - train_loss : 0.0856 - train_accuracy : 0.9775 - val_loss : 0.3990 - val_accuracy : 0.9443\n",
      "round 12 - train_loss : 0.0996 - train_accuracy : 0.9710 - val_loss : 0.3361 - val_accuracy : 0.9469\n",
      "round 13 - train_loss : 0.0887 - train_accuracy : 0.9742 - val_loss : 0.2864 - val_accuracy : 0.9494\n",
      "round 14 - train_loss : 0.0779 - train_accuracy : 0.9803 - val_loss : 0.2479 - val_accuracy : 0.9513\n",
      "round 15 - train_loss : 0.0763 - train_accuracy : 0.9823 - val_loss : 0.2201 - val_accuracy : 0.9529\n",
      "round 16 - train_loss : 0.0817 - train_accuracy : 0.9773 - val_loss : 0.1982 - val_accuracy : 0.9545\n",
      "round 17 - train_loss : 0.0775 - train_accuracy : 0.9820 - val_loss : 0.1813 - val_accuracy : 0.9561\n",
      "round 18 - train_loss : 0.0538 - train_accuracy : 0.9885 - val_loss : 0.1677 - val_accuracy : 0.9574\n",
      "round 19 - train_loss : 0.0546 - train_accuracy : 0.9877 - val_loss : 0.1571 - val_accuracy : 0.9582\n",
      "round 20 - train_loss : 0.0405 - train_accuracy : 0.9910 - val_loss : 0.1485 - val_accuracy : 0.9595\n",
      "round 21 - train_loss : 0.0611 - train_accuracy : 0.9857 - val_loss : 0.1414 - val_accuracy : 0.9600\n",
      "round 22 - train_loss : 0.0440 - train_accuracy : 0.9910 - val_loss : 0.1352 - val_accuracy : 0.9611\n",
      "round 23 - train_loss : 0.0390 - train_accuracy : 0.9922 - val_loss : 0.1298 - val_accuracy : 0.9621\n",
      "round 24 - train_loss : 0.0431 - train_accuracy : 0.9910 - val_loss : 0.1256 - val_accuracy : 0.9623\n",
      "round 25 - train_loss : 0.0452 - train_accuracy : 0.9887 - val_loss : 0.1214 - val_accuracy : 0.9633\n",
      "round 26 - train_loss : 0.0345 - train_accuracy : 0.9925 - val_loss : 0.1179 - val_accuracy : 0.9636\n",
      "round 27 - train_loss : 0.0327 - train_accuracy : 0.9945 - val_loss : 0.1146 - val_accuracy : 0.9643\n",
      "round 28 - train_loss : 0.0349 - train_accuracy : 0.9930 - val_loss : 0.1119 - val_accuracy : 0.9647\n",
      "round 29 - train_loss : 0.0317 - train_accuracy : 0.9937 - val_loss : 0.1095 - val_accuracy : 0.9660\n",
      "round 30 - train_loss : 0.0241 - train_accuracy : 0.9963 - val_loss : 0.1071 - val_accuracy : 0.9665\n",
      "round 31 - train_loss : 0.0238 - train_accuracy : 0.9963 - val_loss : 0.1049 - val_accuracy : 0.9668\n",
      "round 32 - train_loss : 0.0204 - train_accuracy : 0.9978 - val_loss : 0.1035 - val_accuracy : 0.9674\n",
      "round 33 - train_loss : 0.0283 - train_accuracy : 0.9955 - val_loss : 0.1015 - val_accuracy : 0.9677\n",
      "round 34 - train_loss : 0.0169 - train_accuracy : 0.9983 - val_loss : 0.1001 - val_accuracy : 0.9684\n",
      "round 35 - train_loss : 0.0244 - train_accuracy : 0.9953 - val_loss : 0.0988 - val_accuracy : 0.9689\n",
      "round 36 - train_loss : 0.0220 - train_accuracy : 0.9965 - val_loss : 0.0975 - val_accuracy : 0.9691\n",
      "round 37 - train_loss : 0.0180 - train_accuracy : 0.9980 - val_loss : 0.0965 - val_accuracy : 0.9695\n",
      "round 38 - train_loss : 0.0563 - train_accuracy : 0.9918 - val_loss : 0.0955 - val_accuracy : 0.9697\n",
      "round 39 - train_loss : 0.0212 - train_accuracy : 0.9965 - val_loss : 0.0941 - val_accuracy : 0.9702\n",
      "round 40 - train_loss : 0.0167 - train_accuracy : 0.9982 - val_loss : 0.0927 - val_accuracy : 0.9707\n",
      "round 41 - train_loss : 0.0160 - train_accuracy : 0.9975 - val_loss : 0.0917 - val_accuracy : 0.9711\n",
      "round 42 - train_loss : 0.0185 - train_accuracy : 0.9973 - val_loss : 0.0908 - val_accuracy : 0.9718\n",
      "round 43 - train_loss : 0.0167 - train_accuracy : 0.9975 - val_loss : 0.0897 - val_accuracy : 0.9726\n",
      "round 44 - train_loss : 0.0214 - train_accuracy : 0.9965 - val_loss : 0.0886 - val_accuracy : 0.9724\n",
      "round 45 - train_loss : 0.0198 - train_accuracy : 0.9963 - val_loss : 0.0873 - val_accuracy : 0.9730\n",
      "round 46 - train_loss : 0.0185 - train_accuracy : 0.9962 - val_loss : 0.0863 - val_accuracy : 0.9730\n",
      "round 47 - train_loss : 0.0143 - train_accuracy : 0.9980 - val_loss : 0.0857 - val_accuracy : 0.9733\n",
      "round 48 - train_loss : 0.0231 - train_accuracy : 0.9952 - val_loss : 0.0848 - val_accuracy : 0.9731\n",
      "round 49 - train_loss : 0.0176 - train_accuracy : 0.9975 - val_loss : 0.0837 - val_accuracy : 0.9739\n",
      "round 50 - train_loss : 0.0136 - train_accuracy : 0.9985 - val_loss : 0.0826 - val_accuracy : 0.9739\n"
     ]
    }
   ],
   "source": [
    "federater.fit(num_rounds=40, criterion=criterion, num_epochs=5, val_dl=test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f4ba40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2175,
   "id": "9167f3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1 - train_loss : 0.6275 - train_accuracy : 0.8185 - val_loss : 2.2982 - val_accuracy : 0.1241\n"
     ]
    }
   ],
   "source": [
    "# fedavg baseline\n",
    "federater_avg.fit(num_rounds=1, criterion=criterion, num_epochs=1, val_dl=test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2176,
   "id": "b3d7e769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1 - train_loss : 0.8181 - train_accuracy : 0.7648 - val_loss : 2.2974 - val_accuracy : 0.1303\n"
     ]
    }
   ],
   "source": [
    "# fedprox baseline\n",
    "federater_prox.fit(num_rounds=1, criterion=criterion, num_epochs=1, val_dl=test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ba328e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1483e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1848,
   "id": "632ac224",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.optimizer import Optimizer, required\n",
    "\n",
    "class FedProx2(Optimizer):\n",
    "    r\"\"\"Implements FedAvg and FedProx. Local Solver can have momentum.\n",
    "    Nesterov momentum is based on the formula from\n",
    "    `On the importance of initialization and momentum in deep learning`__.\n",
    "    Args:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        ratio (float): relative sample size of client\n",
    "        gmf (float): global/server/slow momentum factor\n",
    "        mu (float): parameter for proximal local SGD\n",
    "        lr (float): learning rate\n",
    "        momentum (float, optional): momentum factor (default: 0)\n",
    "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
    "        dampening (float, optional): dampening for momentum (default: 0)\n",
    "        nesterov (bool, optional): enables Nesterov momentum (default: False)\n",
    "    Example:\n",
    "        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "        >>> optimizer.zero_grad()\n",
    "        >>> loss_fn(model(input), target).backward()\n",
    "        >>> optimizer.step()\n",
    "    __ http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf\n",
    "    .. note::\n",
    "        The implementation of SGD with Momentum/Nesterov subtly differs from\n",
    "        Sutskever et. al. and implementations in some other frameworks.\n",
    "        Considering the specific case of Momentum, the update can be written as\n",
    "        .. math::\n",
    "                  v = \\rho * v + g \\\\\n",
    "                  p = p - lr * v\n",
    "        where p, g, v and :math:`\\rho` denote the parameters, gradient,\n",
    "        velocity, and momentum respectively.\n",
    "        This is in contrast to Sutskever et. al. and\n",
    "        other frameworks which employ an update of the form\n",
    "        .. math::\n",
    "             v = \\rho * v + lr * g \\\\\n",
    "             p = p - v\n",
    "        The Nesterov version is analogously modified.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, ratio, gmf, lr=required, momentum=0, dampening=0,\n",
    "                 weight_decay=0, nesterov=False, variance=0, mu=0):\n",
    "        \n",
    "        self.gmf = gmf\n",
    "        self.ratio = ratio\n",
    "        self.itr = 0\n",
    "        self.a_sum = 0\n",
    "        self.mu = mu\n",
    "\n",
    "\n",
    "        if lr is not required and lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if momentum < 0.0:\n",
    "            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n",
    "        if weight_decay < 0.0:\n",
    "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
    "\n",
    "        defaults = dict(lr=lr, momentum=momentum, dampening=dampening,\n",
    "                        weight_decay=weight_decay, nesterov=nesterov, variance=variance)\n",
    "        if nesterov and (momentum <= 0 or dampening != 0):\n",
    "            raise ValueError(\"Nesterov momentum requires a momentum and zero dampening\")\n",
    "        super(FedProx2, self).__init__(params, defaults)\n",
    "\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(FedProx, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('nesterov', False)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            weight_decay = group['weight_decay']\n",
    "            momentum = group['momentum']\n",
    "            dampening = group['dampening']\n",
    "            nesterov = group['nesterov']\n",
    "            \n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                d_p = p.grad.data\n",
    "\n",
    "                if weight_decay != 0:\n",
    "                    d_p.add_(weight_decay, p.data)\n",
    "                \n",
    "                param_state = self.state[p]\n",
    "                if 'old_init' not in param_state:\n",
    "                    param_state['old_init'] = torch.clone(p.data).detach()\n",
    "\n",
    "                if momentum != 0:\n",
    "                    if 'momentum_buffer' not in param_state:\n",
    "                        buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
    "                    else:\n",
    "                        buf = param_state['momentum_buffer']\n",
    "                        buf.mul_(momentum).add_(1 - dampening, d_p)\n",
    "                    if nesterov:\n",
    "                        d_p = d_p.add(momentum, buf)\n",
    "                    else:\n",
    "                        d_p = buf\n",
    "\n",
    "                # apply proximal update\n",
    "                d_p.add_(self.mu, p.data - param_state['old_init'])\n",
    "                p.data.add_(-group['lr'], d_p)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def average(self):\n",
    "        param_list = []\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                p.data.mul_(self.ratio)\n",
    "                param_list.append(p.data)\n",
    "\n",
    "        communicate(param_list, dist.all_reduce)\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                param_state = self.state[p]\n",
    "                param_state['old_init'] = torch.clone(p.data).detach()\n",
    "                # Reinitialize momentum buffer\n",
    "                if 'momentum_buffer' in param_state:\n",
    "                    param_state['momentum_buffer'].zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2848da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FedAdam(Optimizer):\n",
    "    \"\"\"FederatedAveraging (FedAvg) central server as proposed in \"Communication-Efficient Learning \n",
    "    of Deep Networks from Decentralized Data\" (https://arxiv.org/pdf/1602.05629.pdf)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : \n",
    "    dataloader :\n",
    "    device : str (default='cpu')\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr=1):\n",
    "        if lr <= 0.0:\n",
    "            raise ValueError(f'Invalid learning rate: {lr}')\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        defaults = dict(lr=lr)\n",
    "        \n",
    "        super(FedAvg, self).__init__(params, defaults)\n",
    "        \n",
    "    def aggregate(self, clients, client_weights):\n",
    "        global_state = {}\n",
    "        for k, (client_id, client) in enumerate(clients.items()):\n",
    "            local_state = client.model.state_dict()\n",
    "            for layer_name, param in local_state.items():\n",
    "                if k == 0:\n",
    "                    global_state[layer_name] = self.lr * param * client_weights[k]\n",
    "                else:\n",
    "                    global_state[layer_name] += self.lr * param * client_weights[k]\n",
    "\n",
    "        self.model.load_state_dict(global_state)\n",
    "        \n",
    "    def aggregate(self, clients, client_weights):\n",
    "        \n",
    "        for k, (client_id, client) in enumerate(clients.items()):\n",
    "            client_optimizer = client.optimizer\n",
    "            for group in client_optimizer.param_groups:\n",
    "                for p in group['params']:\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1771,
   "id": "aa2e8abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FedAvg(Optimizer):\n",
    "#     \"\"\"FederatedAveraging (FedAvg) central server as proposed in \"Communication-Efficient Learning \n",
    "#     of Deep Networks from Decentralized Data\" (https://arxiv.org/pdf/1602.05629.pdf)\n",
    "    \n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     model : \n",
    "#     dataloader :\n",
    "#     device : str (default='cpu')\n",
    "#     \"\"\"\n",
    "#     def __init__(self, params, lr=1):\n",
    "#         if lr <= 0.0:\n",
    "#             raise ValueError(f'Invalid learning rate: {lr}')\n",
    "#         self.model = model\n",
    "#         self.lr = lr\n",
    "#         defaults = dict(lr=lr)\n",
    "        \n",
    "#         super(FedAvg, self).__init__(params, defaults)\n",
    "        \n",
    "#     def aggregate(self, clients, client_weights):\n",
    "# #         layer_names = clients[0].model.state_dict().keys()\n",
    "#         global_state = {}\n",
    "#         for k, (client_id, client) in enumerate(clients.items()):\n",
    "#             local_state = client.model.state_dict()\n",
    "#             for layer_name, param in local_state.items():\n",
    "#                 if k == 0:\n",
    "#                     global_state[layer_name] = self.lr * param * client_weights[k]\n",
    "#                 else:\n",
    "#                     global_state[layer_name] += self.lr * param * client_weights[k]\n",
    "\n",
    "#         self.model.load_state_dict(global_state)\n",
    "        \n",
    "#     def aggregate(self, clients, client_weights):\n",
    "#         for k, (client_id, client) in enumerate(clients.items()):\n",
    "#             client_optimizer = client.optimizer\n",
    "#             for group in client_optimizer.param_groups:\n",
    "#                 for p in group['params']:\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcbbb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FedAdam(Optimizer):\n",
    "    \"\"\"FederatedAveraging (FedAvg) central server as proposed in \"Communication-Efficient Learning \n",
    "    of Deep Networks from Decentralized Data\" (https://arxiv.org/pdf/1602.05629.pdf)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : \n",
    "    dataloader :\n",
    "    device : str (default='cpu')\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr=1):\n",
    "        if lr <= 0.0:\n",
    "            raise ValueError(f'Invalid learning rate: {lr}')\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        defaults = dict(lr=lr)\n",
    "        \n",
    "        super(FedAvg, self).__init__(params, defaults)\n",
    "        \n",
    "    def aggregate(self, clients, client_weights):\n",
    "        global_state = {}\n",
    "        for k, (client_id, client) in enumerate(clients.items()):\n",
    "            local_state = client.model.state_dict()\n",
    "            for layer_name, param in local_state.items():\n",
    "                if k == 0:\n",
    "                    global_state[layer_name] = self.lr * param * client_weights[k]\n",
    "                else:\n",
    "                    global_state[layer_name] += self.lr * param * client_weights[k]\n",
    "\n",
    "        self.model.load_state_dict(global_state)\n",
    "        \n",
    "    def aggregate(self, clients, client_weights):\n",
    "        \n",
    "        for k, (client_id, client) in enumerate(clients.items()):\n",
    "            client_optimizer = client.optimizer\n",
    "            for group in client_optimizer.param_groups:\n",
    "                for p in group['params']:\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd74f61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class FedProx(BaseFederater):\n",
    "    \n",
    "    def __init__(self, optimizer, clients, client_weights):\n",
    "        self.optimizer = optimizer\n",
    "        self.param_groups = self.optimizer.param_groups\n",
    "    \n",
    "    def aggregate(self):\n",
    "        layer_names = self.model.state_dict().keys()\n",
    "        global_state = {}\n",
    "        for k, (client_id, client) in enumerate(clients.items()):\n",
    "            local_state = client.model.state_dict()\n",
    "            for layer_name in layer_names:\n",
    "                if k == 0:\n",
    "                    global_state[layer_name] = local_state[layer_name] * client_weights[k]\n",
    "                else:\n",
    "                    global_state[layer_name] += local_state[layer_name] * client_weights[k]\n",
    "\n",
    "        self.update(global_state)\n",
    "        \n",
    "        \n",
    "        \n",
    "class FedAvg(Optimizer):\n",
    "    \n",
    "    def __init__(self, params, cleints, client_weights):\n",
    "        self.clients = clients\n",
    "#         defaults = dict(lr=lr, momentum=momentum)\n",
    "        super(FedProxGlobal, self).__init__(params, defaults)\n",
    "        \n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "        \n",
    "        \n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                pass\n",
    "                \n",
    "                \n",
    "    def aggregate(self, clients, client_weights):\n",
    "        layer_names = self.model.state_dict().keys()\n",
    "        global_state = {}\n",
    "        for k, (client_id, client) in enumerate(clients.items()):\n",
    "            local_state = client.model.state_dict()\n",
    "            for layer_name in layer_names:\n",
    "                if k == 0:\n",
    "                    global_state[layer_name] = local_state[layer_name] * client_weights[k]\n",
    "                else:\n",
    "                    global_state[layer_name] += local_state[layer_name] * client_weights[k]\n",
    "\n",
    "        self.update(global_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b3351b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client optimizer\n",
    "# sever optimizer\n",
    "# federater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39f0e29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1195,
   "id": "024d9a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "clients = get_clients(\n",
    "    train_ds, \n",
    "    method='fedprox',\n",
    "    num_clients=100,\n",
    "    is_iid=False,\n",
    "    shard_size=300,\n",
    "    batch_size=50,\n",
    "    num_workers=0,\n",
    "    device=device\n",
    ")\n",
    "fed_cls = FedProx\n",
    "federater = fed_cls(model, \n",
    "                   clients=clients,\n",
    "                   optim_cls=optim_cls,\n",
    "                   optim_params=optim_params,\n",
    "                   criterion=criterion, \n",
    "                   C=0.1,\n",
    "                    mu=1000,\n",
    "                   output_dir='mu1'\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1196,
   "id": "5dab6f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1 - train_loss : 78.3555 - train_accuracy : 0.8820 - train_xent_loss : 0.9399 - train_weights_delta : 0.0808 - val_loss : 2.3053 - val_accuracy : 0.1034\n",
      "round 2 - train_loss : 81.7870 - train_accuracy : 0.8213 - train_xent_loss : 0.9839 - train_weights_delta : 0.0840 - val_loss : 2.3001 - val_accuracy : 0.1042\n",
      "round 3 - train_loss : 93.3500 - train_accuracy : 0.8510 - train_xent_loss : 0.8035 - train_weights_delta : 0.0922 - val_loss : 2.2993 - val_accuracy : 0.1529\n",
      "round 4 - train_loss : 101.7852 - train_accuracy : 0.8333 - train_xent_loss : 0.7750 - train_weights_delta : 0.0989 - val_loss : 2.3005 - val_accuracy : 0.1253\n",
      "round 5 - train_loss : 106.8303 - train_accuracy : 0.8857 - train_xent_loss : 0.6514 - train_weights_delta : 0.1007 - val_loss : 2.3055 - val_accuracy : 0.1460\n",
      "round 6 - train_loss : 114.3917 - train_accuracy : 0.8423 - train_xent_loss : 0.6932 - train_weights_delta : 0.1076 - val_loss : 2.2933 - val_accuracy : 0.1507\n",
      "round 7 - train_loss : 126.1735 - train_accuracy : 0.8630 - train_xent_loss : 0.6315 - train_weights_delta : 0.1159 - val_loss : 2.2786 - val_accuracy : 0.1913\n",
      "round 8 - train_loss : 121.4042 - train_accuracy : 0.8953 - train_xent_loss : 0.5595 - train_weights_delta : 0.1102 - val_loss : 2.2831 - val_accuracy : 0.0894\n",
      "round 9 - train_loss : 125.0169 - train_accuracy : 0.8500 - train_xent_loss : 0.5672 - train_weights_delta : 0.1135 - val_loss : 2.2835 - val_accuracy : 0.1707\n",
      "round 10 - train_loss : 140.9137 - train_accuracy : 0.8627 - train_xent_loss : 0.5537 - train_weights_delta : 0.1260 - val_loss : 2.2606 - val_accuracy : 0.1720\n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "federater.fit(num_rounds=10, num_epochs=1, val_dl=test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1163,
   "id": "466988a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1 - train_loss : 1.0243 - train_accuracy : 0.8240 - train_xent_loss : 0.9379 - train_weights_delta : 0.0888 - val_loss : 2.3261 - val_accuracy : 0.1138\n",
      "round 2 - train_loss : 0.9458 - train_accuracy : 0.8500 - train_xent_loss : 0.8507 - train_weights_delta : 0.0953 - val_loss : 2.3223 - val_accuracy : 0.1138\n",
      "round 3 - train_loss : 0.8591 - train_accuracy : 0.8500 - train_xent_loss : 0.7551 - train_weights_delta : 0.1012 - val_loss : 2.3194 - val_accuracy : 0.1162\n",
      "round 4 - train_loss : 0.7957 - train_accuracy : 0.8503 - train_xent_loss : 0.6841 - train_weights_delta : 0.1058 - val_loss : 2.3144 - val_accuracy : 0.1673\n",
      "round 5 - train_loss : 0.7910 - train_accuracy : 0.8477 - train_xent_loss : 0.6673 - train_weights_delta : 0.1152 - val_loss : 2.3000 - val_accuracy : 0.1278\n",
      "round 6 - train_loss : 0.7412 - train_accuracy : 0.8677 - train_xent_loss : 0.6166 - train_weights_delta : 0.1148 - val_loss : 2.2938 - val_accuracy : 0.1147\n",
      "round 7 - train_loss : 0.6914 - train_accuracy : 0.8500 - train_xent_loss : 0.5604 - train_weights_delta : 0.1186 - val_loss : 2.2827 - val_accuracy : 0.2067\n",
      "round 8 - train_loss : 0.6707 - train_accuracy : 0.8497 - train_xent_loss : 0.5352 - train_weights_delta : 0.1215 - val_loss : 2.2649 - val_accuracy : 0.2273\n",
      "round 9 - train_loss : 0.6542 - train_accuracy : 0.8997 - train_xent_loss : 0.5266 - train_weights_delta : 0.1147 - val_loss : 2.2764 - val_accuracy : 0.1514\n",
      "round 10 - train_loss : 0.6620 - train_accuracy : 0.8503 - train_xent_loss : 0.5138 - train_weights_delta : 0.1308 - val_loss : 2.2499 - val_accuracy : 0.1162\n",
      "round 11 - train_loss : 0.6142 - train_accuracy : 0.8500 - train_xent_loss : 0.4740 - train_weights_delta : 0.1234 - val_loss : 2.2502 - val_accuracy : 0.2160\n",
      "round 12 - train_loss : 0.5985 - train_accuracy : 0.8920 - train_xent_loss : 0.4593 - train_weights_delta : 0.1219 - val_loss : 2.2617 - val_accuracy : 0.1812\n",
      "round 13 - train_loss : 0.5732 - train_accuracy : 0.8780 - train_xent_loss : 0.4320 - train_weights_delta : 0.1225 - val_loss : 2.2644 - val_accuracy : 0.1280\n",
      "round 14 - train_loss : 0.5880 - train_accuracy : 0.8500 - train_xent_loss : 0.4357 - train_weights_delta : 0.1309 - val_loss : 2.2412 - val_accuracy : 0.1173\n",
      "round 15 - train_loss : 0.5768 - train_accuracy : 0.8690 - train_xent_loss : 0.4347 - train_weights_delta : 0.1231 - val_loss : 2.2472 - val_accuracy : 0.1603\n",
      "round 16 - train_loss : 0.5506 - train_accuracy : 0.8837 - train_xent_loss : 0.4103 - train_weights_delta : 0.1207 - val_loss : 2.2537 - val_accuracy : 0.1804\n",
      "round 17 - train_loss : 0.5753 - train_accuracy : 0.8513 - train_xent_loss : 0.4168 - train_weights_delta : 0.1344 - val_loss : 2.2062 - val_accuracy : 0.1903\n",
      "round 18 - train_loss : 0.5325 - train_accuracy : 0.8813 - train_xent_loss : 0.3788 - train_weights_delta : 0.1299 - val_loss : 2.2142 - val_accuracy : 0.1847\n",
      "round 19 - train_loss : 0.5525 - train_accuracy : 0.8830 - train_xent_loss : 0.3882 - train_weights_delta : 0.1383 - val_loss : 2.2245 - val_accuracy : 0.1000\n",
      "round 20 - train_loss : 0.5190 - train_accuracy : 0.8670 - train_xent_loss : 0.3703 - train_weights_delta : 0.1258 - val_loss : 2.2257 - val_accuracy : 0.1220\n",
      "round 21 - train_loss : 0.6222 - train_accuracy : 0.8373 - train_xent_loss : 0.4386 - train_weights_delta : 0.1544 - val_loss : 2.1860 - val_accuracy : 0.1260\n",
      "round 22 - train_loss : 0.5470 - train_accuracy : 0.8667 - train_xent_loss : 0.3846 - train_weights_delta : 0.1367 - val_loss : 2.1661 - val_accuracy : 0.1050\n",
      "round 23 - train_loss : 0.5459 - train_accuracy : 0.8333 - train_xent_loss : 0.3802 - train_weights_delta : 0.1391 - val_loss : 2.1699 - val_accuracy : 0.2685\n",
      "round 24 - train_loss : 0.5685 - train_accuracy : 0.8740 - train_xent_loss : 0.4007 - train_weights_delta : 0.1414 - val_loss : 2.1473 - val_accuracy : 0.1764\n",
      "round 25 - train_loss : 0.6302 - train_accuracy : 0.8483 - train_xent_loss : 0.4504 - train_weights_delta : 0.1526 - val_loss : 2.1267 - val_accuracy : 0.2851\n",
      "round 26 - train_loss : 0.5381 - train_accuracy : 0.8847 - train_xent_loss : 0.3764 - train_weights_delta : 0.1362 - val_loss : 2.1234 - val_accuracy : 0.3020\n",
      "round 27 - train_loss : 0.5254 - train_accuracy : 0.8880 - train_xent_loss : 0.3690 - train_weights_delta : 0.1317 - val_loss : 2.1742 - val_accuracy : 0.1286\n",
      "round 28 - train_loss : 0.5270 - train_accuracy : 0.8640 - train_xent_loss : 0.3546 - train_weights_delta : 0.1439 - val_loss : 2.1724 - val_accuracy : 0.1697\n",
      "round 29 - train_loss : 0.6333 - train_accuracy : 0.8333 - train_xent_loss : 0.4234 - train_weights_delta : 0.1748 - val_loss : 2.1011 - val_accuracy : 0.2327\n",
      "round 30 - train_loss : 0.5334 - train_accuracy : 0.8780 - train_xent_loss : 0.3668 - train_weights_delta : 0.1400 - val_loss : 2.1021 - val_accuracy : 0.3895\n",
      "round 31 - train_loss : 0.5446 - train_accuracy : 0.8907 - train_xent_loss : 0.3760 - train_weights_delta : 0.1414 - val_loss : 2.1022 - val_accuracy : 0.2050\n",
      "round 32 - train_loss : 0.5162 - train_accuracy : 0.8973 - train_xent_loss : 0.3476 - train_weights_delta : 0.1412 - val_loss : 2.1057 - val_accuracy : 0.2247\n",
      "round 33 - train_loss : 0.5363 - train_accuracy : 0.8683 - train_xent_loss : 0.3499 - train_weights_delta : 0.1550 - val_loss : 2.0942 - val_accuracy : 0.2086\n",
      "round 34 - train_loss : 0.5534 - train_accuracy : 0.8600 - train_xent_loss : 0.3648 - train_weights_delta : 0.1570 - val_loss : 2.0619 - val_accuracy : 0.3084\n",
      "round 35 - train_loss : 0.5457 - train_accuracy : 0.8840 - train_xent_loss : 0.3707 - train_weights_delta : 0.1466 - val_loss : 2.0349 - val_accuracy : 0.4455\n",
      "round 36 - train_loss : 0.5495 - train_accuracy : 0.8930 - train_xent_loss : 0.3722 - train_weights_delta : 0.1484 - val_loss : 2.0483 - val_accuracy : 0.3613\n",
      "round 37 - train_loss : 0.5640 - train_accuracy : 0.8607 - train_xent_loss : 0.3704 - train_weights_delta : 0.1610 - val_loss : 2.0240 - val_accuracy : 0.3031\n",
      "round 38 - train_loss : 0.5290 - train_accuracy : 0.8830 - train_xent_loss : 0.3503 - train_weights_delta : 0.1493 - val_loss : 2.0244 - val_accuracy : 0.4280\n",
      "round 39 - train_loss : 0.5136 - train_accuracy : 0.9090 - train_xent_loss : 0.3431 - train_weights_delta : 0.1424 - val_loss : 2.0472 - val_accuracy : 0.3148\n",
      "round 40 - train_loss : 0.5493 - train_accuracy : 0.8687 - train_xent_loss : 0.3569 - train_weights_delta : 0.1599 - val_loss : 2.0098 - val_accuracy : 0.4679\n",
      "round 41 - train_loss : 0.5392 - train_accuracy : 0.9113 - train_xent_loss : 0.3517 - train_weights_delta : 0.1562 - val_loss : 1.9872 - val_accuracy : 0.4925\n",
      "round 42 - train_loss : 0.5348 - train_accuracy : 0.9213 - train_xent_loss : 0.3357 - train_weights_delta : 0.1652 - val_loss : 2.0595 - val_accuracy : 0.1548\n",
      "round 43 - train_loss : 0.5648 - train_accuracy : 0.8567 - train_xent_loss : 0.3590 - train_weights_delta : 0.1711 - val_loss : 1.9865 - val_accuracy : 0.3844\n",
      "round 44 - train_loss : 0.5192 - train_accuracy : 0.9017 - train_xent_loss : 0.3202 - train_weights_delta : 0.1650 - val_loss : 1.9914 - val_accuracy : 0.2823\n",
      "round 45 - train_loss : 0.4907 - train_accuracy : 0.9200 - train_xent_loss : 0.3065 - train_weights_delta : 0.1532 - val_loss : 2.1003 - val_accuracy : 0.1213\n",
      "round 46 - train_loss : 0.6008 - train_accuracy : 0.8370 - train_xent_loss : 0.3679 - train_weights_delta : 0.1931 - val_loss : 1.9857 - val_accuracy : 0.4145\n",
      "round 47 - train_loss : 0.5019 - train_accuracy : 0.9230 - train_xent_loss : 0.3118 - train_weights_delta : 0.1579 - val_loss : 1.9960 - val_accuracy : 0.3418\n",
      "round 48 - train_loss : 0.6401 - train_accuracy : 0.8427 - train_xent_loss : 0.4145 - train_weights_delta : 0.1872 - val_loss : 1.9200 - val_accuracy : 0.5127\n",
      "round 49 - train_loss : 0.5481 - train_accuracy : 0.8947 - train_xent_loss : 0.3440 - train_weights_delta : 0.1695 - val_loss : 1.9600 - val_accuracy : 0.2407\n",
      "round 50 - train_loss : 0.5244 - train_accuracy : 0.8867 - train_xent_loss : 0.3292 - train_weights_delta : 0.1621 - val_loss : 1.9417 - val_accuracy : 0.2336\n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "federater.fit(num_rounds=50, num_epochs=1, val_dl=test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7d9776",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952b1fc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60c02f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# class FedProxLoss(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Empirically, we observe that increasing µ leads to smaller dissimilarity among local functions\n",
    "#     Fk, and that the dissimilarity metric is consistent with the\n",
    "#     training loss.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, mu, weights_initial):\n",
    "#         super(FedProxLoss, self).__init__()\n",
    "#         self.mu = mu\n",
    "#         self.weights_initial = weights_initial\n",
    "#         self._layer_names = list(weights_initial.keys())\n",
    "        \n",
    "        \n",
    "#     def forward(self, pred, target, weights_new):\n",
    "#         loss = 0\n",
    "#         xent_loss = F.cross_entropy(pred, target)\n",
    "# #         loss = xent_loss\n",
    "#         weights_delta = []\n",
    "#         for layer in self._layer_names:\n",
    "#             layer_delta = torch.sum(torch.pow(self.weights_initial[layer] - weights_new[layer], 2))\n",
    "#             loss += self.mu / 2 * layer_delta\n",
    "#             weights_delta.append(layer_delta)\n",
    "# #             weights_delta.append(\n",
    "# #                 torch.sum(torch.pow(self.weights_initial[layer] - weights_new[layer], 2))\n",
    "# #             )\n",
    "#         weights_delta = torch.sum(torch.stack(weights_delta))\n",
    "#         loss += xent_loss\n",
    "# #         print(xent_loss)\n",
    "# #         loss = xent_loss + self.mu / 2. * weights_delta\n",
    "#         return loss, xent_loss, weights_delta\n",
    "# #         loss += "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
