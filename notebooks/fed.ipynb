{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1dc275f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.py\n",
    "# !pip install tensorboard\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_state(seed=42069):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    \n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bf3a2050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.py\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "dataset_to_transform = {\n",
    "    'mnist': {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307), (0.3081))\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307), (0.3081))\n",
    "        ]),\n",
    "    },\n",
    "    'cifar10': {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "    },\n",
    "    'cifar100': {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.507, 0.487, 0.441],\n",
    "                                 std=[0.267, 0.256, 0.276])\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.507, 0.487, 0.441],\n",
    "                                 std=[0.267, 0.256, 0.276])\n",
    "        ]),\n",
    "    }\n",
    "}\n",
    "\n",
    "train_ds = datasets.MNIST('data/', train=True, download=True, transform=dataset_to_transform['mnist']['train'])\n",
    "test_ds = datasets.MNIST('data/', train=False, download=True, transform=dataset_to_transform['mnist']['val'])\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "test_dl = DataLoader(test_ds, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "68e5c919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_emnist = datasets.EMNIST('data/', split='balanced', train=True, download=True, transform=dataset_to_transform['mnist']['train'])\n",
    "test_emnist = datasets.EMNIST('data/', split='balanced', train=False, download=True, transform=dataset_to_transform['mnist']['val'])\n",
    "train_cifar = datasets.CIFAR10('data/', train=True, download=True, transform=dataset_to_transform['cifar10']['train'])\n",
    "val_cifar = datasets.CIFAR10('data/', train=False, download=True, transform=dataset_to_transform['cifar10']['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "663d30d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.py\n",
    "\n",
    "\n",
    "class Client:\n",
    "    \"\"\"Base client.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    client_id : str\n",
    "        Id of the client.\n",
    "    dataloader : DataLoader\n",
    "        Local dataset used for training on the client.\n",
    "    \"\"\"\n",
    "    def __init__(self, client_id, dataloader, device='cpu'):\n",
    "        self.client_id = client_id\n",
    "        self.dataloader = dataloader\n",
    "        self.device = device\n",
    "        self._model = None\n",
    "        self._device = None\n",
    "        self._optimizer = None\n",
    "        self._scheduler = None\n",
    "        \n",
    "        self._local_steps = 0\n",
    "        \n",
    "    @property\n",
    "    def model(self):\n",
    "        return self._model\n",
    "    \n",
    "    @model.setter\n",
    "    def model(self, model):\n",
    "        self._model = model\n",
    "\n",
    "    @property\n",
    "    def optimizer(self):\n",
    "        return self._optimizer\n",
    "    \n",
    "    @optimizer.setter\n",
    "    def optimizer(self, optimizer):\n",
    "        self._optimizer = optimizer\n",
    "    \n",
    "    @property\n",
    "    def scheduler(self):\n",
    "        return self._scheduler\n",
    "    \n",
    "    @scheduler.setter\n",
    "    def scheduler(self, scheduler):\n",
    "        self._scheduler = scheduler\n",
    "    \n",
    "#     @property\n",
    "#     def device(self):\n",
    "#         return self._device\n",
    "\n",
    "#     @device.setter\n",
    "#     def device(self, device):\n",
    "#         self._device = device\n",
    "    \n",
    "    @property\n",
    "    def local_steps(self):\n",
    "        return self._local_steps\n",
    "    \n",
    "    @local_steps.setter\n",
    "    def local_steps(self, v):\n",
    "        self._local_steps = v\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataloader.dataset)\n",
    "    \n",
    "    def _update(self, criterion, num_epochs=1):\n",
    "        \"\"\"Algorithm 1 (ClientUpdate).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        num_epochs (E) : int\n",
    "            Number of epochs.\n",
    "        criterion : \n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        self.model.to(self.device)\n",
    "        self.local_steps = 0\n",
    "        \n",
    "        total_loss = np.zeros(num_epochs, dtype=np.float32)\n",
    "        total_correct = np.zeros(num_epochs, dtype=np.float32)\n",
    "        for i in range(num_epochs):\n",
    "            for x, y in self.dataloader:\n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                logits = self.model(x)\n",
    "                loss = criterion(logits, y)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                total_loss[i] += loss.item()\n",
    "                total_correct[i] += (logits.argmax(-1) == y).sum().item()\n",
    "                self.local_steps += 1\n",
    "                \n",
    "            # set this to a function we can call that way inherritance is easier\n",
    "            if self.scheduler is not None:\n",
    "                self.scheduler.step()\n",
    "        \n",
    "        results = {\n",
    "            'loss': total_loss / len(self.dataloader),\n",
    "            'accuracy': total_correct / len(self)\n",
    "        }\n",
    "        # move model back to cpu\n",
    "        self.model.to('cpu')\n",
    "        return results\n",
    "    \n",
    "    def update(self, criterion, num_epochs=1):\n",
    "        return self._update(criterion, num_epochs)\n",
    "\n",
    "    def get_gradients(self, criterion):\n",
    "        return get_gradients(self.model, self.dataset, criterion, device=self.device)\n",
    "    \n",
    "    \n",
    "class SCAFFOLDClient:\n",
    "    \n",
    "    def __init__(self, client_id, dataloader, option='II', device='cpu'):\n",
    "        super().__init__(client_id, dataloader, device)\n",
    "        self.option = option\n",
    "        self.control = None\n",
    "        self.control_new = None\n",
    "        self.control_delta = None\n",
    "        self._control_server = None\n",
    "        \n",
    "    @property\n",
    "    def control_server(self):\n",
    "        return self._control_server\n",
    "    \n",
    "    @control_server.setter\n",
    "    def control_server(self, control_server):\n",
    "        self._control_server = control_server\n",
    "        \n",
    "    def set_control_variates(self):\n",
    "        # set control variates if first update\n",
    "        if self.control is None:\n",
    "            self.control = [torch.zeros_like(p.data) for p in self.model]\n",
    "        if self.control_new is None:\n",
    "            self.control_new = [torch.zeros_like(p.data) for p in self.model]\n",
    "        if self.contrl_delta is None:\n",
    "            self.control_delta = [torch.zeros_like(p.data) for p in self.model]\n",
    "            \n",
    "    def update(self, criterion, num_epochs=1):\n",
    "        self.set_control_variates()\n",
    "        model_server = deepcopy(self.model)\n",
    "        results = self._update(criterion, num_epochs=num_epochs)\n",
    "        \n",
    "        # (4) updates to the local control variate\n",
    "        if self.option == 'I':\n",
    "            # gradients of global model w.r.t local data\n",
    "            grads = get_gradients(model_server, self.dataset, criterion, device=self.device)\n",
    "            for d_p, ci_new in zip(grads, self.control_new):\n",
    "                ci_new.data = d_p.data\n",
    "        elif self.option == 'II':\n",
    "            grads = [torch.zeros_like(p.data) for p in self.model.parameters()]\n",
    "            for p_server, p_client, d_p in zip(model_server.parameters(), zip(self.model.parameters()), grads):\n",
    "                d_p.data = p_client.data.detach() - p_server.data.detach()\n",
    "                \n",
    "            lr = self.optimizer.param_groups[0]['lr']\n",
    "            for ci, ci_new, c, d_p in zip(self.control, self.control_new, control_server, grads):\n",
    "                ci_new.data = ci - c + 1 / (self.local_steps * lr) * d_p.data\n",
    "#                 ci_delta.data = - c + 1 / (self.local_steps * lr) * d_p.data\n",
    "        \n",
    "        # store the control correction used in (5) and update the local control variate\n",
    "        for ci, ci_new, ci_delta in zip(self.control, self.control_new, self.control_delta):\n",
    "            ci_delta.data = ci_new.data - ci.data\n",
    "            ci.data = ci_new.data\n",
    "            \n",
    "        return results\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "181843f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradients(model, dataset, criterion, device='cpu'):\n",
    "    \"\"\"Returns a list of gradients of `model` w.r.t. `dataset`\"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    # clear gradients\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None and p.requires_grad:\n",
    "            p.grad.zero_()\n",
    "    for x, y in dataloader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "\n",
    "        # accumulate the average gradient of each batch\n",
    "        loss.backward()\n",
    "\n",
    "    # normalize the accumulated gradient across batches\n",
    "    grads = []\n",
    "    for p in model.parameters():\n",
    "        # what to do when the model has layers that don't require gradients?\n",
    "        grads.append(p.grad / len(dataloader))\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4f2a5b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.py\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class SplitDataset(Dataset):\n",
    "    \"\"\"Dataset for a client partitioned by a list of indices.\"\"\"\n",
    "    def __init__(self, dataset, indices):\n",
    "        self.dataset = dataset\n",
    "        self.indices = list(indices)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.dataset[self.indices[index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b6085f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling.py\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def get_iid_shards(labels,\n",
    "                   num_clients=100,\n",
    "                   client_ids=None,\n",
    "                   seed=None):\n",
    "    \"\"\"Returns a homogeneous (IID) mapping of client ID's to sample indices of a dataset.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    labels : list, np.ndarray, torch.Tensor\n",
    "        List of class labels for a dataset.\n",
    "    num_clients : int (default=100)\n",
    "        Number of clients used for training.\n",
    "    client_ids : list, np.ndarray (default=None)\n",
    "        List of client ID's. If `None`, defaults to a `range(num_clients)`.\n",
    "    seed : int (default=None)\n",
    "        Random state.\n",
    "       \n",
    "    Notes\n",
    "    -----\n",
    "    ```\n",
    "    num_samples_per_client = len(labels) // num_clients # 60000 / 100 = 600 for MNIST\n",
    "    ```\n",
    "    \"\"\"\n",
    "    random_state = np.random.RandomState(seed)\n",
    "    client_ids = client_ids or list(range(num_clients))\n",
    "    \n",
    "    # randomly shuffle sample indices to generate IID (homogeneous) clients\n",
    "    indices_shuffled = random_state.choice(range(len(labels)), len(labels), replace=False)\n",
    "    num_samples_per_client = len(indices_shuffled) // num_clients\n",
    "\n",
    "    # assign `num_samples_per_client` random samples to each client\n",
    "    for i, client_id in enumerate(client_ids):\n",
    "        client_indices = indices_shuffled[i * num_samples_per_client : (i + 1) * num_samples_per_client]\n",
    "        client_to_shard[client_id] = client_indices\n",
    "    \n",
    "    return client_to_shard\n",
    "    \n",
    "    \n",
    "def get_client_shards(labels, \n",
    "                      is_iid=True,\n",
    "                      num_clients=100,\n",
    "                      shard_size=300,\n",
    "                      client_ids=None,\n",
    "                      seed=None):\n",
    "    \"\"\"Returns a mapping of client ID's to sample indices of a dataset.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    labels : list, np.ndarray, torch.Tensor\n",
    "        List of class labels for a dataset.\n",
    "    is_iid : bool\n",
    "        Boolean whether to partition devices into homogenous (IID) or\n",
    "        heterogeneous (non-IID) samples.\n",
    "    num_clients : int (default=100)\n",
    "        Number of clients used for training.\n",
    "    client_ids : list, np.ndarray (default=None)\n",
    "        List of client ID's. If `None`, defaults to a `range(num_clients)`.\n",
    "    shard_size : int (default=300)\n",
    "        Size of each shard to split labels by.\n",
    "    seed : int (default=None)\n",
    "        Random state.\n",
    "       \n",
    "    Notes\n",
    "    -----\n",
    "    ```\n",
    "    num_shards = len(labels) // shard_size # 60000 / 300 = 200 for MNIST\n",
    "    num_shards_per_client = num_shards // num_clients # 200 / 100 = 2 for MNIST\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    random_state = np.random.RandomState(seed)\n",
    "    client_ids = client_ids or list(range(num_clients))\n",
    "    client_to_shard = {c_id: [] for c_id in client_ids}\n",
    "    if is_iid:\n",
    "        # randomly shuffle sample indices if IID (homogoneous)\n",
    "        sample_indices = random_state.choice(range(len(labels)), len(labels), replace=False)\n",
    "    else:\n",
    "        # sort sample indices by by label if non-IID (heterogeneous)\n",
    "        sample_indices = np.argsort(labels).tolist()\n",
    "\n",
    "    num_shards = len(labels) // shard_size\n",
    "    num_shards_per_client = num_shards // num_clients # how many shards fit in each client\n",
    "    shard_indices = set(range(num_shards))\n",
    "\n",
    "    for i, client_id in enumerate(client_ids):\n",
    "        client_shard_indices = random_state.choice(list(shard_indices), \n",
    "                                                   num_shards_per_client, \n",
    "                                                   replace=False)\n",
    "        for shard_idx in client_shard_indices:\n",
    "            client_to_shard[client_id].extend(\n",
    "                sample_indices[shard_idx*shard_size : (shard_idx+1)*shard_size]\n",
    "            )\n",
    "            shard_indices.remove(shard_idx)\n",
    "            \n",
    "    return client_to_shard\n",
    "\n",
    "\n",
    "def get_client_data(dataset, \n",
    "                    num_clients,\n",
    "                    client_ids=None,\n",
    "                    is_iid=True,\n",
    "                    shard_size=300,\n",
    "                    batch_size=32,\n",
    "                    shuffle=True,\n",
    "                    seed=None,\n",
    "                    dataloader_params=None):\n",
    "    \"\"\"Returns a mapping of client ID's to their corresponding train & validation dataloaders.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    is_iid : bool (default=True)\n",
    "        Boolean whether data is IID (Homogeneous) or non-IID (Heterogeneous)\n",
    "    **kwargs\n",
    "        Additional parameters used when instantiating each clients dataloader \n",
    "        \n",
    "    \"\"\"\n",
    "    dataloader_params = dataloader_params or {}\n",
    "    client_to_data = {}\n",
    "    labels = dataset.targets\n",
    "    client_to_shard = get_client_shards(\n",
    "        labels,\n",
    "        is_iid=is_iid,\n",
    "        num_clients=num_clients,\n",
    "        client_ids=client_ids,\n",
    "        shard_size=shard_size,\n",
    "        seed=seed\n",
    "    )\n",
    "    # iterate through each client and create train/val dataloaders using the shard indices\n",
    "    for k in client_to_shard.keys():\n",
    "        client_indices = client_to_shard[k]\n",
    "        client_dataset = SplitDataset(dataset, client_indices)\n",
    "        # reseed workers for reproducibility\n",
    "#         g = torch.Generator()\n",
    "#         g.manual_seed(0)\n",
    "        client_to_data[k] = DataLoader(\n",
    "            client_dataset, \n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle, \n",
    "#             worker_init_fn=seed_worker,\n",
    "#             generator=g,\n",
    "            **dataloader_params\n",
    "        )\n",
    "        \n",
    "    return client_to_data\n",
    "\n",
    "\n",
    "def get_clients(dataset,\n",
    "                num_clients=100,\n",
    "                client_cls=None,\n",
    "                client_ids=None,\n",
    "                is_iid=True,\n",
    "                shard_size=300,\n",
    "                batch_size=32,\n",
    "                client_params=None,\n",
    "                dataloader_params=None):\n",
    "    client_cls = client_cls or Client\n",
    "    client_params = client_params or {}\n",
    "#     if client_params is None:\n",
    "#         client_params = {}\n",
    "#     elif isinstance(client_params, dict):\n",
    "#         client_params = [client_params for _ in range(num_clients)]\n",
    "    client_to_data = get_client_data(\n",
    "        dataset,\n",
    "        num_clients=num_clients,\n",
    "        client_ids=client_ids,\n",
    "        is_iid=is_iid,\n",
    "        shard_size=shard_size,\n",
    "        batch_size=batch_size,\n",
    "        dataloader_params=dataloader_params\n",
    "    )\n",
    "    clients = {\n",
    "        k: client_cls(k, dl, **client_params)\n",
    "        for k, dl\n",
    "        in client_to_data.items()\n",
    "    }\n",
    "    return clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5c7ce54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.py\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \"\"\"CNN described in \"Communication-Efficient Learning of Deep Networks\n",
    "    from Decentralized Data\" (https://arxiv.org/pdf/1602.05629.pdf).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    in_features : int (default=1)\n",
    "        Number of channels in the input image.\n",
    "    num_classes : int (default=10)\n",
    "        Number of class labels.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features=1, num_classes=10):\n",
    "        super(CNN, self).__init__()\n",
    "        self._in_features = in_features\n",
    "        self._num_classes = num_classes\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(self._in_features, 32, 5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5)\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, self._num_classes)\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class MLP(nn.Module):\n",
    "    \"\"\"CNN described in \"Communication-Efficient Learning of Deep Networks\n",
    "    from Decentralized Data\" (https://arxiv.org/pdf/1602.05629.pdf).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    in_features : int (default=784)\n",
    "        Number input features.\n",
    "    hidden_dim : int (cdefault=200)\n",
    "        Number of hidden units.\n",
    "    num_classes : int (default=10)\n",
    "        Number of class labels.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features=784, hidden_dim=200, num_classes=10):\n",
    "        super(MLP, self).__init__()\n",
    "        self._in_features = in_features\n",
    "        self._hidden_dim = hidden_dim\n",
    "        self._num_classes = num_classes\n",
    "        \n",
    "        self.fc1 = nn.Linear(self._in_features, self._hidden_dim)\n",
    "        self.fc2 = nn.Linear(self._hidden_dim, self._num_classes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "073f00e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# servers (aggregates parameters from local solvers)\n",
    "    # base.py\n",
    "    # fedavg.py\n",
    "    # fedprox.py\n",
    "    # feddane.py\n",
    "    # fednova.py\n",
    "    # fedopt.py\n",
    "    # scaffold.py\n",
    "# optimizers (local solvers)\n",
    "    # fedprox.py\n",
    "    # feddane.py\n",
    "    # fedopt.py\n",
    "    # scaffold.py\n",
    "    # fednova.py\n",
    "# utils\n",
    "    # client.py\n",
    "    # sampling.py\n",
    "# models\n",
    "    # mnist\n",
    "        # cnn.py\n",
    "        # mlp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f8d700a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a server for each algo\n",
    "# create a client for each algo\n",
    "# create an optimizer for each algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "bf21843f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizers/fedprox.py\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "\n",
    "class FedProxSolver(Optimizer):\n",
    "    \"\"\"Implements FedProx local solver.\n",
    "    \n",
    "    This adds a proximal term to any clients optimizer.\n",
    "    \n",
    "    This wrapper allows us to pass in any torch.optim.Optimizer for\n",
    "    a given client, not limited to SGD as originally proposed.\n",
    "    \n",
    "    Args:\n",
    "        optimizer (torch.optim.Optimizer): local optimizer.\n",
    "        mu (float): proximal term weight (default: 0)\n",
    "\n",
    "    __ https://arxiv.org/pdf/1812.06127.pdf\n",
    "        \n",
    "    Example:\n",
    "        >>> # train a model locally for a client\n",
    "        >>> client_optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "        >>> client_optimizer = FedProxLocal(client_optimizer, mu=0.1)\n",
    "        >>> client_optimizer.zero_grad()\n",
    "        >>> loss_fn(model(input), target).backward()\n",
    "        >>> client_optimizer.step()\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 optimizer,\n",
    "                 mu=0):\n",
    "        if mu < 0.0:\n",
    "            raise ValueError(f'Invalid mu value: {mu}')\n",
    "        self.optimizer = optimizer\n",
    "        self.mu = mu\n",
    "        self.param_groups = self.optimizer.param_groups\n",
    "        self.state = self.optimizer.state\n",
    "    \n",
    "    def _update(self, group):\n",
    "        \"\"\"Applies a proximal update to a parameter group.\"\"\"\n",
    "        for p in group['params']:\n",
    "            if p.grad is None:\n",
    "                continue\n",
    "            state = self.state[p]\n",
    "            p.data.add_(state['proximal'], alpha=-group['lr'])\n",
    "        \n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        closure : bool\n",
    "            A closure that reevaluates the model and returns the loss.\n",
    "        \"\"\"\n",
    "        # set the initial (global) weights and proximal term before we update the client optimizer\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                state = self.state[p]\n",
    "                if 'initial_weights' not in state:\n",
    "                    state['initial_weights'] = torch.clone(p.data).detach()\n",
    "                state['proximal'] = self.mu * (p.data - state['initial_weights'])\n",
    "\n",
    "        loss = self.optimizer.step(closure=closure)\n",
    "        for group in self.param_groups:\n",
    "            self._update(group)\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "class FedDaneSolver(Optimizer):\n",
    "    \"\"\"Implements FedDane local solver.\n",
    "    \n",
    "    Args:\n",
    "        optimizer (torch.optim.Optimizer): local optimizer.\n",
    "        mu (float): proximal term weight (default: 0)\n",
    "    \n",
    "        \n",
    "    Example:\n",
    "        >>> # train a model locally for a client\n",
    "        >>> client_optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "        >>> client_optimizer = FedDaneLocal(client_optimizer, average_gradients, mu=0.1)\n",
    "        >>> client_optimizer.zero_grad()\n",
    "        >>> loss_fn(model(input), target).backward()\n",
    "        >>> client_optimizer.step()\n",
    "    \n",
    "    __ https://arxiv.org/pdf/2001.01920.pdf\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 optimizer,\n",
    "                 average_gradients,\n",
    "                 mu=0):\n",
    "        if mu < 0.0:\n",
    "            raise ValueError(f'Invalid mu value: {mu}')\n",
    "        self.optimizer = optimizer\n",
    "        self.average_gradients = average_gradients\n",
    "        self.mu = mu\n",
    "        self.param_groups = self.optimizer.param_groups\n",
    "        self.state = self.optimizer.state\n",
    "    \n",
    "    def _update(self, group):\n",
    "        \"\"\"Applies a proximal update to a parameter group.\"\"\"\n",
    "        for p in group['params']:\n",
    "            if p.grad is None:\n",
    "                continue\n",
    "            state = self.state[p]\n",
    "            p.data.add_(state['proximal'] + state['grad_delta'], alpha=-group['lr'])\n",
    "        \n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        closure : bool\n",
    "            A closure that reevaluates the model and returns the loss.\n",
    "        \"\"\"\n",
    "        # set the initial (global) weights and proximal term before we update the client optimizer\n",
    "        for group in self.param_groups:\n",
    "            for i, p in enumerate(group['params']):\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                state = self.state[p]\n",
    "                if 'initial_weights' not in state:\n",
    "                    state['initial_weights'] = torch.clone(p.data).detach()\n",
    "                state['proximal'] = self.mu * (p.data - state['initial_weights'])\n",
    "                if 'average_gradient' not in state:\n",
    "                    state['average_gradient'] = torch.clone(self.average_gradients[i]).detach()\n",
    "                state['grad_delta'] = state['average_gradient'] - p.grad.data\n",
    "\n",
    "        loss = self.optimizer.step(closure=closure)\n",
    "        for group in self.param_groups:\n",
    "            self._update(group)\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "class FedNovaSolver(Optimizer):\n",
    "    \"\"\"Implements FedNova local solver.\n",
    "    \n",
    "    Args:\n",
    "        optimizer (torch.optim.Optimizer): local optimizer.\n",
    "        mu (float): proximal term weight (default: 0)\n",
    "\n",
    "    __ https://arxiv.org/pdf/2007.07481.pdf\n",
    "        \n",
    "    Example:\n",
    "        >>> # train a model locally for a client\n",
    "        >>> client_optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "        >>> client_optimizer = FedNovaLocal(client_optimizer, mu=0.1)\n",
    "        >>> client_optimizer.zero_grad()\n",
    "        >>> loss_fn(model(input), target).backward()\n",
    "        >>> client_optimizer.step()\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 optimizer,\n",
    "                 mu=0):\n",
    "        if mu < 0.0:\n",
    "            raise ValueError(f'Invalid mu value: {mu}')\n",
    "        self.optimizer = optimizer\n",
    "        self.mu = mu\n",
    "        self.param_groups = self.optimizer.param_groups\n",
    "        self.state = self.optimizer.state\n",
    "    \n",
    "    def _update(self, group):\n",
    "        \"\"\"Applies a proximal update to a parameter group.\"\"\"\n",
    "        for p in group['params']:\n",
    "            if p.grad is None:\n",
    "                continue\n",
    "            state = self.state[p]\n",
    "            p.data.add_(state['proximal'], alpha=-group['lr'])\n",
    "        \n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        closure : bool\n",
    "            A closure that reevaluates the model and returns the loss.\n",
    "        \"\"\"\n",
    "        # set the initial (global) weights and proximal term before we update the client optimizer\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                state = self.state[p]\n",
    "                if 'initial_weights' not in state:\n",
    "                    state['initial_weights'] = torch.clone(p.data).detach()\n",
    "                state['proximal'] = self.mu * (p.data - state['initial_weights'])\n",
    "\n",
    "        # update the weights and gradient with the client optimizer\n",
    "        loss = self.optimizer.step(closure=closure)\n",
    "        \n",
    "        # update the weights by adding the (negative) proximal term\n",
    "        for group in self.param_groups:\n",
    "            self._update(group)\n",
    "        \n",
    "        # accumualte gradients after calculating loss\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                \n",
    "                momentum = group.get('momentum', 0)\n",
    "                \n",
    "                state = self.state[p]\n",
    "                if 'local_step' not in state:\n",
    "                    state['local_step'] = 0\n",
    "                state['local_step'] += 1\n",
    "                \n",
    "                # momentum (1 - p^t) / (1 - p)\n",
    "                a = (1 - momentum ** state['local_step']) / (1 - momentum)\n",
    "                # proximal (1 - lr * mu)^t\n",
    "                a *= (1 - group['lr'] * self.mu) ** (state['local_step']-1)\n",
    "                # record the norm factor (a) to divide the l1-norm during aggregation\n",
    "                if 'norm_factor' not in state:\n",
    "                    state['norm_factor'] = []\n",
    "                state['norm_factor'].append(a)\n",
    "                \n",
    "                if 'cgrad' not in state:\n",
    "                    state['cgrad'] = torch.clone(p.grad.data).detach()\n",
    "                    state['cgrad'].mul_(group['lr']) # do we need the lr ?\n",
    "                    state['cgrad'].mul_(a) # G * a\n",
    "                else:\n",
    "                    state['cgrad'].add_(p.grad.data, alpha=group['lr'])\n",
    "                    state['cgrad'].mul_(a) # G * a\n",
    "                    \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "class SCAFFOLDSolver(Optimizer):\n",
    "    \"\"\"Implements SCAFFOLD local solver.\n",
    "    \n",
    "    Args:\n",
    "        optimizer (torch.optim.Optimizer): local optimizer.\n",
    "\n",
    "    __ https://arxiv.org/pdf/1910.06378.pdf\n",
    "        \n",
    "    Example:\n",
    "        >>> # train a model locally for a client\n",
    "        >>> client_optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "        >>> client_optimizer = SCAFFOLDSolver(client_optimizer, mu=0.1)\n",
    "        >>> client_optimizer.zero_grad()\n",
    "        >>> loss_fn(model(input), target).backward()\n",
    "        >>> client_optimizer.step()\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, optimizer, control_global, control_local):\n",
    "        if isinstance(control_global, torch.Tensor):\n",
    "            raise TypeError(\"control_global argument given to the optimizer should be \"\n",
    "                            \"an iterable of Tensors or lists, but got \" +\n",
    "                            torch.typename(control_global))\n",
    "        if isinstance(control_global[0], torch.Tensor):\n",
    "            control_global = [control_global]\n",
    "        if isinstance(control_local, torch.Tensor):\n",
    "            raise TypeError(\"control_local argument given to the optimizer should be \"\n",
    "                            \"an iterable of Tensors or lists, but got \" +\n",
    "                            torch.typename(control_local))\n",
    "        if isinstance(control_local[0], torch.Tensor):\n",
    "            control_local = [control_local]\n",
    "            \n",
    "        self.optimizer = optimizer\n",
    "        self.control_global = control_global\n",
    "        self.control_local = control_local\n",
    "        self.param_groups = self.optimizer.param_groups\n",
    "        self.state = self.optimizer.state\n",
    "        \n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        closure : bool\n",
    "            A closure that reevaluates the model and returns the loss.\n",
    "        \"\"\"\n",
    "        # update the weights and gradient with the client optimizer\n",
    "        loss = self.optimizer.step(closure=closure)\n",
    "        \n",
    "        # (3) update the weights by adding the control variate correction term\n",
    "        for group, group_global, group_local in zip(self.param_groups, self.control_global, self.control_local):\n",
    "            for p, c, ci in zip(group, group_global, group_local):\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                p.data.add_(c - ci, -group['lr'])\n",
    "                    \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "044a2135",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# servers/base.py\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "\n",
    "\n",
    "class BaseFederater:\n",
    "    \"\"\"Base Federater.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "    dataset : torch.utils.data.Dataset\n",
    "    num_clients (K) : int (default=100)\n",
    "        Number of clients to partition `dataset`.\n",
    "    batch_size (B) : int, dict[str, int] (defualt=32)\n",
    "        Number of samples per batch to load on each client. \n",
    "        Can be a dictionary mapping each client ID to it's corresponding batch size\n",
    "        to allow for various batch sizes across clients.\n",
    "    shard_size : int (default=300)\n",
    "    is_iid : bool (default=False)\n",
    "    drop_last : bool (default=True)\n",
    "    num_workers : int (default=0)\n",
    "    device : str (default='cpu')\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 model,\n",
    "                 clients,\n",
    "                 client_optimizer_cls,\n",
    "                 client_optimizer_params,\n",
    "                 server_optimizer=None,\n",
    "                 client_scheduler_cls=None,\n",
    "                 client_scheduler_params=None,\n",
    "                 server_scheduler=None,\n",
    "                 seed=None,\n",
    "                 writer=None):\n",
    "        self.model = model\n",
    "        self.clients = clients\n",
    "        self.client_optimizer_cls = client_optimizer_cls\n",
    "        self.client_optimizer_params = client_optimizer_params\n",
    "        self.server_optimizer = server_optimizer\n",
    "        self.client_scheduler_cls = client_scheduler_cls\n",
    "        self.client_scheduler_params = client_scheduler_params\n",
    "        self.server_scheduler = server_scheduler\n",
    "        self.writer = writer or SummaryWriter()\n",
    "        \n",
    "        self.client_ids = list(self.clients.keys())\n",
    "        self.num_clients = len(self.clients)\n",
    "        self.num_samples = sum([len(c) for c in self.clients.values()]) # n\n",
    "        self.client_weights = [len(c) / self.num_samples for c in self.clients.values()]\n",
    "        \n",
    "        self.device = next(self.model.parameters()).device\n",
    "        self._global_round = 0\n",
    "        self._random_state = np.random.RandomState(seed)\n",
    "        \n",
    "    @property\n",
    "    def global_round(self):\n",
    "        return self._global_round\n",
    "\n",
    "    @global_round.setter\n",
    "    def global_round(self, global_round):\n",
    "        self._global_round = global_round\n",
    "    \n",
    "    def aggregate(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def update(self, \n",
    "               client_ids, \n",
    "               criterion,\n",
    "               num_epochs, \n",
    "               straggler_rate=0):\n",
    "        \"\"\"Performs a full communication round.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        client_ids (S_t): list, np.ndarray\n",
    "            List of client ID's to train.\n",
    "        criterion : nn.Module\n",
    "            Loss function to optimize on each client.\n",
    "        num_epochs (E): int\n",
    "            Number of epochs to train on each client.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        metrics_dict : dict\n",
    "            Dictionary mapping each metric to the average score across `client_ids`\n",
    "        \"\"\"\n",
    "        # send the global model parameters to each client\n",
    "        self.send_model()\n",
    "        \n",
    "        metrics_dict = defaultdict(lambda: 0)\n",
    "        for k in client_ids:\n",
    "            start_time = time.time()\n",
    "            # instantiate client optimizer and scheduler\n",
    "            client = self.clients[k]\n",
    "            client.optimizer = self.get_client_optimizer(client)\n",
    "            client.scheduler = self.get_client_scheduler(client.optimizer)\n",
    "            \n",
    "            # for heterogeneity experiments we can train clients for varying epochs (stragglers)\n",
    "            if self._random_state.random() < straggler_rate:\n",
    "                client_epochs = self._random_state.choice(range(1, num_epochs+1))\n",
    "            else:\n",
    "                client_epochs = num_epochs\n",
    "                \n",
    "            # update the client weights and record the local training metrics\n",
    "            client_metrics_dict = client.update(\n",
    "                criterion,\n",
    "                num_epochs=client_epochs,\n",
    "            )\n",
    "            \n",
    "            # update the summary writer and record loss/acc from the client\n",
    "            elapsed_time = time.time() - start_time\n",
    "            self.writer.add_scalar(f'clients/{k}/elapsed_time', elapsed_time, self.global_round)\n",
    "            for metric, values in client_metrics_dict.items():\n",
    "                for i, value in enumerate(values):\n",
    "                    self.writer.add_scalar(f'client/{k}/round_{self.global_round}/{metric}', \n",
    "                                           value,\n",
    "                                           self.global_round)\n",
    "                metrics_dict[metric] += values[-1] / len(client_ids)\n",
    "        \n",
    "        # aggregate the parameters of the local solvers\n",
    "        self.aggregate()\n",
    "        if self.server_scheduler is not None:\n",
    "            self.server_scheduler.step()\n",
    "        \n",
    "        return metrics_dict\n",
    "        \n",
    "    def fit(self, \n",
    "            num_rounds,\n",
    "            criterion, \n",
    "            num_epochs,\n",
    "            val_dl=None,\n",
    "            C=0.1,\n",
    "            straggler_rate=0,\n",
    "            eval_every_n=1):\n",
    "        \"\"\"Train loop.\"\"\"\n",
    "        start_time = time.time()\n",
    "        # subset a sample of `m` clients each round\n",
    "        m = max(int(np.ceil(self.num_clients * C)), 1)\n",
    "        for t in range(num_rounds):\n",
    "            self.global_round += 1\n",
    "            \n",
    "            # update a subset of clients with the local solver\n",
    "            S = self._random_state.choice(self.client_ids, m, replace=False)\n",
    "            train_metrics = self.update(client_ids=S, \n",
    "                                        criterion=criterion,\n",
    "                                        num_epochs=num_epochs, \n",
    "                                        straggler_rate=straggler_rate)\n",
    "            \n",
    "            # log train summary metrics\n",
    "            elapsed_time = round(time.time() - start_time)\n",
    "            self.writer.add_scalar('train/elapsed_time', elapsed_time, self.global_round)\n",
    "            template_str = f'round {self.global_round} - {elapsed_time}s'\n",
    "            for metric, value in train_metrics.items():\n",
    "                self.writer.add_scalar(f'train/{metric}', value, self.global_round)\n",
    "                template_str += f' - train_{metric} : {value:0.4f}'\n",
    "                \n",
    "            # log validation summary metrics\n",
    "            if eval_every_n is not None and t % eval_every_n == 0 and val_dl is not None:\n",
    "                val_metrics = self.validate(val_dl, criterion)\n",
    "                for metric, value in val_metrics.items():\n",
    "                    self.writer.add_scalar(f'val/{metric}', value, self.global_round)\n",
    "                    template_str += f' - val_{metric} : {value:0.4f}'\n",
    "                \n",
    "            print(template_str)\n",
    "    \n",
    "    def get_client_optimizer(self, client):\n",
    "        \"\"\"Returns a client optimizer (local solver).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        params : iterable\n",
    "            Client parameters to optimize\n",
    "        optimizer_params : dict\n",
    "            Client optimizer hyperparameters\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        torch.optim.Optimizer\n",
    "        \"\"\"\n",
    "        optimizer_params = self.client_optimizer_params or {}\n",
    "        return self.client_optimizer_cls(client.model.parameters(), **optimizer_params)\n",
    "    \n",
    "    def get_client_scheduler(self, optimizer):\n",
    "        \"\"\"Returns a LR scheduler for a client optimizer.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        optimizer : torch.optim.Optimizer\n",
    "            Client optimizer\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        torch.optim.lr_scheduler._LRScheduler or None\n",
    "            Client LR scheduler, or None if not specified\n",
    "        \"\"\"\n",
    "        if self.client_scheduler_cls is not None:\n",
    "            scheduler_params = self.client_scheduler_params or {}\n",
    "            return self.client_scheduler_cls(optimizer, **scheduler_params)\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def validate(self, val_dl, criterion):\n",
    "#         self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        loss = 0 \n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_dl:\n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "                logits = self.model(x)\n",
    "                correct += (logits.argmax(-1) == y).sum().item()\n",
    "                loss += criterion(logits, y).item()\n",
    "                \n",
    "        results = {\n",
    "            'loss': loss / len(val_dl),\n",
    "            'accuracy': correct / len(val_dl.dataset)\n",
    "        }\n",
    "        return results\n",
    "    \n",
    "    def send_model(self, client_ids=None):\n",
    "        \"\"\"Send the current state of the global model to each client.\"\"\"\n",
    "        if client_ids is None:\n",
    "            client_ids = self.client_ids\n",
    "        for client_id in client_ids:\n",
    "            self.clients[client_id].model = deepcopy(self.model)\n",
    "            \n",
    "    def get_gradients(self, client_ids, criterion):\n",
    "        self.send_model(client_ids)\n",
    "        grads = []\n",
    "        for k, client_id in enumerate(client_ids):\n",
    "            client = self.clients[client_id]\n",
    "            client_grads = client.get_gradients(criterion)\n",
    "            grads.append(client_grads)\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "d847fa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FedAvg(BaseFederater):\n",
    "    \"\"\"Federated Averaging (FedAvg)\n",
    "    \n",
    "    https://arxiv.org/pdf/1602.05629.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 model,\n",
    "                 clients,\n",
    "                 client_optimizer_cls,\n",
    "                 client_optimizer_params,\n",
    "                 client_scheduler_cls=None,\n",
    "                 client_scheduler_params=None,\n",
    "                 seed=None,\n",
    "                 writer=None):\n",
    "        super().__init__(model,\n",
    "                         clients,\n",
    "                         client_optimizer_cls=client_optimizer_cls,\n",
    "                         client_optimizer_params=client_optimizer_params,\n",
    "                         client_scheduler_cls=client_scheduler_cls,\n",
    "                         client_scheduler_params=client_scheduler_params,\n",
    "                         seed=seed,\n",
    "                         writer=writer)\n",
    "    \n",
    "    def aggregate(self):\n",
    "        global_state = {} # self.model.state_dict()\n",
    "        for k, (client_id, client) in enumerate(self.clients.items()):\n",
    "            local_state = client.model.state_dict()\n",
    "            for layer_name, param in local_state.items():\n",
    "                if k == 0:\n",
    "                    global_state[layer_name] = self.client_weights[k] * param\n",
    "                else:\n",
    "                    global_state[layer_name] += self.client_weights[k] * param\n",
    "\n",
    "        self.model.load_state_dict(global_state)\n",
    "                    \n",
    "\n",
    "class FedProx(BaseFederater):\n",
    "    \"\"\"FedProx\n",
    "    \n",
    "    https://arxiv.org/pdf/1812.06127.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 model,\n",
    "                 clients,\n",
    "                 client_optimizer_cls,\n",
    "                 client_optimizer_params,\n",
    "                 server_optimizer,\n",
    "                 mu=0,\n",
    "                 client_scheduler_cls=None,\n",
    "                 client_scheduler_params=None,\n",
    "                 server_scheduler=None,\n",
    "                 seed=None,\n",
    "                 writer=None):\n",
    "        super().__init__(model,\n",
    "                         clients,\n",
    "                         client_optimizer_cls,\n",
    "                         client_optimizer_params,\n",
    "                         server_optimizer=server_optimizer,\n",
    "                         server_scheduler=server_scheduler,\n",
    "                         client_scheduler_cls=client_scheduler_cls,\n",
    "                         client_scheduler_params=client_scheduler_params,\n",
    "                         seed=seed,\n",
    "                         writer=writer)\n",
    "        self.mu = mu\n",
    "            \n",
    "    def get_client_optimizer(self, client):\n",
    "        optimizer_params = self.client_optimizer_params or {}\n",
    "        client_optimizer = self.client_optimizer_cls(client.model.parameters(), **optimizer_params)\n",
    "        return FedProxSolver(client_optimizer, mu=self.mu)\n",
    "    \n",
    "    def aggregate(self):\n",
    "        self.server_optimizer.zero_grad()\n",
    "        for k, client in enumerate(self.clients.values()):\n",
    "            for p_server, p_client in zip(self.model.parameters(), client.model.parameters()):\n",
    "                if p_server.requires_grad:\n",
    "                    if k == 0:\n",
    "                        p_server.grad = self.client_weights[k] * (p_server.data - p_client.data)\n",
    "                    else:\n",
    "                        p_server.grad.data.add_(p_server.data - p_client.data, alpha=self.client_weights[k])\n",
    "        \n",
    "        self.server_optimizer.step()\n",
    "\n",
    "    \n",
    "class FedOpt(BaseFederater):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 clients,\n",
    "                 client_optimizer_cls,\n",
    "                 client_optimizer_params,\n",
    "                 server_optimizer,\n",
    "                 server_scheduler=None,\n",
    "                 client_scheduler_cls=None,\n",
    "                 client_scheduler_params=None,\n",
    "                 seed=None,\n",
    "                 writer=None):\n",
    "        super().__init__(model,\n",
    "                         clients,\n",
    "                         client_optimizer_cls,\n",
    "                         client_optimizer_params,\n",
    "                         server_optimizer=server_optimizer,\n",
    "                         server_scheduler=server_scheduler,\n",
    "                         client_scheduler_cls=client_scheduler_cls,\n",
    "                         client_scheduler_params=client_scheduler_params,\n",
    "                         seed=seed,\n",
    "                         writer=writer)\n",
    "    \n",
    "    def aggregate(self):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        self.server_optimizer.zero_grad()\n",
    "        # iterate through each client\n",
    "        for k, client in enumerate(self.clients.values()):\n",
    "            for p_server, p_client in zip(self.model.parameters(), client.model.parameters()):\n",
    "                if p_server.requires_grad:\n",
    "                    if k == 0:\n",
    "                        p_server.grad = client_weights[k] * (p_server.data - p_client.data)\n",
    "                    else:\n",
    "                        p_server.grad.add_(p_server.data - p_client.data, alpha=self.client_weights[k])\n",
    "        \n",
    "        self.server_optimizer.step()\n",
    "        \n",
    "        \n",
    "class FedNova(BaseFederater):\n",
    "    \"\"\"FedNova\n",
    "    \n",
    "    https://arxiv.org/pdf/2007.07481.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 clients,\n",
    "                 server_optimizer,\n",
    "                 client_optimizer_cls,\n",
    "                 client_optimizer_params,\n",
    "                 mu=0,\n",
    "                 server_scheduler=None,\n",
    "                 client_scheduler_cls=None,\n",
    "                 client_scheduler_params=None,\n",
    "                 seed=None,\n",
    "                 writer=None):\n",
    "        super().__init__(model,\n",
    "                         clients,\n",
    "                         client_optimizer_cls=client_optimizer_cls,\n",
    "                         client_optimizer_params=client_optimizer_params,\n",
    "                         server_optimizer=server_optimizer,\n",
    "                         server_scheduler=server_scheduler,\n",
    "                         client_scheduler_cls=client_scheduler_cls,\n",
    "                         client_scheduler_params=client_scheduler_params,\n",
    "                         seed=seed,\n",
    "                         writer=writer)\n",
    "        self.mu = mu\n",
    "        \n",
    "    def get_client_optimizer(self, client):\n",
    "        optimizer_params = self.client_optimizer_params or {}\n",
    "        client_optimizer = self.client_optimizer_cls(client.model.parameters(), **optimizer_params)\n",
    "        client_optimizer = FedNovaSolver(client_optimizer, mu=self.mu)\n",
    "        return client_optimizer\n",
    "    \n",
    "    def aggregate(self):\n",
    "        \"\"\" \"\"\"\n",
    "        \n",
    "        self.server_optimizer.zero_grad()\n",
    "        # iterate through each client and set gradients\n",
    "        for k, client in enumerate(self.clients.values()):\n",
    "            # skip clients with no optimizer\n",
    "            # we may want to use the weights of the local model instead\n",
    "            if client.optimizer is None:\n",
    "                continue\n",
    "            for group_server, group_client in zip(self.server_optimizer.param_groups, \n",
    "                                                  client.optimizer.param_groups):\n",
    "                for p_server, p_client in zip(group_server['params'], group_client['params']):\n",
    "                    if p_server.requires_grad:\n",
    "                        state = client.optimizer.state[p_client]\n",
    "                        w = self.client_weights[k]\n",
    "                        G_a = state['cgrad']\n",
    "                        a = torch.tensor(state['norm_factor'])\n",
    "                        d = G_a / a.abs().sum()\n",
    "                        tau_eff = client.local_steps\n",
    "                        if p_server.grad is None:\n",
    "                            p_server.grad = tau_eff * w * d  # need to take lr off of G ? jk lr is necessary for client (local)\n",
    "                        else:\n",
    "                            p_server.grad.data.add_(d, alpha=tau_eff * w)\n",
    "\n",
    "        self.server_optimizer.step()\n",
    "        \n",
    "        \n",
    "class FedDane(BaseFederater):\n",
    "    def __init__(self, \n",
    "                 model,\n",
    "                 clients,\n",
    "                 client_optimizer_cls,\n",
    "                 client_optimizer_params,\n",
    "                 server_optimizer,\n",
    "                 mu=0,\n",
    "                 client_scheduler_cls=None,\n",
    "                 client_scheduler_params=None,\n",
    "                 server_scheduler=None,\n",
    "                 seed=None,\n",
    "                 writer=None):\n",
    "        super().__init__(model,\n",
    "                         clients,\n",
    "                         client_optimizer_cls,\n",
    "                         client_optimizer_params,\n",
    "                         server_optimizer=server_optimizer,\n",
    "                         server_scheduler=server_scheduler,\n",
    "                         client_scheduler_cls=client_scheduler_cls,\n",
    "                         client_scheduler_params=client_scheduler_params,\n",
    "                         seed=seed,\n",
    "                         writer=writer)\n",
    "        self.mu = mu\n",
    "        self.average_gradients = None\n",
    "            \n",
    "    def get_client_optimizer(self, client):\n",
    "        optimizer_params = self.client_optimizer_params or {}\n",
    "        client_optimizer = self.client_optimizer_cls(client.model.parameters(), **optimizer_params)\n",
    "        client_optimizer = FedDaneSolver(client_optimizer, \n",
    "                                         average_gradients=self.average_gradients,\n",
    "                                         mu=self.mu)\n",
    "        return client_optimizer\n",
    "    \n",
    "    def fit(self, \n",
    "            num_rounds,\n",
    "            criterion, \n",
    "            num_epochs,\n",
    "            val_dl=None,\n",
    "            C=0.1,\n",
    "            straggler_rate=0,\n",
    "            eval_every_n=1):\n",
    "        # subset a sample of `m` clients each round\n",
    "        m = max(int(np.ceil(self.num_clients * C)), 1)\n",
    "        \n",
    "        for t in range(num_rounds):\n",
    "            self.global_round += 1\n",
    "            \n",
    "            # calculate the average gradient on a subset of clients\n",
    "            S_grad = self._random_state.choice(self.client_ids, m, replace=False)\n",
    "            self.set_average_gradients(S_grad, criterion)\n",
    "            \n",
    "            # update a subset of clients with the local solver\n",
    "            S = self._random_state.choice(self.client_ids, m, replace=False)\n",
    "            train_metrics = self.update(client_ids=S, \n",
    "                                        criterion=criterion,\n",
    "                                        num_epochs=num_epochs, \n",
    "                                        straggler_rate=straggler_rate)\n",
    "            \n",
    "            if eval_every_n is not None and t % eval_every_n == 0 and val_dl is not None:\n",
    "                template_str = f'round {self.global_round}'\n",
    "                val_metrics = self.validate(val_dl, criterion)\n",
    "                for metric, value in train_metrics.items():\n",
    "                    self.writer.add_scalar(f'train/{metric}', value, self.global_round)\n",
    "                    template_str += f' - train_{metric} : {value:0.4f}'\n",
    "                for metric, value in val_metrics.items():\n",
    "                    self.writer.add_scalar(f'val/{metric}', value, self.global_round)\n",
    "                    template_str += f' - val_{metric} : {value:0.4f}'\n",
    "                \n",
    "                print(template_str)\n",
    "    \n",
    "    def aggregate(self):\n",
    "        self.server_optimizer.zero_grad()\n",
    "        for k, client in enumerate(self.clients.values()):\n",
    "            for p_server, p_client in zip(self.model.parameters(), client.model.parameters()):\n",
    "                if p_server.requires_grad:\n",
    "                    if k == 0:\n",
    "                        p_server.grad = self.client_weights[k] * (p_server.data - p_client.data)\n",
    "                    else:\n",
    "                        p_server.grad.add_(p_server.data - p_client.data, alpha=self.client_weights[k])\n",
    "        \n",
    "        self.server_optimizer.step()\n",
    "        \n",
    "    def set_average_gradients(self, client_ids, criterion):\n",
    "        grads = self.get_gradients(client_ids, criterion)\n",
    "        average_gradients = [0] * len(grads[0])\n",
    "        for client_grads in grads:\n",
    "            for i, g in enumerate(client_grads):\n",
    "                average_gradients[i] += g\n",
    "        self.average_gradients = [g / len(grads) for g in average_gradients]\n",
    "        \n",
    "        \n",
    "        \n",
    "# scaffold.py\n",
    "class SCAFFOLD(BaseFederater):\n",
    "    \"\"\"SCAFFOLD\n",
    "    \n",
    "    https://arxiv.org/pdf/1910.06378.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 model,\n",
    "                 clients,\n",
    "                 client_optimizer_cls,\n",
    "                 client_optimizer_params,\n",
    "                 server_optimizer,\n",
    "                 option='II',\n",
    "                 client_scheduler_cls=None,\n",
    "                 client_scheduler_params=None,\n",
    "                 server_scheduler=None,\n",
    "                 seed=None,\n",
    "                 writer=None):\n",
    "        super().__init__(model,\n",
    "                         clients,\n",
    "                         client_optimizer_cls,\n",
    "                         client_optimizer_params,\n",
    "                         server_optimizer=server_optimizer,\n",
    "                         server_scheduler=server_scheduler,\n",
    "                         client_scheduler_cls=client_scheduler_cls,\n",
    "                         client_scheduler_params=client_scheduler_params,\n",
    "                         seed=seed,\n",
    "                         writer=writer)\n",
    "        self.option = option\n",
    "        self.control_server = [torch.zeros_like(p.data) for p in model.parameters()]\n",
    "\n",
    "    def send_model(self, client_ids=None):\n",
    "        \"\"\"Send the current state of the global model to each client.\"\"\"\n",
    "        if client_ids is None:\n",
    "            client_ids = self.client_ids\n",
    "        for client_id in client_ids:\n",
    "            self.clients[client_id].model = deepcopy(self.model)\n",
    "            self.clients[client_id].control_server = self.control_server\n",
    "            \n",
    "    def get_client_optimizer(self, client):\n",
    "        optimizer_params = self.client_optimizer_params or {}\n",
    "        client_optimizer = self.client_optimizer_cls(client.model.parameters(), **optimizer_params)\n",
    "        client_optimizer = SCAFFOLDSolver(client_optimizer, \n",
    "                                          control_global=self.control_server, \n",
    "                                          control_local=client.control)\n",
    "        return client_optimizer\n",
    "    \n",
    "    def aggregate(self):\n",
    "        # (5) update global parameters\n",
    "        self.server_optimizer.zero_grad()\n",
    "        for k, client in enumerate(self.clients.values()):\n",
    "            for p_server, p_client, c_client in zip(self.model.parameters(), client.model.parameters(), client.control):\n",
    "                if p_server.requires_grad:\n",
    "                    if k == 0:\n",
    "                        p_server.grad = self.client_weights[k] * (p_server.data - p_client.data)\n",
    "                    else:\n",
    "                        p_server.grad.data.add_(p_server.data - p_client.data, alpha=self.client_weights[k])\n",
    "        \n",
    "        self.server_optimizer.step()\n",
    "        \n",
    "        # (5) update global control variate\n",
    "        for client in self.clients.values():\n",
    "            for c, ci_delta in zip(self.control_server, client.control_delta):\n",
    "                c.data.add_(ci_delta, 1/self.num_clients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "00056244",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \n",
    "    fed_avg = {\n",
    "        'mnist': {\n",
    "            'clients': {\n",
    "                'num_clients': 100,\n",
    "                'shard_size': 300,\n",
    "                'batch_size': 10,\n",
    "                'is_iid': False,\n",
    "            },\n",
    "            'client_optimizer': 'SGD',\n",
    "            'client_optimizer_params': {\n",
    "                'lr': 0.1,\n",
    "            },\n",
    "            'federater': {\n",
    "                'C': 0.1\n",
    "            },\n",
    "            'fit': {\n",
    "                'num_rounds': 240,\n",
    "                'num_epochs': 20\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    fed_prox = {\n",
    "        'mnist': {\n",
    "            'clients': {\n",
    "                'num_clients': 100,\n",
    "                'shard_size': 300,\n",
    "                'batch_size': 10,\n",
    "                'is_iid': False,\n",
    "            },\n",
    "            'client_optimizer': 'SGD',\n",
    "            'client_optimizer_params': {\n",
    "                'lr': 0.03,\n",
    "            },\n",
    "            'federater': {\n",
    "                'C': 0.1,\n",
    "                'mu': 0.1\n",
    "            },\n",
    "            'fit': {\n",
    "                'num_rounds': 1,\n",
    "                'num_epochs': 1,\n",
    "#                 'straggler_rate': 0.5,\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "# config = Config()\n",
    "config = {\n",
    "    'fedavg': {\n",
    "        'mnist': {\n",
    "            'clients': {\n",
    "                'num_clients': 100,\n",
    "                'shard_size': 300,\n",
    "                'batch_size': 10,\n",
    "                'is_iid': True,\n",
    "            },\n",
    "            'client_optimizer': 'SGD',\n",
    "            'client_optimizer_params': {\n",
    "                'lr': 0.1,\n",
    "            },\n",
    "            'federater': {\n",
    "                'C': 0.1\n",
    "            },\n",
    "            'fit': {\n",
    "                'num_rounds': 50,\n",
    "                'num_epochs': 20\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'fedprox': {\n",
    "        'mnist': {\n",
    "            'clients': {\n",
    "                'num_clients': 100,\n",
    "                'shard_size': 300,\n",
    "                'batch_size': 10,\n",
    "                'is_iid': False,\n",
    "            },\n",
    "            'server_optimizer': 'SGD',\n",
    "            'server_optimizer_params': {\n",
    "                'lr': 1\n",
    "            },\n",
    "            'client_optimizer': 'SGD',\n",
    "            'client_optimizer_params': {\n",
    "                'lr': 0.03,\n",
    "            },\n",
    "            'federater': {\n",
    "                'C': 0.1,\n",
    "                'mu': 0.1\n",
    "            },\n",
    "            'fit': {\n",
    "                'num_rounds': 100,\n",
    "                'num_epochs': 20,\n",
    "#                 'straggler_rate': 0.5,\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'fedadam': {\n",
    "        'mnist': {\n",
    "            'clients': {\n",
    "                'num_clients': 100,\n",
    "                'shard_size': 300,\n",
    "                'batch_size': 10,\n",
    "                'is_iid': False,\n",
    "            },\n",
    "            'client_optimizer': 'SGD',\n",
    "            'client_optimizer_params': {\n",
    "                'lr': 0.01,\n",
    "            },\n",
    "            'server_optimizer': 'adam',\n",
    "            'server_optimizer_params': {\n",
    "                'lr': 1,\n",
    "            },\n",
    "            'federater': {\n",
    "                'C': 0.1\n",
    "            },\n",
    "            'fit': {\n",
    "                'num_rounds': 240,\n",
    "                'num_epochs': 20\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "9b20acfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "\n",
    "def create_experiment_name(method, dataset, params=None):\n",
    "    params = params or {}\n",
    "    experiment_name = f'{method}_{dataset}'\n",
    "    if params.get('clients'):\n",
    "        if params['clients'].get('is_iid'):\n",
    "            if params['clients']['is_iid']:\n",
    "                experiment_name += \"_iid\"\n",
    "            else:\n",
    "                experiment_name += \"_noniid\"\n",
    "        if params['clients'].get('num_clients'):\n",
    "            experiment_name += f\"_K={params['clients']['num_clients']}\"\n",
    "        if params['clients'].get('batch_size'):\n",
    "            experiment_name += f\"_B={params['clients']['batch_size']}\"\n",
    "    if params.get('fit'):\n",
    "        if params['fit'].get('num_rounds'):\n",
    "            experiment_name += f\"_T={params['fit']['num_rounds']}\"\n",
    "        if params['fit'].get('num_epochs'):\n",
    "            experiment_name += f\"_E={params['fit']['num_epochs']}\"\n",
    "    if params.get('server_optimizer'):\n",
    "            experiment_name += f\"_SOPT={params['server_optimizer']}\"\n",
    "    if params.get('client_optimizer'):\n",
    "        experiment_name += f\"_COPT={params['client_optimizer']}\"\n",
    "    \n",
    "    experiment_name += f\"_{datetime.datetime.now().strftime('%Y%m%d_%H%M')}\"\n",
    "    return experiment_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e9af4176",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment : tmp\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [32, 784, 5, 5], expected input[10, 1, 28, 28] to have 784 channels, but got 1 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-132-f6957d8f06c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m                     \u001b[0mclient_optimizer_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclient_optimizer_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                     **fed_params)\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mfederater\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_rounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-127-62c9c5122ac3>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, num_rounds, criterion, num_epochs, val_dl, straggler_rate, eval_every_n)\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;31m# update a subset of clients with the local solver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0mS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_random_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             train_metrics = self.update(client_ids=S, \n\u001b[0m\u001b[1;32m    148\u001b[0m                                         \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                                         \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-127-62c9c5122ac3>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, client_ids, criterion, num_epochs, straggler_rate)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;31m# update the client weights and record the local training metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             client_metrics_dict = client.update(\n\u001b[0m\u001b[1;32m    110\u001b[0m                 \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclient_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-3e02d27f4cd6>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, criterion, num_epochs)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-122-5790c38984d9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    417\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 419\u001b[0;31m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0m\u001b[1;32m    420\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [32, 784, 5, 5], expected input[10, 1, 28, 28] to have 784 channels, but got 1 channels instead"
     ]
    }
   ],
   "source": [
    "# Fed avg baseline\n",
    "method = 'fedavg'\n",
    "dataset = 'mnist'\n",
    "seed = 42069\n",
    "device = 'cuda:0'\n",
    "num_workers = 0\n",
    "\n",
    "experiment_name = create_experiment_name(method, dataset, config[method][dataset])\n",
    "experiment_name = 'tmp'\n",
    "print(f'Experiment : {experiment_name}')\n",
    "\n",
    "writer = SummaryWriter(os.path.join('logs', experiment_name))\n",
    "\n",
    "set_state(seed)\n",
    "client_params = config[method][dataset]['clients']\n",
    "clients = get_clients(\n",
    "    train_ds, \n",
    "    num_workers=num_workers,\n",
    "    seed=seed,\n",
    "    device=device,\n",
    "    **client_params,\n",
    ")\n",
    "\n",
    "model = CNN()\n",
    "# client optimizer\n",
    "# client_optimizer_cls = torch.optim.SGD\n",
    "# client_optimizer_params = {\n",
    "#     'lr': 0.1,\n",
    "# }\n",
    "client_optimizer_cls = getattr(torch.optim, config[method][dataset]['client_optimizer'])\n",
    "client_optimizer_params = config[method][dataset]['client_optimizer_params']\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "fed_params = config[method][dataset]['federater']\n",
    "fed_params['seed'] = seed\n",
    "# fed_params = {\n",
    "#     'seed': seed,\n",
    "#     'C': 0.1,\n",
    "# }\n",
    "num_rounds = 1#config[method][dataset]['fit']['num_rounds']\n",
    "num_epochs = 1#config[method][dataset]['fit']['num_epochs']\n",
    "\n",
    "federater = FedAvg(model,\n",
    "                    clients=clients,\n",
    "                    client_optimizer_cls=client_optimizer_cls,\n",
    "                    client_optimizer_params=client_optimizer_params,\n",
    "                    **fed_params)\n",
    "federater.fit(num_rounds=num_rounds, criterion=criterion, num_epochs=num_epochs, val_dl=test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "45d30976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment : tmp_prox\n",
      "round 1 - 32s - train_loss : 0.0375 - train_accuracy : 0.9913 - val_loss : 2.2986 - val_accuracy : 0.1739\n",
      "round 2 - 32s - train_loss : 0.0208 - train_accuracy : 0.9957 - val_loss : 2.2849 - val_accuracy : 0.2173\n",
      "round 3 - 32s - train_loss : 0.0232 - train_accuracy : 0.9950 - val_loss : 2.2678 - val_accuracy : 0.2514\n",
      "round 4 - 32s - train_loss : 0.0159 - train_accuracy : 0.9978 - val_loss : 2.2504 - val_accuracy : 0.2188\n",
      "round 5 - 32s - train_loss : 0.0208 - train_accuracy : 0.9958 - val_loss : 2.2265 - val_accuracy : 0.2835\n",
      "round 6 - 32s - train_loss : 0.0218 - train_accuracy : 0.9958 - val_loss : 2.1943 - val_accuracy : 0.2985\n",
      "round 7 - 32s - train_loss : 0.0175 - train_accuracy : 0.9958 - val_loss : 2.1677 - val_accuracy : 0.3214\n",
      "round 8 - 33s - train_loss : 0.0154 - train_accuracy : 0.9978 - val_loss : 2.1320 - val_accuracy : 0.3983\n",
      "round 9 - 33s - train_loss : 0.0319 - train_accuracy : 0.9917 - val_loss : 2.0957 - val_accuracy : 0.4159\n",
      "round 10 - 33s - train_loss : 0.0157 - train_accuracy : 0.9968 - val_loss : 2.0641 - val_accuracy : 0.4031\n",
      "round 11 - 32s - train_loss : 0.0172 - train_accuracy : 0.9970 - val_loss : 2.0218 - val_accuracy : 0.4272\n",
      "round 12 - 33s - train_loss : 0.0169 - train_accuracy : 0.9962 - val_loss : 1.9784 - val_accuracy : 0.4994\n",
      "round 13 - 32s - train_loss : 0.0148 - train_accuracy : 0.9972 - val_loss : 1.9365 - val_accuracy : 0.5200\n",
      "round 14 - 32s - train_loss : 0.0426 - train_accuracy : 0.9905 - val_loss : 1.8902 - val_accuracy : 0.5288\n",
      "round 15 - 32s - train_loss : 0.0402 - train_accuracy : 0.9933 - val_loss : 1.8459 - val_accuracy : 0.5592\n",
      "round 16 - 33s - train_loss : 0.0208 - train_accuracy : 0.9950 - val_loss : 1.8077 - val_accuracy : 0.5612\n",
      "round 17 - 32s - train_loss : 0.0115 - train_accuracy : 0.9977 - val_loss : 1.7624 - val_accuracy : 0.5805\n",
      "round 18 - 32s - train_loss : 0.0241 - train_accuracy : 0.9947 - val_loss : 1.7155 - val_accuracy : 0.6281\n",
      "round 19 - 32s - train_loss : 0.0124 - train_accuracy : 0.9980 - val_loss : 1.6755 - val_accuracy : 0.6182\n",
      "round 20 - 32s - train_loss : 0.0188 - train_accuracy : 0.9948 - val_loss : 1.6319 - val_accuracy : 0.6490\n",
      "round 21 - 32s - train_loss : 0.0239 - train_accuracy : 0.9947 - val_loss : 1.5818 - val_accuracy : 0.7050\n",
      "round 22 - 33s - train_loss : 0.0136 - train_accuracy : 0.9972 - val_loss : 1.5395 - val_accuracy : 0.7104\n",
      "round 23 - 33s - train_loss : 0.0240 - train_accuracy : 0.9932 - val_loss : 1.4959 - val_accuracy : 0.7307\n",
      "round 24 - 33s - train_loss : 0.0295 - train_accuracy : 0.9927 - val_loss : 1.4600 - val_accuracy : 0.7233\n",
      "round 25 - 33s - train_loss : 0.0194 - train_accuracy : 0.9943 - val_loss : 1.4128 - val_accuracy : 0.7226\n",
      "round 26 - 33s - train_loss : 0.0228 - train_accuracy : 0.9943 - val_loss : 1.3622 - val_accuracy : 0.7589\n",
      "round 27 - 32s - train_loss : 0.0186 - train_accuracy : 0.9970 - val_loss : 1.3258 - val_accuracy : 0.7530\n",
      "round 28 - 32s - train_loss : 0.0105 - train_accuracy : 0.9980 - val_loss : 1.2826 - val_accuracy : 0.7704\n",
      "round 29 - 33s - train_loss : 0.0204 - train_accuracy : 0.9943 - val_loss : 1.2465 - val_accuracy : 0.7816\n",
      "round 30 - 33s - train_loss : 0.0288 - train_accuracy : 0.9932 - val_loss : 1.2091 - val_accuracy : 0.8320\n",
      "round 31 - 33s - train_loss : 0.0221 - train_accuracy : 0.9935 - val_loss : 1.1681 - val_accuracy : 0.8482\n",
      "round 32 - 33s - train_loss : 0.0103 - train_accuracy : 0.9983 - val_loss : 1.1358 - val_accuracy : 0.8427\n",
      "round 33 - 33s - train_loss : 0.0092 - train_accuracy : 0.9987 - val_loss : 1.1073 - val_accuracy : 0.8254\n",
      "round 34 - 33s - train_loss : 0.0075 - train_accuracy : 0.9992 - val_loss : 1.0738 - val_accuracy : 0.7978\n",
      "round 35 - 33s - train_loss : 0.0067 - train_accuracy : 0.9998 - val_loss : 1.0360 - val_accuracy : 0.8003\n",
      "round 36 - 33s - train_loss : 0.0144 - train_accuracy : 0.9970 - val_loss : 1.0041 - val_accuracy : 0.8333\n",
      "round 37 - 33s - train_loss : 0.0102 - train_accuracy : 0.9982 - val_loss : 0.9637 - val_accuracy : 0.8755\n",
      "round 38 - 33s - train_loss : 0.0455 - train_accuracy : 0.9893 - val_loss : 0.9379 - val_accuracy : 0.8559\n",
      "round 39 - 32s - train_loss : 0.0107 - train_accuracy : 0.9978 - val_loss : 0.9111 - val_accuracy : 0.8714\n",
      "round 40 - 32s - train_loss : 0.0246 - train_accuracy : 0.9933 - val_loss : 0.8910 - val_accuracy : 0.8560\n",
      "round 41 - 33s - train_loss : 0.0153 - train_accuracy : 0.9962 - val_loss : 0.8707 - val_accuracy : 0.8453\n",
      "round 42 - 33s - train_loss : 0.0111 - train_accuracy : 0.9982 - val_loss : 0.8477 - val_accuracy : 0.8402\n",
      "round 43 - 32s - train_loss : 0.0136 - train_accuracy : 0.9975 - val_loss : 0.8138 - val_accuracy : 0.8861\n",
      "round 44 - 32s - train_loss : 0.0090 - train_accuracy : 0.9978 - val_loss : 0.7831 - val_accuracy : 0.8953\n",
      "round 45 - 32s - train_loss : 0.0178 - train_accuracy : 0.9955 - val_loss : 0.7646 - val_accuracy : 0.8965\n",
      "round 46 - 32s - train_loss : 0.0154 - train_accuracy : 0.9955 - val_loss : 0.7362 - val_accuracy : 0.9082\n",
      "round 47 - 32s - train_loss : 0.0108 - train_accuracy : 0.9978 - val_loss : 0.7217 - val_accuracy : 0.9059\n",
      "round 48 - 32s - train_loss : 0.0075 - train_accuracy : 0.9983 - val_loss : 0.7023 - val_accuracy : 0.9093\n",
      "round 49 - 33s - train_loss : 0.0107 - train_accuracy : 0.9970 - val_loss : 0.6808 - val_accuracy : 0.9094\n",
      "round 50 - 33s - train_loss : 0.0147 - train_accuracy : 0.9968 - val_loss : 0.6670 - val_accuracy : 0.9102\n",
      "round 51 - 33s - train_loss : 0.0063 - train_accuracy : 0.9992 - val_loss : 0.6428 - val_accuracy : 0.9114\n",
      "round 52 - 33s - train_loss : 0.0412 - train_accuracy : 0.9925 - val_loss : 0.6316 - val_accuracy : 0.9118\n",
      "round 53 - 33s - train_loss : 0.0138 - train_accuracy : 0.9972 - val_loss : 0.6207 - val_accuracy : 0.9127\n",
      "round 54 - 32s - train_loss : 0.0238 - train_accuracy : 0.9938 - val_loss : 0.6048 - val_accuracy : 0.9165\n",
      "round 55 - 32s - train_loss : 0.0059 - train_accuracy : 0.9992 - val_loss : 0.5898 - val_accuracy : 0.9127\n",
      "round 56 - 32s - train_loss : 0.0176 - train_accuracy : 0.9953 - val_loss : 0.5801 - val_accuracy : 0.9139\n",
      "round 57 - 32s - train_loss : 0.0077 - train_accuracy : 0.9983 - val_loss : 0.5584 - val_accuracy : 0.9164\n",
      "round 58 - 32s - train_loss : 0.0088 - train_accuracy : 0.9977 - val_loss : 0.5410 - val_accuracy : 0.9205\n",
      "round 59 - 32s - train_loss : 0.0259 - train_accuracy : 0.9940 - val_loss : 0.5302 - val_accuracy : 0.9178\n",
      "round 60 - 31s - train_loss : 0.0154 - train_accuracy : 0.9960 - val_loss : 0.5212 - val_accuracy : 0.9158\n",
      "round 61 - 31s - train_loss : 0.0236 - train_accuracy : 0.9955 - val_loss : 0.5138 - val_accuracy : 0.9152\n",
      "round 62 - 32s - train_loss : 0.0074 - train_accuracy : 0.9987 - val_loss : 0.4991 - val_accuracy : 0.9121\n",
      "round 63 - 32s - train_loss : 0.0133 - train_accuracy : 0.9958 - val_loss : 0.4920 - val_accuracy : 0.9011\n",
      "round 64 - 31s - train_loss : 0.0130 - train_accuracy : 0.9973 - val_loss : 0.4934 - val_accuracy : 0.8911\n",
      "round 65 - 31s - train_loss : 0.0075 - train_accuracy : 0.9983 - val_loss : 0.4721 - val_accuracy : 0.9059\n",
      "round 66 - 32s - train_loss : 0.0077 - train_accuracy : 0.9978 - val_loss : 0.4721 - val_accuracy : 0.8983\n",
      "round 67 - 32s - train_loss : 0.0113 - train_accuracy : 0.9977 - val_loss : 0.4585 - val_accuracy : 0.9043\n",
      "round 68 - 32s - train_loss : 0.0042 - train_accuracy : 0.9997 - val_loss : 0.4629 - val_accuracy : 0.8925\n",
      "round 69 - 32s - train_loss : 0.0121 - train_accuracy : 0.9970 - val_loss : 0.4439 - val_accuracy : 0.9039\n",
      "round 70 - 32s - train_loss : 0.0107 - train_accuracy : 0.9975 - val_loss : 0.4254 - val_accuracy : 0.9126\n",
      "round 71 - 32s - train_loss : 0.0051 - train_accuracy : 0.9997 - val_loss : 0.4125 - val_accuracy : 0.9137\n",
      "round 72 - 33s - train_loss : 0.0077 - train_accuracy : 0.9982 - val_loss : 0.4049 - val_accuracy : 0.9109\n",
      "round 73 - 33s - train_loss : 0.0060 - train_accuracy : 0.9990 - val_loss : 0.3993 - val_accuracy : 0.9102\n",
      "round 74 - 33s - train_loss : 0.0069 - train_accuracy : 0.9983 - val_loss : 0.3954 - val_accuracy : 0.9095\n",
      "round 75 - 33s - train_loss : 0.0057 - train_accuracy : 0.9985 - val_loss : 0.3837 - val_accuracy : 0.9130\n",
      "round 76 - 33s - train_loss : 0.0095 - train_accuracy : 0.9975 - val_loss : 0.3813 - val_accuracy : 0.9133\n",
      "round 77 - 33s - train_loss : 0.0072 - train_accuracy : 0.9983 - val_loss : 0.3788 - val_accuracy : 0.9084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 78 - 33s - train_loss : 0.0324 - train_accuracy : 0.9960 - val_loss : 0.3744 - val_accuracy : 0.9102\n",
      "round 79 - 33s - train_loss : 0.0165 - train_accuracy : 0.9975 - val_loss : 0.3677 - val_accuracy : 0.9127\n",
      "round 80 - 33s - train_loss : 0.0071 - train_accuracy : 0.9982 - val_loss : 0.3624 - val_accuracy : 0.9128\n",
      "round 81 - 33s - train_loss : 0.0064 - train_accuracy : 0.9987 - val_loss : 0.3597 - val_accuracy : 0.9083\n",
      "round 82 - 33s - train_loss : 0.0056 - train_accuracy : 0.9990 - val_loss : 0.3443 - val_accuracy : 0.9221\n",
      "round 83 - 32s - train_loss : 0.0040 - train_accuracy : 0.9998 - val_loss : 0.3341 - val_accuracy : 0.9284\n",
      "round 84 - 32s - train_loss : 0.0086 - train_accuracy : 0.9977 - val_loss : 0.3321 - val_accuracy : 0.9250\n",
      "round 85 - 33s - train_loss : 0.0070 - train_accuracy : 0.9987 - val_loss : 0.3237 - val_accuracy : 0.9315\n",
      "round 86 - 32s - train_loss : 0.0179 - train_accuracy : 0.9953 - val_loss : 0.3171 - val_accuracy : 0.9338\n",
      "round 87 - 32s - train_loss : 0.0066 - train_accuracy : 0.9987 - val_loss : 0.3118 - val_accuracy : 0.9356\n",
      "round 88 - 32s - train_loss : 0.0050 - train_accuracy : 0.9992 - val_loss : 0.3055 - val_accuracy : 0.9377\n",
      "round 89 - 33s - train_loss : 0.0138 - train_accuracy : 0.9962 - val_loss : 0.2978 - val_accuracy : 0.9420\n",
      "round 90 - 33s - train_loss : 0.0213 - train_accuracy : 0.9952 - val_loss : 0.2934 - val_accuracy : 0.9425\n",
      "round 91 - 33s - train_loss : 0.0062 - train_accuracy : 0.9990 - val_loss : 0.2899 - val_accuracy : 0.9422\n",
      "round 92 - 32s - train_loss : 0.0121 - train_accuracy : 0.9972 - val_loss : 0.2852 - val_accuracy : 0.9418\n",
      "round 93 - 32s - train_loss : 0.0049 - train_accuracy : 0.9997 - val_loss : 0.2780 - val_accuracy : 0.9460\n",
      "round 94 - 32s - train_loss : 0.0209 - train_accuracy : 0.9962 - val_loss : 0.2748 - val_accuracy : 0.9453\n",
      "round 95 - 32s - train_loss : 0.0043 - train_accuracy : 0.9993 - val_loss : 0.2696 - val_accuracy : 0.9450\n",
      "round 96 - 32s - train_loss : 0.0070 - train_accuracy : 0.9985 - val_loss : 0.2678 - val_accuracy : 0.9449\n",
      "round 97 - 32s - train_loss : 0.0046 - train_accuracy : 0.9993 - val_loss : 0.2660 - val_accuracy : 0.9445\n",
      "round 98 - 32s - train_loss : 0.0161 - train_accuracy : 0.9957 - val_loss : 0.2644 - val_accuracy : 0.9444\n",
      "round 99 - 33s - train_loss : 0.0048 - train_accuracy : 0.9990 - val_loss : 0.2595 - val_accuracy : 0.9445\n",
      "round 100 - 33s - train_loss : 0.0050 - train_accuracy : 0.9985 - val_loss : 0.2579 - val_accuracy : 0.9437\n",
      "round 101 - 33s - train_loss : 0.0053 - train_accuracy : 0.9993 - val_loss : 0.2546 - val_accuracy : 0.9448\n",
      "round 102 - 32s - train_loss : 0.0077 - train_accuracy : 0.9983 - val_loss : 0.2528 - val_accuracy : 0.9449\n",
      "round 103 - 31s - train_loss : 0.0050 - train_accuracy : 0.9987 - val_loss : 0.2509 - val_accuracy : 0.9429\n",
      "round 104 - 32s - train_loss : 0.0148 - train_accuracy : 0.9960 - val_loss : 0.2482 - val_accuracy : 0.9442\n",
      "round 105 - 32s - train_loss : 0.0047 - train_accuracy : 0.9993 - val_loss : 0.2438 - val_accuracy : 0.9432\n",
      "round 106 - 32s - train_loss : 0.0028 - train_accuracy : 0.9998 - val_loss : 0.2380 - val_accuracy : 0.9475\n",
      "round 107 - 32s - train_loss : 0.0047 - train_accuracy : 0.9995 - val_loss : 0.2375 - val_accuracy : 0.9468\n",
      "round 108 - 32s - train_loss : 0.0035 - train_accuracy : 0.9998 - val_loss : 0.2350 - val_accuracy : 0.9479\n",
      "round 109 - 32s - train_loss : 0.0028 - train_accuracy : 0.9998 - val_loss : 0.2348 - val_accuracy : 0.9471\n",
      "round 110 - 32s - train_loss : 0.0165 - train_accuracy : 0.9955 - val_loss : 0.2355 - val_accuracy : 0.9461\n",
      "round 111 - 33s - train_loss : 0.0025 - train_accuracy : 0.9998 - val_loss : 0.2346 - val_accuracy : 0.9449\n",
      "round 112 - 33s - train_loss : 0.0069 - train_accuracy : 0.9982 - val_loss : 0.2314 - val_accuracy : 0.9452\n",
      "round 113 - 32s - train_loss : 0.0165 - train_accuracy : 0.9967 - val_loss : 0.2326 - val_accuracy : 0.9420\n",
      "round 114 - 32s - train_loss : 0.0109 - train_accuracy : 0.9972 - val_loss : 0.2254 - val_accuracy : 0.9478\n",
      "round 115 - 32s - train_loss : 0.0028 - train_accuracy : 0.9997 - val_loss : 0.2261 - val_accuracy : 0.9451\n",
      "round 116 - 32s - train_loss : 0.0064 - train_accuracy : 0.9987 - val_loss : 0.2218 - val_accuracy : 0.9466\n",
      "round 117 - 32s - train_loss : 0.0047 - train_accuracy : 0.9995 - val_loss : 0.2198 - val_accuracy : 0.9464\n",
      "round 118 - 32s - train_loss : 0.0036 - train_accuracy : 0.9997 - val_loss : 0.2188 - val_accuracy : 0.9454\n",
      "round 119 - 32s - train_loss : 0.0039 - train_accuracy : 0.9993 - val_loss : 0.2199 - val_accuracy : 0.9435\n",
      "round 120 - 32s - train_loss : 0.0028 - train_accuracy : 0.9997 - val_loss : 0.2181 - val_accuracy : 0.9441\n",
      "round 121 - 32s - train_loss : 0.0402 - train_accuracy : 0.9955 - val_loss : 0.2158 - val_accuracy : 0.9468\n",
      "round 122 - 32s - train_loss : 0.0104 - train_accuracy : 0.9972 - val_loss : 0.2133 - val_accuracy : 0.9487\n",
      "round 123 - 32s - train_loss : 0.0044 - train_accuracy : 0.9992 - val_loss : 0.2094 - val_accuracy : 0.9493\n",
      "round 124 - 32s - train_loss : 0.0084 - train_accuracy : 0.9975 - val_loss : 0.2070 - val_accuracy : 0.9502\n",
      "round 125 - 32s - train_loss : 0.0060 - train_accuracy : 0.9980 - val_loss : 0.2077 - val_accuracy : 0.9494\n",
      "round 126 - 32s - train_loss : 0.0084 - train_accuracy : 0.9977 - val_loss : 0.2049 - val_accuracy : 0.9500\n",
      "round 127 - 32s - train_loss : 0.0017 - train_accuracy : 0.9998 - val_loss : 0.2049 - val_accuracy : 0.9488\n",
      "round 128 - 32s - train_loss : 0.0051 - train_accuracy : 0.9990 - val_loss : 0.2031 - val_accuracy : 0.9504\n",
      "round 129 - 32s - train_loss : 0.0036 - train_accuracy : 0.9992 - val_loss : 0.2003 - val_accuracy : 0.9507\n",
      "round 130 - 32s - train_loss : 0.0057 - train_accuracy : 0.9988 - val_loss : 0.1990 - val_accuracy : 0.9509\n",
      "round 131 - 32s - train_loss : 0.0048 - train_accuracy : 0.9982 - val_loss : 0.2000 - val_accuracy : 0.9502\n",
      "round 132 - 32s - train_loss : 0.0202 - train_accuracy : 0.9948 - val_loss : 0.1980 - val_accuracy : 0.9506\n",
      "round 133 - 32s - train_loss : 0.0077 - train_accuracy : 0.9975 - val_loss : 0.1963 - val_accuracy : 0.9503\n",
      "round 134 - 32s - train_loss : 0.0299 - train_accuracy : 0.9937 - val_loss : 0.1973 - val_accuracy : 0.9498\n",
      "round 135 - 32s - train_loss : 0.0037 - train_accuracy : 0.9998 - val_loss : 0.1956 - val_accuracy : 0.9506\n",
      "round 136 - 33s - train_loss : 0.0046 - train_accuracy : 0.9987 - val_loss : 0.1902 - val_accuracy : 0.9538\n",
      "round 137 - 32s - train_loss : 0.0056 - train_accuracy : 0.9988 - val_loss : 0.1891 - val_accuracy : 0.9532\n",
      "round 138 - 33s - train_loss : 0.0054 - train_accuracy : 0.9985 - val_loss : 0.1891 - val_accuracy : 0.9524\n",
      "round 139 - 33s - train_loss : 0.0015 - train_accuracy : 1.0000 - val_loss : 0.1870 - val_accuracy : 0.9535\n",
      "round 140 - 32s - train_loss : 0.0025 - train_accuracy : 0.9997 - val_loss : 0.1842 - val_accuracy : 0.9535\n",
      "round 141 - 32s - train_loss : 0.0080 - train_accuracy : 0.9985 - val_loss : 0.1846 - val_accuracy : 0.9542\n",
      "round 142 - 33s - train_loss : 0.0036 - train_accuracy : 0.9998 - val_loss : 0.1852 - val_accuracy : 0.9506\n",
      "round 143 - 32s - train_loss : 0.0049 - train_accuracy : 0.9988 - val_loss : 0.1808 - val_accuracy : 0.9535\n",
      "round 144 - 32s - train_loss : 0.0032 - train_accuracy : 0.9998 - val_loss : 0.1791 - val_accuracy : 0.9522\n",
      "round 145 - 32s - train_loss : 0.0135 - train_accuracy : 0.9973 - val_loss : 0.1760 - val_accuracy : 0.9545\n",
      "round 146 - 33s - train_loss : 0.0339 - train_accuracy : 0.9950 - val_loss : 0.1773 - val_accuracy : 0.9557\n",
      "round 147 - 33s - train_loss : 0.0030 - train_accuracy : 0.9992 - val_loss : 0.1764 - val_accuracy : 0.9565\n",
      "round 148 - 33s - train_loss : 0.0086 - train_accuracy : 0.9983 - val_loss : 0.1760 - val_accuracy : 0.9560\n",
      "round 149 - 33s - train_loss : 0.0022 - train_accuracy : 0.9998 - val_loss : 0.1745 - val_accuracy : 0.9560\n",
      "round 150 - 33s - train_loss : 0.0278 - train_accuracy : 0.9967 - val_loss : 0.1741 - val_accuracy : 0.9566\n",
      "round 151 - 33s - train_loss : 0.0053 - train_accuracy : 0.9990 - val_loss : 0.1732 - val_accuracy : 0.9566\n",
      "round 152 - 33s - train_loss : 0.0038 - train_accuracy : 0.9995 - val_loss : 0.1720 - val_accuracy : 0.9566\n",
      "round 153 - 33s - train_loss : 0.0051 - train_accuracy : 0.9992 - val_loss : 0.1715 - val_accuracy : 0.9561\n",
      "round 154 - 32s - train_loss : 0.0090 - train_accuracy : 0.9980 - val_loss : 0.1719 - val_accuracy : 0.9559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 155 - 31s - train_loss : 0.0108 - train_accuracy : 0.9967 - val_loss : 0.1708 - val_accuracy : 0.9565\n",
      "round 156 - 31s - train_loss : 0.0080 - train_accuracy : 0.9988 - val_loss : 0.1708 - val_accuracy : 0.9550\n",
      "round 157 - 33s - train_loss : 0.0035 - train_accuracy : 0.9995 - val_loss : 0.1696 - val_accuracy : 0.9555\n",
      "round 158 - 32s - train_loss : 0.0048 - train_accuracy : 0.9988 - val_loss : 0.1677 - val_accuracy : 0.9552\n",
      "round 159 - 33s - train_loss : 0.0029 - train_accuracy : 0.9993 - val_loss : 0.1665 - val_accuracy : 0.9548\n",
      "round 160 - 33s - train_loss : 0.0175 - train_accuracy : 0.9957 - val_loss : 0.1683 - val_accuracy : 0.9554\n",
      "round 161 - 32s - train_loss : 0.0034 - train_accuracy : 0.9995 - val_loss : 0.1674 - val_accuracy : 0.9555\n",
      "round 162 - 32s - train_loss : 0.0262 - train_accuracy : 0.9962 - val_loss : 0.1690 - val_accuracy : 0.9544\n",
      "round 163 - 32s - train_loss : 0.0032 - train_accuracy : 0.9993 - val_loss : 0.1679 - val_accuracy : 0.9540\n",
      "round 164 - 32s - train_loss : 0.0065 - train_accuracy : 0.9980 - val_loss : 0.1652 - val_accuracy : 0.9547\n",
      "round 165 - 32s - train_loss : 0.0088 - train_accuracy : 0.9978 - val_loss : 0.1653 - val_accuracy : 0.9552\n",
      "round 166 - 33s - train_loss : 0.0030 - train_accuracy : 0.9997 - val_loss : 0.1649 - val_accuracy : 0.9552\n",
      "round 167 - 33s - train_loss : 0.0054 - train_accuracy : 0.9987 - val_loss : 0.1659 - val_accuracy : 0.9552\n",
      "round 168 - 33s - train_loss : 0.0111 - train_accuracy : 0.9982 - val_loss : 0.1624 - val_accuracy : 0.9563\n",
      "round 169 - 33s - train_loss : 0.0063 - train_accuracy : 0.9992 - val_loss : 0.1620 - val_accuracy : 0.9570\n",
      "round 170 - 33s - train_loss : 0.0034 - train_accuracy : 0.9992 - val_loss : 0.1637 - val_accuracy : 0.9556\n",
      "round 171 - 33s - train_loss : 0.0134 - train_accuracy : 0.9973 - val_loss : 0.1611 - val_accuracy : 0.9561\n",
      "round 172 - 32s - train_loss : 0.0163 - train_accuracy : 0.9968 - val_loss : 0.1595 - val_accuracy : 0.9564\n",
      "round 173 - 32s - train_loss : 0.0063 - train_accuracy : 0.9988 - val_loss : 0.1597 - val_accuracy : 0.9551\n",
      "round 174 - 32s - train_loss : 0.0050 - train_accuracy : 0.9985 - val_loss : 0.1591 - val_accuracy : 0.9558\n",
      "round 175 - 32s - train_loss : 0.0273 - train_accuracy : 0.9948 - val_loss : 0.1580 - val_accuracy : 0.9570\n",
      "round 176 - 32s - train_loss : 0.0054 - train_accuracy : 0.9983 - val_loss : 0.1572 - val_accuracy : 0.9575\n",
      "round 177 - 32s - train_loss : 0.0050 - train_accuracy : 0.9987 - val_loss : 0.1576 - val_accuracy : 0.9572\n",
      "round 178 - 33s - train_loss : 0.0170 - train_accuracy : 0.9972 - val_loss : 0.1543 - val_accuracy : 0.9578\n",
      "round 179 - 33s - train_loss : 0.0083 - train_accuracy : 0.9982 - val_loss : 0.1550 - val_accuracy : 0.9575\n",
      "round 180 - 33s - train_loss : 0.0040 - train_accuracy : 0.9993 - val_loss : 0.1545 - val_accuracy : 0.9578\n",
      "round 181 - 33s - train_loss : 0.0039 - train_accuracy : 0.9990 - val_loss : 0.1532 - val_accuracy : 0.9583\n",
      "round 182 - 32s - train_loss : 0.0058 - train_accuracy : 0.9987 - val_loss : 0.1552 - val_accuracy : 0.9577\n",
      "round 183 - 32s - train_loss : 0.0086 - train_accuracy : 0.9977 - val_loss : 0.1526 - val_accuracy : 0.9582\n",
      "round 184 - 33s - train_loss : 0.0056 - train_accuracy : 0.9985 - val_loss : 0.1524 - val_accuracy : 0.9588\n",
      "round 185 - 32s - train_loss : 0.0090 - train_accuracy : 0.9978 - val_loss : 0.1527 - val_accuracy : 0.9589\n",
      "round 186 - 33s - train_loss : 0.0097 - train_accuracy : 0.9970 - val_loss : 0.1496 - val_accuracy : 0.9590\n",
      "round 187 - 33s - train_loss : 0.0042 - train_accuracy : 0.9992 - val_loss : 0.1485 - val_accuracy : 0.9590\n",
      "round 188 - 33s - train_loss : 0.0121 - train_accuracy : 0.9968 - val_loss : 0.1481 - val_accuracy : 0.9594\n",
      "round 189 - 32s - train_loss : 0.0058 - train_accuracy : 0.9990 - val_loss : 0.1455 - val_accuracy : 0.9608\n",
      "round 190 - 33s - train_loss : 0.0047 - train_accuracy : 0.9987 - val_loss : 0.1461 - val_accuracy : 0.9601\n",
      "round 191 - 32s - train_loss : 0.0057 - train_accuracy : 0.9988 - val_loss : 0.1464 - val_accuracy : 0.9598\n",
      "round 192 - 33s - train_loss : 0.0098 - train_accuracy : 0.9967 - val_loss : 0.1466 - val_accuracy : 0.9603\n",
      "round 193 - 33s - train_loss : 0.0025 - train_accuracy : 0.9998 - val_loss : 0.1431 - val_accuracy : 0.9620\n",
      "round 194 - 32s - train_loss : 0.0048 - train_accuracy : 0.9992 - val_loss : 0.1424 - val_accuracy : 0.9622\n",
      "round 195 - 33s - train_loss : 0.0050 - train_accuracy : 0.9982 - val_loss : 0.1415 - val_accuracy : 0.9620\n",
      "round 196 - 33s - train_loss : 0.0040 - train_accuracy : 0.9992 - val_loss : 0.1402 - val_accuracy : 0.9623\n",
      "round 197 - 33s - train_loss : 0.0037 - train_accuracy : 0.9995 - val_loss : 0.1394 - val_accuracy : 0.9622\n",
      "round 198 - 33s - train_loss : 0.0034 - train_accuracy : 0.9998 - val_loss : 0.1397 - val_accuracy : 0.9623\n",
      "round 199 - 33s - train_loss : 0.0066 - train_accuracy : 0.9985 - val_loss : 0.1397 - val_accuracy : 0.9620\n",
      "round 200 - 33s - train_loss : 0.0031 - train_accuracy : 0.9995 - val_loss : 0.1402 - val_accuracy : 0.9609\n",
      "round 201 - 33s - train_loss : 0.0023 - train_accuracy : 0.9997 - val_loss : 0.1375 - val_accuracy : 0.9627\n",
      "round 202 - 32s - train_loss : 0.0044 - train_accuracy : 0.9992 - val_loss : 0.1359 - val_accuracy : 0.9625\n",
      "round 203 - 32s - train_loss : 0.0036 - train_accuracy : 0.9992 - val_loss : 0.1358 - val_accuracy : 0.9634\n",
      "round 204 - 32s - train_loss : 0.0120 - train_accuracy : 0.9967 - val_loss : 0.1371 - val_accuracy : 0.9625\n",
      "round 205 - 32s - train_loss : 0.0031 - train_accuracy : 0.9993 - val_loss : 0.1371 - val_accuracy : 0.9628\n",
      "round 206 - 32s - train_loss : 0.0026 - train_accuracy : 0.9997 - val_loss : 0.1367 - val_accuracy : 0.9627\n",
      "round 207 - 32s - train_loss : 0.0059 - train_accuracy : 0.9993 - val_loss : 0.1366 - val_accuracy : 0.9627\n",
      "round 208 - 32s - train_loss : 0.0038 - train_accuracy : 0.9990 - val_loss : 0.1366 - val_accuracy : 0.9617\n",
      "round 209 - 32s - train_loss : 0.0102 - train_accuracy : 0.9977 - val_loss : 0.1368 - val_accuracy : 0.9622\n",
      "round 210 - 32s - train_loss : 0.0226 - train_accuracy : 0.9957 - val_loss : 0.1371 - val_accuracy : 0.9621\n",
      "round 211 - 32s - train_loss : 0.0035 - train_accuracy : 0.9995 - val_loss : 0.1359 - val_accuracy : 0.9618\n",
      "round 212 - 32s - train_loss : 0.0031 - train_accuracy : 0.9993 - val_loss : 0.1354 - val_accuracy : 0.9615\n",
      "round 213 - 32s - train_loss : 0.0069 - train_accuracy : 0.9985 - val_loss : 0.1342 - val_accuracy : 0.9621\n",
      "round 214 - 33s - train_loss : 0.0015 - train_accuracy : 0.9998 - val_loss : 0.1333 - val_accuracy : 0.9621\n",
      "round 215 - 33s - train_loss : 0.0040 - train_accuracy : 0.9993 - val_loss : 0.1327 - val_accuracy : 0.9623\n",
      "round 216 - 33s - train_loss : 0.0022 - train_accuracy : 0.9998 - val_loss : 0.1309 - val_accuracy : 0.9626\n",
      "round 217 - 33s - train_loss : 0.0028 - train_accuracy : 0.9995 - val_loss : 0.1297 - val_accuracy : 0.9631\n",
      "round 218 - 33s - train_loss : 0.0102 - train_accuracy : 0.9977 - val_loss : 0.1299 - val_accuracy : 0.9636\n",
      "round 219 - 33s - train_loss : 0.0026 - train_accuracy : 0.9998 - val_loss : 0.1298 - val_accuracy : 0.9638\n",
      "round 220 - 33s - train_loss : 0.0021 - train_accuracy : 0.9997 - val_loss : 0.1281 - val_accuracy : 0.9637\n",
      "round 221 - 33s - train_loss : 0.0021 - train_accuracy : 1.0000 - val_loss : 0.1265 - val_accuracy : 0.9643\n",
      "round 222 - 33s - train_loss : 0.0106 - train_accuracy : 0.9977 - val_loss : 0.1265 - val_accuracy : 0.9634\n",
      "round 223 - 32s - train_loss : 0.0026 - train_accuracy : 0.9997 - val_loss : 0.1261 - val_accuracy : 0.9637\n",
      "round 224 - 32s - train_loss : 0.0028 - train_accuracy : 0.9995 - val_loss : 0.1265 - val_accuracy : 0.9633\n",
      "round 225 - 32s - train_loss : 0.0028 - train_accuracy : 0.9993 - val_loss : 0.1252 - val_accuracy : 0.9641\n",
      "round 226 - 32s - train_loss : 0.0017 - train_accuracy : 1.0000 - val_loss : 0.1245 - val_accuracy : 0.9635\n",
      "round 227 - 33s - train_loss : 0.0099 - train_accuracy : 0.9968 - val_loss : 0.1250 - val_accuracy : 0.9641\n",
      "round 228 - 33s - train_loss : 0.0042 - train_accuracy : 0.9993 - val_loss : 0.1246 - val_accuracy : 0.9640\n",
      "round 229 - 33s - train_loss : 0.0097 - train_accuracy : 0.9978 - val_loss : 0.1236 - val_accuracy : 0.9647\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-efb9629f7587>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m                    \u001b[0mclient_optimizer_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclient_optimizer_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                    **fed_params)\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mfederater\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_rounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-9bcf3d8022c5>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, num_rounds, criterion, num_epochs, val_dl, straggler_rate, eval_every_n)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;31m# update a subset of clients with the local solver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0mS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_random_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             train_metrics = self.update(client_ids=S, \n\u001b[0m\u001b[1;32m    146\u001b[0m                                         \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                                         \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-9bcf3d8022c5>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, client_ids, criterion, num_epochs, straggler_rate)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;31m# update the client weights and record the local training metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             client_metrics_dict = client.update(\n\u001b[0m\u001b[1;32m    109\u001b[0m                 \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclient_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-452946d5bec8>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, criterion, num_epochs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0melapsed_times\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-0e1e26f39e41>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \"\"\"\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fedprox baseline\n",
    "method = 'fedprox'\n",
    "dataset = 'mnist'\n",
    "seed = 42069\n",
    "device = 'cuda:0'\n",
    "num_workers = 0\n",
    "\n",
    "experiment_name = create_experiment_name(method, dataset, config[method][dataset])\n",
    "experiment_name = 'tmp_prox'\n",
    "print(f'Experiment : {experiment_name}')\n",
    "\n",
    "writer = SummaryWriter(os.path.join('logs', experiment_name))\n",
    "\n",
    "set_state(seed)\n",
    "client_params = config[method][dataset]['clients']\n",
    "clients = get_clients(\n",
    "    train_ds, \n",
    "    num_workers=num_workers,\n",
    "    seed=seed,\n",
    "    device=device,\n",
    "    **client_params,\n",
    ")\n",
    "\n",
    "model = CNN()\n",
    "client_optimizer_cls = getattr(torch.optim, config[method][dataset]['client_optimizer'])\n",
    "client_optimizer_params = config[method][dataset]['client_optimizer_params']\n",
    "server_optimizer = getattr(torch.optim, config[method][dataset]['server_optimizer'])\n",
    "server_optimizer = server_optimizer(model.parameters(), **config[method][dataset]['server_optimizer_params'])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "fed_params = config[method][dataset]['federater']\n",
    "fed_params['seed'] = seed\n",
    "num_rounds = config[method][dataset]['fit']['num_rounds']\n",
    "num_epochs = config[method][dataset]['fit']['num_epochs']\n",
    "\n",
    "federater = FedProx(model,\n",
    "                   clients=clients,\n",
    "                    server_optimizer=server_optimizer,\n",
    "                   client_optimizer_cls=client_optimizer_cls,\n",
    "                   client_optimizer_params=client_optimizer_params,\n",
    "                   **fed_params)\n",
    "federater.fit(num_rounds=num_rounds, criterion=criterion, num_epochs=num_epochs, val_dl=test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6cfbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fed avg baseline\n",
    "method = 'fedavg'\n",
    "dataset = 'mnist'\n",
    "seed = 42069\n",
    "device = 'cuda:0'\n",
    "num_workers = 0\n",
    "\n",
    "experiment_name = create_experiment_name(method, dataset, config[method][dataset])\n",
    "experiment_name = 'tmp'\n",
    "print(f'Experiment : {experiment_name}')\n",
    "\n",
    "writer = SummaryWriter(os.path.join('logs', experiment_name))\n",
    "\n",
    "set_state(seed)\n",
    "client_params = config[method][dataset]['clients']\n",
    "clients = get_clients(\n",
    "    train_ds, \n",
    "    num_workers=num_workers,\n",
    "    seed=seed,\n",
    "    device=device,\n",
    "    **client_params,\n",
    ")\n",
    "\n",
    "model = CNN()\n",
    "# client optimizer\n",
    "# client_optimizer_cls = torch.optim.SGD\n",
    "# client_optimizer_params = {\n",
    "#     'lr': 0.1,\n",
    "# }\n",
    "client_optimizer_cls = getattr(torch.optim, config[method][dataset]['client_optimizer'])\n",
    "client_optimizer_params = config[method][dataset]['client_optimizer_params']\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "fed_params = config[method][dataset]['federater']\n",
    "fed_params['seed'] = seed\n",
    "# fed_params = {\n",
    "#     'seed': seed,\n",
    "#     'C': 0.1,\n",
    "# }\n",
    "num_rounds = 1#config[method][dataset]['fit']['num_rounds']\n",
    "num_epochs = 1#config[method][dataset]['fit']['num_epochs']\n",
    "\n",
    "federater = FedAvg(model,\n",
    "                    clients=clients,\n",
    "                    client_optimizer_cls=client_optimizer_cls,\n",
    "                    client_optimizer_params=client_optimizer_params,\n",
    "                    **fed_params)\n",
    "federater.fit(num_rounds=num_rounds, criterion=criterion, num_epochs=num_epochs, val_dl=test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "56ba968a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "495dd562",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 100/100 [00:00<00:00, 73403.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57 46 35 28 40 41 44 40 40 47 35 40 39 36 33 50 40 42 41 41 45 54 51 45\n",
      " 41 50 35 32 49 43 44 42 39 55 46 46 53 38 41 50 28 39 46 46 46 45 36]\n",
      "[57 46 35 28 40 41 44 40 40 47 35 40 39 36 33 50 40 42 41 41 45 54 51 45\n",
      " 41 50 35 32 49 43 44 42 39 55 46 46 53 38 41 50 28 39 46 46 46 45 36]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import emnist\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "import argparse\n",
    "from os.path import dirname\n",
    "\n",
    "similarity = 1\n",
    "num_of_users = 100\n",
    "samples_num = 20\n",
    "dataset = 'balanced'\n",
    "images, train_labels = emnist.extract_training_samples(dataset)  # TODO: add test samples\n",
    "images = np.reshape(images, (images.shape[0], -1))\n",
    "images = images.astype(np.float32)\n",
    "train_labels = train_labels.astype(np.int)\n",
    "num_of_labels = len(set(train_labels))\n",
    "\n",
    "emnist_data = []\n",
    "for i in range(min(train_labels), num_of_labels + min(train_labels)):\n",
    "    idx = train_labels == i\n",
    "    emnist_data.append(images[idx])\n",
    "\n",
    "iid_samples = int(similarity * samples_num)\n",
    "X = [[] for _ in range(num_of_users)]\n",
    "y = [[] for _ in range(num_of_users)]\n",
    "idx = np.zeros(num_of_labels, dtype=np.int64)\n",
    "\n",
    "# create %similarity of iid data\n",
    "for user in range(num_of_users):\n",
    "    labels = np.random.randint(0, num_of_labels, iid_samples)\n",
    "    for label in labels:\n",
    "        X[user].append(emnist_data[label][idx[label]].tolist())\n",
    "        y[user] += (label * np.ones(1)).tolist()\n",
    "        idx[label] += 1\n",
    "\n",
    "print(idx)\n",
    "\n",
    "# fill remaining data\n",
    "for user in range(num_of_users):\n",
    "    label = user % num_of_labels\n",
    "    X[user] += emnist_data[label][idx[label]:idx[label] + samples_num - iid_samples].tolist()\n",
    "    y[user] += (label * np.ones(samples_num - iid_samples)).tolist()\n",
    "    idx[label] += samples_num - iid_samples\n",
    "\n",
    "print(idx)\n",
    "\n",
    "train_data = {'users': [], 'user_data': {}, 'num_samples': []}\n",
    "test_data = {'users': [], 'user_data': {}, 'num_samples': []}\n",
    "\n",
    "for i in trange(num_of_users, ncols=120):\n",
    "    uname = 'f_{0:05d}'.format(i)\n",
    "\n",
    "    combined = list(zip(X[i], y[i]))\n",
    "    random.shuffle(combined)\n",
    "    X[i][:], y[i][:] = zip(*combined)\n",
    "    num_samples = len(X[i])\n",
    "    train_len = int(0.9 * num_samples)\n",
    "    test_len = num_samples - train_len\n",
    "\n",
    "    train_data['users'].append(uname)\n",
    "    train_data['user_data'][uname] = {'x': X[i][:train_len], 'y': y[i][:train_len]}\n",
    "    train_data['num_samples'].append(train_len)\n",
    "    test_data['users'].append(uname)\n",
    "    test_data['user_data'][uname] = {'x': X[i][train_len:], 'y': y[i][train_len:]}\n",
    "    test_data['num_samples'].append(test_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d857bb04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "4e37c9c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47,)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_labels).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "93d7f8e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data['user_data']['f_00000']['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "cf6bd544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.0    2\n",
       "29.0    2\n",
       "5.0     1\n",
       "28.0    1\n",
       "42.0    1\n",
       "24.0    1\n",
       "31.0    1\n",
       "12.0    1\n",
       "10.0    1\n",
       "38.0    1\n",
       "37.0    1\n",
       "22.0    1\n",
       "6.0     1\n",
       "33.0    1\n",
       "2.0     1\n",
       "34.0    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.Series(train_data['user_data']['f_00000']['y']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "ce61df48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0     9\n",
       "32.0    2\n",
       "27.0    1\n",
       "31.0    1\n",
       "41.0    1\n",
       "15.0    1\n",
       "37.0    1\n",
       "14.0    1\n",
       "8.0     1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.Series(train_data['user_data']['f_00000']['y']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "bb4defba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 41.0,\n",
       " 14.0,\n",
       " 27.0,\n",
       " 32.0,\n",
       " 0.0,\n",
       " 37.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 8.0,\n",
       " 0.0,\n",
       " 32.0,\n",
       " 31.0,\n",
       " 0.0,\n",
       " 15.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['user_data']['f_00000']['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7701df02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40f9271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fedprox baseline\n",
    "method = 'fedopt'\n",
    "dataset = 'mnist'\n",
    "seed = 42069\n",
    "device = 'cuda:0'\n",
    "num_workers = 0\n",
    "\n",
    "experiment_name = create_experiment_name(method, dataset, config[method][dataset])\n",
    "experiment_name = 'tmp'\n",
    "print(f'Experiment : {experiment_name}')\n",
    "\n",
    "writer = SummaryWriter(os.path.join('logs', experiment_name))\n",
    "\n",
    "set_state(seed)\n",
    "client_params = config[method][dataset]['clients']\n",
    "clients = get_clients(\n",
    "    train_ds, \n",
    "    num_workers=num_workers,\n",
    "    seed=seed,\n",
    "    device=device,\n",
    "    **client_params,\n",
    ")\n",
    "\n",
    "model = CNN()\n",
    "client_optimizer_cls = getattr(torch.optim, config[method][dataset]['client_optimizer'])\n",
    "client_optimizer_params = config[method][dataset]['client_optimizer_params']\n",
    "server_optimizer = getattr(torch.optim, config[method][dataset]['server_optimizer'])\n",
    "server_optimizer = server_optimizer(model.parameters(), **config[method][dataset]['server_optimizer_params'])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "fed_params = config[method][dataset]['federater']\n",
    "fed_params['seed'] = seed\n",
    "num_rounds = config[method][dataset]['fit']['num_rounds']\n",
    "num_epochs = config[method][dataset]['fit']['num_epochs']\n",
    "\n",
    "federater = FedProx(model,\n",
    "                   clients=clients,\n",
    "                    server_optimizer=server_optimizer,\n",
    "                   client_optimizer_cls=client_optimizer_cls,\n",
    "                   client_optimizer_params=client_optimizer_params,\n",
    "                   **fed_params)\n",
    "federater.fit(num_rounds=num_rounds, criterion=criterion, num_epochs=num_epochs, val_dl=test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0b3c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f975d50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa6be157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-4f7cd57028e0eb16\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-4f7cd57028e0eb16\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "%tensorboard --logdir logs/fedavg_avg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b4c85f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['events.out.tfevents.1624399407.64acb651e23a.401.20',\n",
       " 'events.out.tfevents.1624399370.64acb651e23a.401.19',\n",
       " 'events.out.tfevents.1624399485.64acb651e23a.401.27',\n",
       " 'events.out.tfevents.1624399462.64acb651e23a.401.25',\n",
       " 'events.out.tfevents.1624399450.64acb651e23a.401.23',\n",
       " 'events.out.tfevents.1624399444.64acb651e23a.401.22',\n",
       " 'events.out.tfevents.1624399432.64acb651e23a.401.21']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('logs/fedavg_baseline/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485b6c2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "30e0d152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12 56 43 34 32 55 21  2 41 77]\n",
      "round 1 - train_loss : 0.6275 - train_accuracy : 0.8185 - val_loss : 2.2982 - val_accuracy : 0.1241\n",
      "[ 1 10 86 59 64 47  3 27 80 72]\n",
      "round 2 - train_loss : 0.7072 - train_accuracy : 0.7943 - val_loss : 2.2948 - val_accuracy : 0.1811\n",
      "[91 80  2 34 63 97 12 14 49 89]\n",
      "round 3 - train_loss : 0.6144 - train_accuracy : 0.8287 - val_loss : 2.2911 - val_accuracy : 0.2160\n",
      "[ 5 14 29 47 48  3 74 62 64 66]\n",
      "round 4 - train_loss : 0.6083 - train_accuracy : 0.8285 - val_loss : 2.2900 - val_accuracy : 0.2767\n",
      "[73 94 87 63  3 44 23 88 59 84]\n",
      "round 5 - train_loss : 0.5569 - train_accuracy : 0.8543 - val_loss : 2.2860 - val_accuracy : 0.3574\n"
     ]
    }
   ],
   "source": [
    "# Fed avg baselien\n",
    "seed = 42069\n",
    "device = 'cpu'\n",
    "set_state(seed)\n",
    "model_avg = CNN()\n",
    "clients_avg = get_clients(\n",
    "    train_ds, \n",
    "    num_clients=100,\n",
    "    is_iid=False,\n",
    "    shard_size=300,\n",
    "    batch_size=10,\n",
    "    num_workers=0,\n",
    "    seed=seed,\n",
    "    device=device\n",
    ")\n",
    "# server optimizer\n",
    "server_optimizer_params = {\n",
    "    'lr': 1,\n",
    "    'momentum': 0\n",
    "}\n",
    "server_optimizer = torch.optim.SGD(model_avg.parameters(), **server_optimizer_params)\n",
    "\n",
    "# client optimizer\n",
    "client_optimizer_cls = torch.optim.SGD\n",
    "client_optimizer_params = {\n",
    "    'lr': 0.1,\n",
    "}\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "fed_params = {\n",
    "    'seed': seed,\n",
    "    'C': 0.1,\n",
    "}\n",
    "\n",
    "federater_avg = FedAvg(model_avg,\n",
    "                    clients=clients_avg,\n",
    "                    client_optimizer_cls=client_optimizer_cls,\n",
    "                    client_optimizer_params=client_optimizer_params,\n",
    "                    **fed_params)\n",
    "federater_avg.fit(num_rounds=5, criterion=criterion, num_epochs=1, val_dl=test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2ab2e069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12 56 43 34 32 55 21  2 41 77]\n",
      "round 1 - train_loss : 0.6275 - train_accuracy : 0.8185 - val_loss : 2.2982 - val_accuracy : 0.1241\n",
      "[ 1 10 86 59 64 47  3 27 80 72]\n",
      "round 2 - train_loss : 0.7070 - train_accuracy : 0.7947 - val_loss : 2.2948 - val_accuracy : 0.1812\n",
      "[91 80  2 34 63 97 12 14 49 89]\n",
      "round 3 - train_loss : 0.6384 - train_accuracy : 0.8200 - val_loss : 2.2918 - val_accuracy : 0.2129\n",
      "[ 5 14 29 47 48  3 74 62 64 66]\n",
      "round 4 - train_loss : 0.6047 - train_accuracy : 0.8390 - val_loss : 2.2907 - val_accuracy : 0.2724\n",
      "[73 94 87 63  3 44 23 88 59 84]\n",
      "round 5 - train_loss : 0.5962 - train_accuracy : 0.8512 - val_loss : 2.2884 - val_accuracy : 0.3317\n"
     ]
    }
   ],
   "source": [
    "# Fed avg baselien\n",
    "seed = 42069\n",
    "device = 'cpu'\n",
    "num_workers = 0\n",
    "experiment_name = 'fedavg_baseline'\n",
    "set_state(seed)\n",
    "writer = SummaryWriter(os.path.join('logs', experiment_name))\n",
    "\n",
    "data_params = config.fed_avg['mnist']['data']\n",
    "model_prox = CNN()\n",
    "clients_prox = get_clients(\n",
    "    train_ds, \n",
    "    num_workers=num_workers,\n",
    "    seed=seed,\n",
    "    device=device\n",
    "    **data_params,\n",
    ")\n",
    "# server optimizer\n",
    "server_optimizer_params = {\n",
    "    'lr': 1,\n",
    "    'momentum': 0\n",
    "}\n",
    "server_optimizer = torch.optim.SGD(model_prox.parameters(), **server_optimizer_params)\n",
    "\n",
    "# client optimizer\n",
    "client_optimizer_cls = torch.optim.SGD\n",
    "client_optimizer_params = {\n",
    "    'lr': 0.1,\n",
    "}\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "fed_params = {\n",
    "    'seed': seed,\n",
    "    'mu': 0,\n",
    "    'C': 0.1,\n",
    "}\n",
    "\n",
    "federater_prox = FedProx(model_prox,\n",
    "                    clients=clients_prox,\n",
    "                    client_optimizer_cls=client_optimizer_cls,\n",
    "                    client_optimizer_params=client_optimizer_params,\n",
    "                    server_optimizer=server_optimizer,\n",
    "                    **fed_params)\n",
    "federater_prox.fit(num_rounds=100, criterion=criterion, num_epochs=1, val_dl=test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "076baa3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1237,  0.0496, -0.0533, -0.0223,  0.0750],\n",
       "         [ 0.1293,  0.0621, -0.1308,  0.1752, -0.0304],\n",
       "         [ 0.1696,  0.1714, -0.1323, -0.0063,  0.0882],\n",
       "         [-0.0299,  0.1077,  0.0123,  0.1446,  0.1252],\n",
       "         [ 0.1886,  0.1777, -0.1097, -0.0612, -0.1472]]],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model_avg.parameters())[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "011049ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1241,  0.0502, -0.0526, -0.0217,  0.0757],\n",
       "         [ 0.1296,  0.0627, -0.1302,  0.1758, -0.0299],\n",
       "         [ 0.1698,  0.1716, -0.1321, -0.0061,  0.0883],\n",
       "         [-0.0298,  0.1078,  0.0124,  0.1448,  0.1253],\n",
       "         [ 0.1890,  0.1780, -0.1095, -0.0609, -0.1471]]],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model_avg.parameters())[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4accb726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1237,  0.0496, -0.0533, -0.0223,  0.0750],\n",
       "         [ 0.1293,  0.0621, -0.1308,  0.1752, -0.0304],\n",
       "         [ 0.1696,  0.1714, -0.1323, -0.0063,  0.0882],\n",
       "         [-0.0299,  0.1077,  0.0123,  0.1446,  0.1252],\n",
       "         [ 0.1886,  0.1777, -0.1097, -0.0612, -0.1472]]],\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_avg = torch.stack([federater_avg.client_weights[i] * list(clients_avg[i].model.parameters())[0][0] for i in range(100)]).sum(0)\n",
    "new_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7e56b6e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0003355368971824646"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(list(model_avg.parameters())[0][0] - new_avg)[0][0][0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4f3699ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = []\n",
    "for i in range(100):\n",
    "    g.append(list(model_avg.parameters())[0][0] - list(clients_avg[i].model.parameters())[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cbddf2e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00033553242683410646"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g[12][0][0][0].item() * federater_avg.client_weights[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2e9f91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab8e068",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "id": "fefc13d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0.]]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 764,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model_avg.parameters())[0][0] - list(clients_avg[0].model.parameters())[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "22ac6340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-6.2098e-04, -8.3167e-04, -8.5653e-04, -1.0109e-04, -1.4994e-05],\n",
       "         [-1.3470e-03, -1.2671e-03, -7.1046e-04,  1.7881e-05, -3.6251e-04],\n",
       "         [-1.1452e-03, -1.8276e-03, -1.0516e-03, -6.7298e-04, -6.9550e-04],\n",
       "         [-1.1875e-03, -1.6784e-03, -7.8158e-04, -3.3040e-04, -7.4774e-05],\n",
       "         [-1.0530e-03, -9.4239e-04, -5.7211e-04, -4.1787e-04,  1.0461e-04]]],\n",
       "       grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model_avg.parameters())[0][0] - list(model_prox.parameters())[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fe824912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12 56 43 34 32 55 21  2 41 77]\n",
      "round 1 - train_loss : 0.7237 - train_accuracy : 0.7873 - val_loss : 2.3106 - val_accuracy : 0.1532\n",
      "[ 1 10 86 59 64 47  3 27 80 72]\n",
      "round 2 - train_loss : 0.8594 - train_accuracy : 0.7323 - val_loss : 2.3106 - val_accuracy : 0.1536\n",
      "[91 80  2 34 63 97 12 14 49 89]\n",
      "round 3 - train_loss : 0.7307 - train_accuracy : 0.7828 - val_loss : 2.3104 - val_accuracy : 0.1576\n",
      "[ 5 14 29 47 48  3 74 62 64 66]\n",
      "round 4 - train_loss : 0.7079 - train_accuracy : 0.8025 - val_loss : 2.3098 - val_accuracy : 0.1640\n",
      "[73 94 87 63  3 44 23 88 59 84]\n",
      "round 5 - train_loss : 0.8606 - train_accuracy : 0.7702 - val_loss : 2.3088 - val_accuracy : 0.1728\n"
     ]
    }
   ],
   "source": [
    "# Fed avg baselien\n",
    "seed = 42069\n",
    "device = 'cpu'\n",
    "set_state(seed)\n",
    "model_nova = CNN()\n",
    "clients_nova = get_clients(\n",
    "    train_ds, \n",
    "    num_clients=100,\n",
    "    is_iid=False,\n",
    "    shard_size=300,\n",
    "    batch_size=10,\n",
    "    num_workers=0,\n",
    "    seed=seed,\n",
    "    device=device\n",
    ")\n",
    "# server optimizer\n",
    "server_optimizer_params = {\n",
    "    'lr': 1,\n",
    "    'momentum': 0.9\n",
    "}\n",
    "server_optimizer = torch.optim.SGD(model_nova.parameters(), **server_optimizer_params)\n",
    "\n",
    "# client optimizer\n",
    "client_optimizer_cls = torch.optim.SGD\n",
    "client_optimizer_params = {\n",
    "    'lr': 0.1,\n",
    "}\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "fed_params = {\n",
    "    'seed': seed,\n",
    "    'C': 0.1,\n",
    "    'mu': 0.1,\n",
    "}\n",
    "\n",
    "federater_nova = FedNova(model_nova,\n",
    "                    clients=clients_nova,\n",
    "                    client_optimizer_cls=client_optimizer_cls,\n",
    "                    client_optimizer_params=client_optimizer_params,\n",
    "                         server_optimizer=server_optimizer,\n",
    "                    **fed_params)\n",
    "federater_nova.fit(num_rounds=5, criterion=criterion, num_epochs=1, val_dl=test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "b9387308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1 - train_loss : 0.6275 - train_accuracy : 0.8185 - val_loss : 2.2982 - val_accuracy : 0.1241\n",
      "round 2 - train_loss : 0.7086 - train_accuracy : 0.7940 - val_loss : 2.2951 - val_accuracy : 0.2126\n",
      "round 3 - train_loss : 0.6189 - train_accuracy : 0.8248 - val_loss : 2.2946 - val_accuracy : 0.2199\n",
      "round 4 - train_loss : 0.4919 - train_accuracy : 0.8758 - val_loss : 2.2911 - val_accuracy : 0.2282\n",
      "round 5 - train_loss : 0.5117 - train_accuracy : 0.8835 - val_loss : 2.2805 - val_accuracy : 0.2499\n"
     ]
    }
   ],
   "source": [
    "seed = 42069\n",
    "device = 'cpu'\n",
    "set_state(random_state)\n",
    "model = CNN()\n",
    "clients = get_clients(\n",
    "    train_ds, \n",
    "    num_clients=100,\n",
    "    is_iid=False,\n",
    "    shard_size=300,\n",
    "    batch_size=10,\n",
    "    num_workers=0,\n",
    "    seed=seed,\n",
    "    device=device\n",
    ")\n",
    "# server optimizer\n",
    "server_optimizer_params = {\n",
    "    'lr': 1,\n",
    "    'momentum': 0\n",
    "}\n",
    "server_optimizer = torch.optim.SGD(model.parameters(), **server_optimizer_params)\n",
    "\n",
    "# client optimizer\n",
    "client_optimizer_cls = torch.optim.SGD\n",
    "client_optimizer_params = {\n",
    "    'lr': 0.1,\n",
    "}\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "fed_params = {\n",
    "    'mu': 0,\n",
    "    'seed': seed,\n",
    "}\n",
    "\n",
    "federater = FedNova(model,\n",
    "                    clients=clients,\n",
    "                    client_optimizer_cls=client_optimizer_cls,\n",
    "                    client_optimizer_params=client_optimizer_params,\n",
    "                    server_optimizer=server_optimizer,\n",
    "                    **fed_params)\n",
    "\n",
    "# federater.fit(num_rounds=1, criterion=criterion, num_epochs=2, val_dl=test_dl)\n",
    "federater.fit(num_rounds=5, criterion=criterion, num_epochs=1, val_dl=test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f839a66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34af7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "federater.fit(num_rounds=5, criterion=criterion, num_epochs=1, val_dl=test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04c4d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "round 1 - train_loss : 0.0692 - train_accuracy : 0.9733 - val_loss : 2.2971 - val_accuracy : 0.1301"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e8c695e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1 - train_loss : 0.0896 - train_accuracy : 0.9767 - val_loss : 2.3033 - val_accuracy : 0.1749\n",
      "round 2 - train_loss : 0.0930 - train_accuracy : 0.9748 - val_loss : 2.2953 - val_accuracy : 0.1983\n",
      "round 3 - train_loss : 0.1644 - train_accuracy : 0.9507 - val_loss : 2.2892 - val_accuracy : 0.1776\n",
      "round 4 - train_loss : 0.0562 - train_accuracy : 0.9815 - val_loss : 2.2814 - val_accuracy : 0.1884\n",
      "round 5 - train_loss : 0.1558 - train_accuracy : 0.9643 - val_loss : 2.2728 - val_accuracy : 0.1976\n",
      "round 6 - train_loss : 0.1310 - train_accuracy : 0.9698 - val_loss : 2.2597 - val_accuracy : 0.2462\n",
      "round 7 - train_loss : 0.1211 - train_accuracy : 0.9558 - val_loss : 2.2503 - val_accuracy : 0.3114\n",
      "round 8 - train_loss : 0.1018 - train_accuracy : 0.9707 - val_loss : 2.2407 - val_accuracy : 0.3875\n",
      "round 9 - train_loss : 0.1268 - train_accuracy : 0.9698 - val_loss : 2.2280 - val_accuracy : 0.4173\n",
      "round 10 - train_loss : 0.0696 - train_accuracy : 0.9813 - val_loss : 2.2142 - val_accuracy : 0.4048\n",
      "round 11 - train_loss : 0.0906 - train_accuracy : 0.9768 - val_loss : 2.1966 - val_accuracy : 0.4343\n",
      "round 12 - train_loss : 0.0539 - train_accuracy : 0.9848 - val_loss : 2.1820 - val_accuracy : 0.5140\n",
      "round 13 - train_loss : 0.1284 - train_accuracy : 0.9687 - val_loss : 2.1688 - val_accuracy : 0.5509\n",
      "round 14 - train_loss : 0.0710 - train_accuracy : 0.9788 - val_loss : 2.1408 - val_accuracy : 0.5398\n",
      "round 15 - train_loss : 0.1180 - train_accuracy : 0.9703 - val_loss : 2.1267 - val_accuracy : 0.5815\n",
      "round 16 - train_loss : 0.0812 - train_accuracy : 0.9775 - val_loss : 2.1140 - val_accuracy : 0.6182\n",
      "round 17 - train_loss : 0.0371 - train_accuracy : 0.9895 - val_loss : 2.0866 - val_accuracy : 0.6332\n",
      "round 18 - train_loss : 0.0387 - train_accuracy : 0.9882 - val_loss : 2.0516 - val_accuracy : 0.6558\n",
      "round 19 - train_loss : 0.0488 - train_accuracy : 0.9860 - val_loss : 2.0209 - val_accuracy : 0.6747\n",
      "round 20 - train_loss : 0.1107 - train_accuracy : 0.9668 - val_loss : 1.9982 - val_accuracy : 0.6801\n",
      "round 21 - train_loss : 0.0906 - train_accuracy : 0.9762 - val_loss : 1.9676 - val_accuracy : 0.7257\n",
      "round 22 - train_loss : 0.0542 - train_accuracy : 0.9828 - val_loss : 1.9325 - val_accuracy : 0.7371\n",
      "round 23 - train_loss : 0.0565 - train_accuracy : 0.9833 - val_loss : 1.8955 - val_accuracy : 0.7533\n",
      "round 24 - train_loss : 0.0437 - train_accuracy : 0.9890 - val_loss : 1.8586 - val_accuracy : 0.7483\n",
      "round 25 - train_loss : 0.0571 - train_accuracy : 0.9843 - val_loss : 1.8202 - val_accuracy : 0.7300\n",
      "round 26 - train_loss : 0.0891 - train_accuracy : 0.9750 - val_loss : 1.7815 - val_accuracy : 0.7641\n",
      "round 27 - train_loss : 0.0684 - train_accuracy : 0.9795 - val_loss : 1.7390 - val_accuracy : 0.7771\n",
      "round 28 - train_loss : 0.0483 - train_accuracy : 0.9880 - val_loss : 1.6936 - val_accuracy : 0.7707\n",
      "round 29 - train_loss : 0.0466 - train_accuracy : 0.9865 - val_loss : 1.6553 - val_accuracy : 0.7895\n",
      "round 30 - train_loss : 0.1327 - train_accuracy : 0.9703 - val_loss : 1.6506 - val_accuracy : 0.8401\n",
      "round 31 - train_loss : 0.0437 - train_accuracy : 0.9867 - val_loss : 1.6068 - val_accuracy : 0.8400\n",
      "round 32 - train_loss : 0.0320 - train_accuracy : 0.9910 - val_loss : 1.5750 - val_accuracy : 0.8310\n",
      "round 33 - train_loss : 0.0361 - train_accuracy : 0.9905 - val_loss : 1.5433 - val_accuracy : 0.8134\n",
      "round 34 - train_loss : 0.0224 - train_accuracy : 0.9948 - val_loss : 1.4882 - val_accuracy : 0.7841\n",
      "round 35 - train_loss : 0.0320 - train_accuracy : 0.9918 - val_loss : 1.4289 - val_accuracy : 0.7929\n",
      "round 36 - train_loss : 0.0466 - train_accuracy : 0.9882 - val_loss : 1.4034 - val_accuracy : 0.8131\n",
      "round 37 - train_loss : 0.0968 - train_accuracy : 0.9757 - val_loss : 1.3948 - val_accuracy : 0.8584\n",
      "round 38 - train_loss : 0.0763 - train_accuracy : 0.9797 - val_loss : 1.3620 - val_accuracy : 0.8639\n",
      "round 39 - train_loss : 0.0331 - train_accuracy : 0.9903 - val_loss : 1.3076 - val_accuracy : 0.8788\n",
      "round 40 - train_loss : 0.0370 - train_accuracy : 0.9893 - val_loss : 1.2643 - val_accuracy : 0.8769\n",
      "round 41 - train_loss : 0.0602 - train_accuracy : 0.9823 - val_loss : 1.2527 - val_accuracy : 0.8579\n",
      "round 42 - train_loss : 0.0465 - train_accuracy : 0.9873 - val_loss : 1.2253 - val_accuracy : 0.8484\n",
      "round 43 - train_loss : 0.0324 - train_accuracy : 0.9897 - val_loss : 1.1812 - val_accuracy : 0.8886\n",
      "round 44 - train_loss : 0.0194 - train_accuracy : 0.9943 - val_loss : 1.1238 - val_accuracy : 0.8977\n",
      "round 45 - train_loss : 0.0399 - train_accuracy : 0.9892 - val_loss : 1.0846 - val_accuracy : 0.9016\n",
      "round 46 - train_loss : 0.0706 - train_accuracy : 0.9842 - val_loss : 1.0694 - val_accuracy : 0.9098\n",
      "round 47 - train_loss : 0.0721 - train_accuracy : 0.9782 - val_loss : 1.0585 - val_accuracy : 0.9109\n",
      "round 48 - train_loss : 0.0180 - train_accuracy : 0.9958 - val_loss : 1.0190 - val_accuracy : 0.9146\n",
      "round 49 - train_loss : 0.0229 - train_accuracy : 0.9937 - val_loss : 0.9746 - val_accuracy : 0.9204\n",
      "round 50 - train_loss : 0.0313 - train_accuracy : 0.9915 - val_loss : 0.9566 - val_accuracy : 0.9210\n",
      "round 51 - train_loss : 0.0739 - train_accuracy : 0.9833 - val_loss : 0.9446 - val_accuracy : 0.9212\n",
      "round 52 - train_loss : 0.0633 - train_accuracy : 0.9845 - val_loss : 0.9189 - val_accuracy : 0.9234\n",
      "round 53 - train_loss : 0.0492 - train_accuracy : 0.9863 - val_loss : 0.8966 - val_accuracy : 0.9215\n",
      "round 54 - train_loss : 0.0822 - train_accuracy : 0.9808 - val_loss : 0.8932 - val_accuracy : 0.9156\n",
      "round 55 - train_loss : 0.0312 - train_accuracy : 0.9925 - val_loss : 0.8566 - val_accuracy : 0.9170\n",
      "round 56 - train_loss : 0.0777 - train_accuracy : 0.9818 - val_loss : 0.8499 - val_accuracy : 0.9230\n",
      "round 57 - train_loss : 0.0240 - train_accuracy : 0.9932 - val_loss : 0.8062 - val_accuracy : 0.9171\n",
      "round 58 - train_loss : 0.0404 - train_accuracy : 0.9897 - val_loss : 0.7853 - val_accuracy : 0.9228\n",
      "round 59 - train_loss : 0.0273 - train_accuracy : 0.9918 - val_loss : 0.7503 - val_accuracy : 0.9290\n",
      "round 60 - train_loss : 0.0612 - train_accuracy : 0.9878 - val_loss : 0.7544 - val_accuracy : 0.9262\n",
      "round 61 - train_loss : 0.0408 - train_accuracy : 0.9888 - val_loss : 0.7404 - val_accuracy : 0.9259\n",
      "round 62 - train_loss : 0.0193 - train_accuracy : 0.9955 - val_loss : 0.7126 - val_accuracy : 0.9274\n",
      "round 63 - train_loss : 0.0276 - train_accuracy : 0.9920 - val_loss : 0.6802 - val_accuracy : 0.9249\n",
      "round 64 - train_loss : 0.0320 - train_accuracy : 0.9905 - val_loss : 0.6659 - val_accuracy : 0.9200\n",
      "round 65 - train_loss : 0.0163 - train_accuracy : 0.9957 - val_loss : 0.6325 - val_accuracy : 0.9283\n",
      "round 66 - train_loss : 0.0481 - train_accuracy : 0.9908 - val_loss : 0.6325 - val_accuracy : 0.9281\n",
      "round 67 - train_loss : 0.0452 - train_accuracy : 0.9870 - val_loss : 0.6322 - val_accuracy : 0.9294\n",
      "round 68 - train_loss : 0.0125 - train_accuracy : 0.9972 - val_loss : 0.6062 - val_accuracy : 0.9270\n",
      "round 69 - train_loss : 0.0640 - train_accuracy : 0.9857 - val_loss : 0.6124 - val_accuracy : 0.9294\n",
      "round 70 - train_loss : 0.0665 - train_accuracy : 0.9843 - val_loss : 0.6030 - val_accuracy : 0.9333\n",
      "round 71 - train_loss : 0.0296 - train_accuracy : 0.9907 - val_loss : 0.5788 - val_accuracy : 0.9330\n",
      "round 72 - train_loss : 0.0565 - train_accuracy : 0.9855 - val_loss : 0.5685 - val_accuracy : 0.9296\n",
      "round 73 - train_loss : 0.0179 - train_accuracy : 0.9953 - val_loss : 0.5457 - val_accuracy : 0.9324\n",
      "round 74 - train_loss : 0.0213 - train_accuracy : 0.9945 - val_loss : 0.5303 - val_accuracy : 0.9313\n",
      "round 75 - train_loss : 0.0202 - train_accuracy : 0.9947 - val_loss : 0.5130 - val_accuracy : 0.9315\n",
      "round 76 - train_loss : 0.0414 - train_accuracy : 0.9908 - val_loss : 0.5111 - val_accuracy : 0.9330\n",
      "round 77 - train_loss : 0.0358 - train_accuracy : 0.9917 - val_loss : 0.5003 - val_accuracy : 0.9296\n",
      "round 78 - train_loss : 0.0467 - train_accuracy : 0.9907 - val_loss : 0.4926 - val_accuracy : 0.9324\n",
      "round 79 - train_loss : 0.0288 - train_accuracy : 0.9935 - val_loss : 0.4814 - val_accuracy : 0.9319\n",
      "round 80 - train_loss : 0.0346 - train_accuracy : 0.9887 - val_loss : 0.4775 - val_accuracy : 0.9287\n",
      "round 81 - train_loss : 0.0295 - train_accuracy : 0.9925 - val_loss : 0.4675 - val_accuracy : 0.9261\n",
      "round 82 - train_loss : 0.0192 - train_accuracy : 0.9943 - val_loss : 0.4512 - val_accuracy : 0.9335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 83 - train_loss : 0.0351 - train_accuracy : 0.9910 - val_loss : 0.4455 - val_accuracy : 0.9396\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-fa73057580ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfederater\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-58-a87d903ae813>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, num_rounds, criterion, num_epochs, val_dl, straggler_rate, eval_every_n)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;31m# update a subset of clients with the local solver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0mS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_random_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             train_metrics = self.update(client_ids=S, \n\u001b[0m\u001b[1;32m    142\u001b[0m                                         \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                                         \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-58-a87d903ae813>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, client_ids, criterion, num_epochs, straggler_rate)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;31m# update the client weights and record the local training metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             client_metrics_dict = client.update(\n\u001b[0m\u001b[1;32m    109\u001b[0m                 \u001b[0mclient_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-178521c6a7a4>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, optimizer, criterion, num_epochs, scheduler)\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "federater.fit(num_rounds=100, criterion=criterion, num_epochs=20, val_dl=test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e167fd5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2072,
   "id": "89adc5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 11 - train_loss : 0.0856 - train_accuracy : 0.9775 - val_loss : 0.3990 - val_accuracy : 0.9443\n",
      "round 12 - train_loss : 0.0996 - train_accuracy : 0.9710 - val_loss : 0.3361 - val_accuracy : 0.9469\n",
      "round 13 - train_loss : 0.0887 - train_accuracy : 0.9742 - val_loss : 0.2864 - val_accuracy : 0.9494\n",
      "round 14 - train_loss : 0.0779 - train_accuracy : 0.9803 - val_loss : 0.2479 - val_accuracy : 0.9513\n",
      "round 15 - train_loss : 0.0763 - train_accuracy : 0.9823 - val_loss : 0.2201 - val_accuracy : 0.9529\n",
      "round 16 - train_loss : 0.0817 - train_accuracy : 0.9773 - val_loss : 0.1982 - val_accuracy : 0.9545\n",
      "round 17 - train_loss : 0.0775 - train_accuracy : 0.9820 - val_loss : 0.1813 - val_accuracy : 0.9561\n",
      "round 18 - train_loss : 0.0538 - train_accuracy : 0.9885 - val_loss : 0.1677 - val_accuracy : 0.9574\n",
      "round 19 - train_loss : 0.0546 - train_accuracy : 0.9877 - val_loss : 0.1571 - val_accuracy : 0.9582\n",
      "round 20 - train_loss : 0.0405 - train_accuracy : 0.9910 - val_loss : 0.1485 - val_accuracy : 0.9595\n",
      "round 21 - train_loss : 0.0611 - train_accuracy : 0.9857 - val_loss : 0.1414 - val_accuracy : 0.9600\n",
      "round 22 - train_loss : 0.0440 - train_accuracy : 0.9910 - val_loss : 0.1352 - val_accuracy : 0.9611\n",
      "round 23 - train_loss : 0.0390 - train_accuracy : 0.9922 - val_loss : 0.1298 - val_accuracy : 0.9621\n",
      "round 24 - train_loss : 0.0431 - train_accuracy : 0.9910 - val_loss : 0.1256 - val_accuracy : 0.9623\n",
      "round 25 - train_loss : 0.0452 - train_accuracy : 0.9887 - val_loss : 0.1214 - val_accuracy : 0.9633\n",
      "round 26 - train_loss : 0.0345 - train_accuracy : 0.9925 - val_loss : 0.1179 - val_accuracy : 0.9636\n",
      "round 27 - train_loss : 0.0327 - train_accuracy : 0.9945 - val_loss : 0.1146 - val_accuracy : 0.9643\n",
      "round 28 - train_loss : 0.0349 - train_accuracy : 0.9930 - val_loss : 0.1119 - val_accuracy : 0.9647\n",
      "round 29 - train_loss : 0.0317 - train_accuracy : 0.9937 - val_loss : 0.1095 - val_accuracy : 0.9660\n",
      "round 30 - train_loss : 0.0241 - train_accuracy : 0.9963 - val_loss : 0.1071 - val_accuracy : 0.9665\n",
      "round 31 - train_loss : 0.0238 - train_accuracy : 0.9963 - val_loss : 0.1049 - val_accuracy : 0.9668\n",
      "round 32 - train_loss : 0.0204 - train_accuracy : 0.9978 - val_loss : 0.1035 - val_accuracy : 0.9674\n",
      "round 33 - train_loss : 0.0283 - train_accuracy : 0.9955 - val_loss : 0.1015 - val_accuracy : 0.9677\n",
      "round 34 - train_loss : 0.0169 - train_accuracy : 0.9983 - val_loss : 0.1001 - val_accuracy : 0.9684\n",
      "round 35 - train_loss : 0.0244 - train_accuracy : 0.9953 - val_loss : 0.0988 - val_accuracy : 0.9689\n",
      "round 36 - train_loss : 0.0220 - train_accuracy : 0.9965 - val_loss : 0.0975 - val_accuracy : 0.9691\n",
      "round 37 - train_loss : 0.0180 - train_accuracy : 0.9980 - val_loss : 0.0965 - val_accuracy : 0.9695\n",
      "round 38 - train_loss : 0.0563 - train_accuracy : 0.9918 - val_loss : 0.0955 - val_accuracy : 0.9697\n",
      "round 39 - train_loss : 0.0212 - train_accuracy : 0.9965 - val_loss : 0.0941 - val_accuracy : 0.9702\n",
      "round 40 - train_loss : 0.0167 - train_accuracy : 0.9982 - val_loss : 0.0927 - val_accuracy : 0.9707\n",
      "round 41 - train_loss : 0.0160 - train_accuracy : 0.9975 - val_loss : 0.0917 - val_accuracy : 0.9711\n",
      "round 42 - train_loss : 0.0185 - train_accuracy : 0.9973 - val_loss : 0.0908 - val_accuracy : 0.9718\n",
      "round 43 - train_loss : 0.0167 - train_accuracy : 0.9975 - val_loss : 0.0897 - val_accuracy : 0.9726\n",
      "round 44 - train_loss : 0.0214 - train_accuracy : 0.9965 - val_loss : 0.0886 - val_accuracy : 0.9724\n",
      "round 45 - train_loss : 0.0198 - train_accuracy : 0.9963 - val_loss : 0.0873 - val_accuracy : 0.9730\n",
      "round 46 - train_loss : 0.0185 - train_accuracy : 0.9962 - val_loss : 0.0863 - val_accuracy : 0.9730\n",
      "round 47 - train_loss : 0.0143 - train_accuracy : 0.9980 - val_loss : 0.0857 - val_accuracy : 0.9733\n",
      "round 48 - train_loss : 0.0231 - train_accuracy : 0.9952 - val_loss : 0.0848 - val_accuracy : 0.9731\n",
      "round 49 - train_loss : 0.0176 - train_accuracy : 0.9975 - val_loss : 0.0837 - val_accuracy : 0.9739\n",
      "round 50 - train_loss : 0.0136 - train_accuracy : 0.9985 - val_loss : 0.0826 - val_accuracy : 0.9739\n"
     ]
    }
   ],
   "source": [
    "federater.fit(num_rounds=40, criterion=criterion, num_epochs=5, val_dl=test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f4ba40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2175,
   "id": "9167f3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1 - train_loss : 0.6275 - train_accuracy : 0.8185 - val_loss : 2.2982 - val_accuracy : 0.1241\n"
     ]
    }
   ],
   "source": [
    "# fedavg baseline\n",
    "federater_avg.fit(num_rounds=1, criterion=criterion, num_epochs=1, val_dl=test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2176,
   "id": "b3d7e769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1 - train_loss : 0.8181 - train_accuracy : 0.7648 - val_loss : 2.2974 - val_accuracy : 0.1303\n"
     ]
    }
   ],
   "source": [
    "# fedprox baseline\n",
    "federater_prox.fit(num_rounds=1, criterion=criterion, num_epochs=1, val_dl=test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ba328e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1449ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caf041a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
